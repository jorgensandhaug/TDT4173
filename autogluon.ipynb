{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Processing location A...\n",
      "Shape of X_train_observed before dropping in-between hour rows:  (118669, 45)\n",
      "HEIHEI: X_train_observed gaps in dates:  0\n",
      "HEIHEI: X_train_observed first gap in dates:  DatetimeIndex([], dtype='datetime64[ns]', name='ds', freq=None)\n",
      "HEIHEI: X_train_observed list of size (in days) of each gap:  []\n",
      "HEIHEI: X_train_observed gaps in dates after filling missing dates:  0\n",
      "Shape of X_train_observed after dropping in-between hour rows:  (29668, 45)\n",
      "Shape of X_train_estimated before dropping in-between hour rows:  (17576, 46)\n",
      "HEIHEI: X_train_estimated gaps in dates:  1\n",
      "HEIHEI: X_train_estimated first gap in dates:  DatetimeIndex(['2023-01-27'], dtype='datetime64[ns]', name='ds', freq=None)\n",
      "HEIHEI: X_train_estimated list of size (in days) of each gap:  [1.01041667]\n",
      "HEIHEI: X_train_estimated gaps in dates after filling missing dates:  0\n",
      "Shape of X_train_estimated after dropping in-between hour rows:  (4418, 46)\n",
      "Shape of X_test before dropping in-between hour rows:  (2880, 46)\n",
      "HEIHEI: X_test gaps in dates:  17\n",
      "HEIHEI: X_test first gap in dates:  DatetimeIndex(['2023-05-06'], dtype='datetime64[ns]', name='ds', freq=None)\n",
      "HEIHEI: X_test list of size (in days) of each gap:  [4.01041667 7.01041667 3.01041667 1.01041667 1.01041667 1.01041667\n",
      " 1.01041667 1.01041667 1.01041667 2.01041667 1.01041667 1.01041667\n",
      " 3.01041667 2.01041667 3.01041667 1.01041667 1.01041667]\n",
      "HEIHEI: X_test gaps in dates after filling missing dates:  0\n",
      "Shape of X_test after dropping in-between hour rows:  (1536, 46)\n",
      "X_train_observed shape: (29668, 46)\n",
      "X_train_estimated shape: (4418, 46)\n",
      "X_test shape: (1536, 46)\n",
      "y_train shape: (34085, 1)\n",
      "y_train columns:  Index(['y'], dtype='object')\n",
      "Shape of y_train before filling missing dates:  (34085, 1)\n",
      "Shape of y_train after filling missing dates:  (34274, 1)\n",
      "LOOK: Number of gaps in X_train_observed plus number of gaps in X_train_estimated before:  0\n",
      "LOOK: Number of gaps in X_train_observed plus number of gaps in X_train_estimated after:  1\n",
      "LOOK: list of size (in days) of each gap:  [7.875]\n",
      "if the number is bigger after than before that means there is a gap in time between the observed and estimated training sets\n",
      "X_train dates info:  2019-06-02 22:00:00 2023-04-30 23:00:00 1428 days 01:00:00\n",
      "X_test dates info:  2023-05-01 00:00:00 2023-07-03 23:00:00 63 days 23:00:00\n",
      "y_train dates info:  2019-06-02 22:00:00 2023-04-30 23:00:00 1428 days 01:00:00\n",
      "X_train gaps in dates:  1\n",
      "X_test gaps in dates:  0\n",
      "y_train gaps in dates:  0\n",
      "X_train gaps in dates after filling missing dates:  0\n",
      "X_test gaps in dates after filling missing dates:  0\n",
      "Number of missing values in X_train:  53521\n",
      "Number of missing values in X_test:  38573\n",
      "Number of missing values in y_train:  189\n",
      "Number of missing values in X_train after merging with y_train:  53521\n",
      "Final shape of X_train for location A:  (34274, 48)\n",
      "Final shape of X_test for location A:  (1536, 47)\n",
      "\n",
      "\n",
      "\n",
      "Processing location B...\n",
      "Shape of X_train_observed before dropping in-between hour rows:  (116929, 45)\n",
      "HEIHEI: X_train_observed gaps in dates:  0\n",
      "HEIHEI: X_train_observed first gap in dates:  DatetimeIndex([], dtype='datetime64[ns]', name='ds', freq=None)\n",
      "HEIHEI: X_train_observed list of size (in days) of each gap:  []\n",
      "HEIHEI: X_train_observed gaps in dates after filling missing dates:  0\n",
      "Shape of X_train_observed after dropping in-between hour rows:  (29233, 45)\n",
      "Shape of X_train_estimated before dropping in-between hour rows:  (17576, 46)\n",
      "HEIHEI: X_train_estimated gaps in dates:  1\n",
      "HEIHEI: X_train_estimated first gap in dates:  DatetimeIndex(['2023-01-27'], dtype='datetime64[ns]', name='ds', freq=None)\n",
      "HEIHEI: X_train_estimated list of size (in days) of each gap:  [1.01041667]\n",
      "HEIHEI: X_train_estimated gaps in dates after filling missing dates:  0\n",
      "Shape of X_train_estimated after dropping in-between hour rows:  (4418, 46)\n",
      "Shape of X_test before dropping in-between hour rows:  (2880, 46)\n",
      "HEIHEI: X_test gaps in dates:  17\n",
      "HEIHEI: X_test first gap in dates:  DatetimeIndex(['2023-05-06'], dtype='datetime64[ns]', name='ds', freq=None)\n",
      "HEIHEI: X_test list of size (in days) of each gap:  [4.01041667 7.01041667 3.01041667 1.01041667 1.01041667 1.01041667\n",
      " 1.01041667 1.01041667 1.01041667 2.01041667 1.01041667 1.01041667\n",
      " 3.01041667 2.01041667 3.01041667 1.01041667 1.01041667]\n",
      "HEIHEI: X_test gaps in dates after filling missing dates:  0\n",
      "Shape of X_test after dropping in-between hour rows:  (1536, 46)\n",
      "X_train_observed shape: (29233, 46)\n",
      "X_train_estimated shape: (4418, 46)\n",
      "X_test shape: (1536, 46)\n",
      "y_train shape: (32848, 1)\n",
      "y_train columns:  Index(['y'], dtype='object')\n",
      "Shape of y_train before filling missing dates:  (32848, 1)\n",
      "Shape of y_train after filling missing dates:  (37945, 1)\n",
      "LOOK: Number of gaps in X_train_observed plus number of gaps in X_train_estimated before:  0\n",
      "LOOK: Number of gaps in X_train_observed plus number of gaps in X_train_estimated after:  1\n",
      "LOOK: list of size (in days) of each gap:  [178.91666667]\n",
      "if the number is bigger after than before that means there is a gap in time between the observed and estimated training sets\n",
      "X_train dates info:  2019-01-01 00:00:00 2023-04-30 23:00:00 1580 days 23:00:00\n",
      "X_test dates info:  2023-05-01 00:00:00 2023-07-03 23:00:00 63 days 23:00:00\n",
      "y_train dates info:  2018-12-31 23:00:00 2023-04-30 23:00:00 1581 days 00:00:00\n",
      "X_train gaps in dates:  1\n",
      "X_test gaps in dates:  0\n",
      "y_train gaps in dates:  0\n",
      "X_train gaps in dates after filling missing dates:  0\n",
      "X_test gaps in dates after filling missing dates:  0\n",
      "Number of missing values in X_train:  239726\n",
      "Number of missing values in X_test:  38553\n",
      "Number of missing values in y_train:  5101\n",
      "Number of missing values in X_train after merging with y_train:  239772\n",
      "Final shape of X_train for location B:  (37945, 48)\n",
      "Final shape of X_test for location B:  (1536, 47)\n",
      "\n",
      "\n",
      "\n",
      "Processing location C...\n",
      "Shape of X_train_observed before dropping in-between hour rows:  (116825, 45)\n",
      "HEIHEI: X_train_observed gaps in dates:  0\n",
      "HEIHEI: X_train_observed first gap in dates:  DatetimeIndex([], dtype='datetime64[ns]', name='ds', freq=None)\n",
      "HEIHEI: X_train_observed list of size (in days) of each gap:  []\n",
      "HEIHEI: X_train_observed gaps in dates after filling missing dates:  0\n",
      "Shape of X_train_observed after dropping in-between hour rows:  (29207, 45)\n",
      "Shape of X_train_estimated before dropping in-between hour rows:  (17576, 46)\n",
      "HEIHEI: X_train_estimated gaps in dates:  1\n",
      "HEIHEI: X_train_estimated first gap in dates:  DatetimeIndex(['2023-01-27'], dtype='datetime64[ns]', name='ds', freq=None)\n",
      "HEIHEI: X_train_estimated list of size (in days) of each gap:  [1.01041667]\n",
      "HEIHEI: X_train_estimated gaps in dates after filling missing dates:  0\n",
      "Shape of X_train_estimated after dropping in-between hour rows:  (4418, 46)\n",
      "Shape of X_test before dropping in-between hour rows:  (2880, 46)\n",
      "HEIHEI: X_test gaps in dates:  17\n",
      "HEIHEI: X_test first gap in dates:  DatetimeIndex(['2023-05-06'], dtype='datetime64[ns]', name='ds', freq=None)\n",
      "HEIHEI: X_test list of size (in days) of each gap:  [4.01041667 7.01041667 3.01041667 1.01041667 1.01041667 1.01041667\n",
      " 1.01041667 1.01041667 1.01041667 2.01041667 1.01041667 1.01041667\n",
      " 3.01041667 2.01041667 3.01041667 1.01041667 1.01041667]\n",
      "HEIHEI: X_test gaps in dates after filling missing dates:  0\n",
      "Shape of X_test after dropping in-between hour rows:  (1536, 46)\n",
      "X_train_observed shape: (29207, 46)\n",
      "X_train_estimated shape: (4418, 46)\n",
      "X_test shape: (1536, 46)\n",
      "y_train shape: (32155, 1)\n",
      "y_train columns:  Index(['y'], dtype='object')\n",
      "Shape of y_train before filling missing dates:  (32155, 1)\n",
      "Shape of y_train after filling missing dates:  (37945, 1)\n",
      "LOOK: Number of gaps in X_train_observed plus number of gaps in X_train_estimated before:  0\n",
      "LOOK: Number of gaps in X_train_observed plus number of gaps in X_train_estimated after:  1\n",
      "LOOK: list of size (in days) of each gap:  [180.]\n",
      "if the number is bigger after than before that means there is a gap in time between the observed and estimated training sets\n",
      "X_train dates info:  2019-01-01 00:00:00 2023-04-30 23:00:00 1580 days 23:00:00\n",
      "X_test dates info:  2023-05-01 00:00:00 2023-07-03 23:00:00 63 days 23:00:00\n",
      "y_train dates info:  2018-12-31 23:00:00 2023-04-30 23:00:00 1581 days 00:00:00\n",
      "X_train gaps in dates:  1\n",
      "X_test gaps in dates:  0\n",
      "y_train gaps in dates:  0\n",
      "X_train gaps in dates after filling missing dates:  0\n",
      "X_test gaps in dates after filling missing dates:  0\n",
      "Number of missing values in X_train:  240647\n",
      "Number of missing values in X_test:  38610\n",
      "Number of missing values in y_train:  11850\n",
      "Number of missing values in X_train after merging with y_train:  240693\n",
      "Final shape of X_train for location C:  (37945, 48)\n",
      "Final shape of X_test for location C:  (1536, 47)\n",
      "Final shape of X_train:  (110164, 48)\n",
      "Final shape of X_test:  (4608, 47)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from darts import TimeSeries\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def fix_datetime(X, name):\n",
    "    \"\"\"\n",
    "    Function to fix and standardize datetime in the given DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: DataFrame to be modified.\n",
    "    - name: String representing the name of the DataFrame, used for logging.\n",
    "    \n",
    "    Returns:\n",
    "    - Modified DataFrame with standardized datetime.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert 'date_forecast' to datetime format and replace original column with 'ds'\n",
    "    X['ds'] = pd.to_datetime(X['date_forecast'])\n",
    "    X.drop(columns=['date_forecast'], inplace=True, errors='ignore')\n",
    "\n",
    "    # Sort DataFrame by the new datetime column ('ds') and set it as the index\n",
    "    X.sort_values(by='ds', inplace=True)\n",
    "    X.set_index('ds', inplace=True)\n",
    "\n",
    "    # Log the shape of the DataFrame before dropping rows with in-between minutes\n",
    "    print(f\"Shape of {name} before dropping in-between hour rows: \", X.shape)\n",
    "\n",
    "    # Identify and log gaps in the date sequence\n",
    "    print(f\"HEIHEI: {name} gaps in dates: \", X.index.to_series().diff().dt.total_seconds().gt(60*15).sum())\n",
    "    print(f\"HEIHEI: {name} first gap in dates: \", X[X.index.to_series().diff().dt.total_seconds().gt(60*15)==True].index[:1])\n",
    "\n",
    "    # Calculate and log the size of each gap in the date sequence\n",
    "    temp = X.index.to_series().diff().dt.total_seconds()\n",
    "    if temp.shape[0] > 0:\n",
    "        print(f\"HEIHEI: {name} list of size (in days) of each gap: \", temp[temp.gt(60*15)].values / (60*60*24))\n",
    "    \n",
    "    # temporarily transform into darts time series to fill missing dates\n",
    "    # get date_calc if date_calc is column in X\n",
    "    temp_calc = None\n",
    "    if \"date_calc\" in X.columns:\n",
    "        temp_calc = X[\"date_calc\"]\n",
    "        X.drop(columns=['date_calc'], inplace=True)\n",
    "    X = TimeSeries.from_dataframe(df=X, freq=\"15T\", fill_missing_dates=True, fillna_value=None).pd_dataframe()\n",
    "    if temp_calc is not None:\n",
    "        X[\"date_calc\"] = temp_calc\n",
    "\n",
    "    print(f\"HEIHEI: {name} gaps in dates after filling missing dates: \", X.index.to_series().diff().dt.total_seconds().gt(60*15).sum())\n",
    "\n",
    "\n",
    "    # Drop rows where the minute part of the time is not 0\n",
    "    X = X[X.index.minute == 0]\n",
    "\n",
    "    # Log the shape of the DataFrame after dropping rows with in-between minutes\n",
    "    print(f\"Shape of {name} after dropping in-between hour rows: \", X.shape)\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "\n",
    "def convert_to_datetime(X_train_observed, X_train_estimated, X_test, y_train):\n",
    "    X_train_observed = fix_datetime(X_train_observed, \"X_train_observed\")\n",
    "    X_train_estimated = fix_datetime(X_train_estimated, \"X_train_estimated\")\n",
    "    X_test = fix_datetime(X_test, \"X_test\")\n",
    "\n",
    "\n",
    "    X_train_observed[\"estimated_diff_hours\"] = 0\n",
    "    X_train_estimated[\"estimated_diff_hours\"] = (X_train_estimated.index - pd.to_datetime(X_train_estimated[\"date_calc\"])).dt.total_seconds() / 3600.0\n",
    "    X_test[\"estimated_diff_hours\"] = (X_test.index - pd.to_datetime(X_test[\"date_calc\"])).dt.total_seconds() / 3600.0\n",
    "\n",
    "    X_train_estimated.drop(columns=['date_calc'], inplace=True)\n",
    "    X_test.drop(columns=['date_calc'], inplace=True)\n",
    "\n",
    "    y_train['ds'] = pd.to_datetime(y_train['time'])\n",
    "    y_train.drop(columns=['time'], inplace=True)\n",
    "    y_train.sort_values(by='ds', inplace=True)\n",
    "    y_train.set_index('ds', inplace=True)\n",
    "\n",
    "    return X_train_observed, X_train_estimated, X_test, y_train\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# location_map = {\n",
    "#     \"A\": 0,\n",
    "#     \"B\": 1,\n",
    "#     \"C\": 2\n",
    "# }\n",
    "\n",
    "\n",
    "def preprocess_data(X_train_observed, X_train_estimated, X_test, y_train, location):\n",
    "    # convert to datetime\n",
    "    X_train_observed, X_train_estimated, X_test, y_train = convert_to_datetime(X_train_observed, X_train_estimated, X_test, y_train)\n",
    "\n",
    "\n",
    "    # # cast all columns to float64\n",
    "    # X_train = X_train.astype('float64')\n",
    "    # X_test = X_test.astype('float64')\n",
    "\n",
    "\n",
    "    print(f\"X_train_observed shape: {X_train_observed.shape}\")\n",
    "    print(f\"X_train_estimated shape: {X_train_estimated.shape}\")\n",
    "    print(f\"X_test shape: {X_test.shape}\")\n",
    "    print(f\"y_train shape: {y_train.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "    y_train[\"y\"] = y_train[\"pv_measurement\"].astype('float64')\n",
    "    y_train.drop(columns=['pv_measurement'], inplace=True)\n",
    "    print(\"y_train columns: \", y_train.columns)\n",
    "\n",
    "    # temporarily transform into darts time series to fill missing dates\n",
    "    print(\"Shape of y_train before filling missing dates: \", y_train.shape)\n",
    "    y_train = TimeSeries.from_dataframe(df=y_train, freq=\"H\", fill_missing_dates=True, fillna_value=None).pd_dataframe()\n",
    "    print(\"Shape of y_train after filling missing dates: \", y_train.shape)\n",
    "\n",
    "\n",
    "    # number of gaps in X_train_observed + X_train_estimated before\n",
    "    print(f\"LOOK: Number of gaps in X_train_observed plus number of gaps in X_train_estimated before: \", X_train_observed.index.to_series().diff().dt.total_seconds().gt(3600).sum() + X_train_estimated.index.to_series().diff().dt.total_seconds().gt(3600).sum())\n",
    "    X_train = pd.concat([X_train_observed, X_train_estimated])\n",
    "    print(f\"LOOK: Number of gaps in X_train_observed plus number of gaps in X_train_estimated after: \", X_train.index.to_series().diff().dt.total_seconds().gt(3600).sum())\n",
    "    # print size of gaps in X_train\n",
    "    temp = X_train.index.to_series().diff().dt.total_seconds()\n",
    "    if temp.shape[0] > 0:\n",
    "        print(\"LOOK: list of size (in days) of each gap: \", temp[temp.gt(3600)].values / (60*60*24))\n",
    "    print(\"if the number is bigger after than before that means there is a gap in time between the observed and estimated training sets\")\n",
    "\n",
    "    # print info on dates in X_train, and if there are any missing dates\n",
    "    print(\"X_train dates info: \", X_train.index.min(), X_train.index.max(), X_train.index.max() - X_train.index.min())\n",
    "    print(\"X_test dates info: \", X_test.index.min(), X_test.index.max(), X_test.index.max() - X_test.index.min())\n",
    "    print(\"y_train dates info: \", y_train.index.min(), y_train.index.max(), y_train.index.max() - y_train.index.min())\n",
    "\n",
    "    # any gaps in dates?\n",
    "    print(\"X_train gaps in dates: \", X_train.index.to_series().diff().dt.total_seconds().gt(3600).sum())\n",
    "    print(\"X_test gaps in dates: \", X_test.index.to_series().diff().dt.total_seconds().gt(3600).sum())\n",
    "    print(\"y_train gaps in dates: \", y_train.index.to_series().diff().dt.total_seconds().gt(3600).sum())\n",
    "\n",
    "    # temporarily transform into darts time series to fill missing dates\n",
    "    X_train = TimeSeries.from_dataframe(df=X_train, freq=\"H\", fill_missing_dates=True, fillna_value=None).pd_dataframe()\n",
    "    X_test = TimeSeries.from_dataframe(df=X_test, freq=\"H\", fill_missing_dates=True, fillna_value=None).pd_dataframe()\n",
    "    print(\"X_train gaps in dates after filling missing dates: \", X_train.index.to_series().diff().dt.total_seconds().gt(3600).sum())\n",
    "    print(\"X_test gaps in dates after filling missing dates: \", X_test.index.to_series().diff().dt.total_seconds().gt(3600).sum())\n",
    "\n",
    "    \n",
    "\n",
    "    # clip all y values to 0 if negative\n",
    "    y_train[\"y\"] = y_train[\"y\"].clip(lower=0)\n",
    "    \n",
    "    # print Number of missing values in X train\n",
    "    print(\"Number of missing values in X_train: \", X_train.isnull().sum().sum())\n",
    "    print(\"Number of missing values in X_test: \", X_test.isnull().sum().sum())\n",
    "    # y_train missing values\n",
    "    print(\"Number of missing values in y_train: \", y_train.isnull().sum().sum())\n",
    "    X_train = pd.merge(X_train, y_train, how=\"outer\", left_index=True, right_index=True)\n",
    "    print(\"Number of missing values in X_train after merging with y_train: \", X_train.drop(columns=['y']).isnull().sum().sum())\n",
    "\n",
    "\n",
    "\n",
    "    X_train[\"location\"] = location\n",
    "    X_test[\"location\"] = location\n",
    "    \n",
    "    return X_train, X_test\n",
    "    \n",
    "\n",
    "\n",
    "# Define locations\n",
    "locations = ['A', 'B', 'C']\n",
    "\n",
    "X_trains = []\n",
    "X_tests = []\n",
    "y_trains = []\n",
    "# Loop through locations\n",
    "for loc in locations:\n",
    "    print(\"\\n\\n\")\n",
    "    print(f\"Processing location {loc}...\")\n",
    "    # Read target training data\n",
    "    y_train = pd.read_parquet(f'{loc}/train_targets.parquet')\n",
    "    \n",
    "    # Read estimated training data and add location feature\n",
    "    X_train_estimated = pd.read_parquet(f'{loc}/X_train_estimated.parquet')\n",
    "    \n",
    "    # Read observed training data and add location feature\n",
    "    X_train_observed= pd.read_parquet(f'{loc}/X_train_observed.parquet')\n",
    "\n",
    "    # Read estimated test data and add location feature\n",
    "    X_test_estimated = pd.read_parquet(f'{loc}/X_test_estimated.parquet')\n",
    "    \n",
    "    # Concatenate observed and estimated datasets for each location\n",
    "    #X_train = pd.concat([X_train_estimated, X_train_observed])\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # Preprocess data\n",
    "    X_train, X_test = preprocess_data(X_train_observed, X_train_estimated, X_test_estimated, y_train, loc)\n",
    "\n",
    "    print(f\"Final shape of X_train for location {loc}: \", X_train.shape)\n",
    "    print(f\"Final shape of X_test for location {loc}: \", X_test.shape)\n",
    "\n",
    "    # print(y_train.head(), y_train.shape)\n",
    "    # print(X_train.head(), X_train.shape)\n",
    "    # print(X_train.head(), X_train.shape)\n",
    "    # print(type(X_train['y']))\n",
    "\n",
    "    # Save data to csv\n",
    "    X_train.to_csv(f'{loc}/X_train.csv', index=True)\n",
    "    X_test.to_csv(f'{loc}/X_test.csv', index=True)\n",
    "\n",
    "\n",
    "    X_trains.append(X_train)\n",
    "    X_tests.append(X_test)\n",
    "\n",
    "# Concatenate all data and save to csv\n",
    "X_train = pd.concat(X_trains)\n",
    "X_test = pd.concat(X_tests)\n",
    "\n",
    "print(f\"Final shape of X_train: \", X_train.shape)\n",
    "print(f\"Final shape of X_test: \", X_test.shape)\n",
    "\n",
    "X_train.to_csv('X_train_raw.csv', index=True)\n",
    "X_test.to_csv('X_test_raw.csv', index=True)\n",
    "\n",
    "\n",
    "# save where nan y values are dropped\n",
    "X_train.dropna(subset=['y'], inplace=True)\n",
    "X_train.to_csv('X_train.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded data from: X_train.csv | Columns = 49 / 49 | Rows = 93024 -> 93024\n",
      "Loaded data from: X_test_raw.csv | Columns = 48 / 48 | Rows = 4608 -> 4608\n",
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20231005_083216/\"\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=20\n",
      "Beginning AutoGluon training ... Time limit = 14400s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20231005_083216/\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.10.12\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Debian 5.10.179-5 (2023-08-08)\n",
      "Disk Space Avail:   102.58 GB / 105.09 GB (97.6%)\n",
      "Train Data Rows:    93024\n",
      "Train Data Columns: 48\n",
      "Label Column: y\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (5733.42, -0.0, 287.01965, 766.40778)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    64812.06 MB\n",
      "\tTrain Data (Original)  Memory Usage: 46.7 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                      : 46 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', ...]\n",
      "\t\t('object', [])                     :  1 | ['location']\n",
      "\t\t('object', ['datetime_as_object']) :  1 | ['ds']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])             :  1 | ['location']\n",
      "\t\t('float', [])                : 45 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', ...]\n",
      "\t\t('int', ['bool'])            :  1 | ['snow_density:kgm3']\n",
      "\t\t('int', ['datetime_as_int']) :  5 | ['ds', 'ds.year', 'ds.month', 'ds.day', 'ds.dayofweek']\n",
      "\t0.7s = Fit runtime\n",
      "\t48 features in original data used to generate 52 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 37.4 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.79s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 9597.07s of the 14399.2s of remaining time.\n",
      "\t-285.3275\t = Validation score   (-mean_absolute_error)\n",
      "\t0.16s\t = Training   runtime\n",
      "\t8.37s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 9588.33s of the 14390.47s of remaining time.\n",
      "\t-337.9007\t = Validation score   (-mean_absolute_error)\n",
      "\t0.14s\t = Training   runtime\n",
      "\t8.33s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 9579.65s of the 14381.78s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-70.6812\t = Validation score   (-mean_absolute_error)\n",
      "\t109.53s\t = Training   runtime\n",
      "\t140.61s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 9449.21s of the 14251.35s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-71.9967\t = Validation score   (-mean_absolute_error)\n",
      "\t126.34s\t = Training   runtime\n",
      "\t139.12s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 9303.0s of the 14105.13s of remaining time.\n",
      "\t-83.3124\t = Validation score   (-mean_absolute_error)\n",
      "\t66.32s\t = Training   runtime\n",
      "\t4.84s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 9230.71s of the 14032.84s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-80.7018\t = Validation score   (-mean_absolute_error)\n",
      "\t744.11s\t = Training   runtime\n",
      "\t0.36s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 8484.68s of the 13286.81s of remaining time.\n",
      "\t-83.8083\t = Validation score   (-mean_absolute_error)\n",
      "\t12.55s\t = Training   runtime\n",
      "\t4.63s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 8466.38s of the 13268.51s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=12906, ip=10.128.0.5)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 829, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 327, in _fit\n",
      "    self.model.fit_one_cycle(epochs, params[\"lr\"], cbs=callbacks)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/fastai/callback/schedule.py\", line 119, in fit_one_cycle\n",
      "    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/fastai/learner.py\", line 264, in fit\n",
      "    self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/fastai/learner.py\", line 201, in _with_events\n",
      "    self(f'after_{event_type}');  final()\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/fastai/learner.py\", line 172, in __call__\n",
      "    def __call__(self, event_name): L(event_name).map(self._call_one)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/fastcore/foundation.py\", line 156, in map\n",
      "    def map(self, f, *args, **kwargs): return self._new(map_ex(self, f, *args, gen=False, **kwargs))\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/fastcore/basics.py\", line 840, in map_ex\n",
      "    return list(res)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/fastcore/basics.py\", line 825, in __call__\n",
      "    return self.func(*fargs, **kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/fastai/learner.py\", line 176, in _call_one\n",
      "    for cb in self.cbs.sorted('order'): cb(event_name)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/fastai/callback/core.py\", line 62, in __call__\n",
      "    except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/fastai/callback/core.py\", line 60, in __call__\n",
      "    try: res = getcallable(self, event_name)()\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/callbacks.py\", line 107, in after_fit\n",
      "    self.learn.load(f\"{self.fname}\", with_opt=self.with_opt)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/fastai/learner.py\", line 420, in load\n",
      "    load_model(file, self.model, self.opt, device=device, **kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/fastai/learner.py\", line 51, in load_model\n",
      "    state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/serialization.py\", line 771, in load\n",
      "    with _open_file_like(f, 'rb') as opened_file:\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/serialization.py\", line 270, in _open_file_like\n",
      "    return _open_file(name_or_buffer, mode)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/serialization.py\", line 251, in __init__\n",
      "    super(_open_file, self).__init__(open(name, mode))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/var/tmp/tmpbtqq73km/models/model.pth'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1733, in _train_and_save\n",
      "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1684, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 829, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 169, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 266, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 592, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 581, in after_all_folds_scheduled\n",
      "    raise processed_exception\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 541, in after_all_folds_scheduled\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time = self.ray.get(finished)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 105, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/ray/_private/worker.py\", line 2380, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(FileNotFoundError): \u001b[36mray::_ray_fit()\u001b[39m (pid=12906, ip=10.128.0.5)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 829, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 327, in _fit\n",
      "    self.model.fit_one_cycle(epochs, params[\"lr\"], cbs=callbacks)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/fastai/callback/schedule.py\", line 119, in fit_one_cycle\n",
      "    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/fastai/learner.py\", line 264, in fit\n",
      "    self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/fastai/learner.py\", line 201, in _with_events\n",
      "    self(f'after_{event_type}');  final()\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/fastai/learner.py\", line 172, in __call__\n",
      "    def __call__(self, event_name): L(event_name).map(self._call_one)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/fastcore/foundation.py\", line 156, in map\n",
      "    def map(self, f, *args, **kwargs): return self._new(map_ex(self, f, *args, gen=False, **kwargs))\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/fastcore/basics.py\", line 840, in map_ex\n",
      "    return list(res)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/fastcore/basics.py\", line 825, in __call__\n",
      "    return self.func(*fargs, **kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/fastai/learner.py\", line 176, in _call_one\n",
      "    for cb in self.cbs.sorted('order'): cb(event_name)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/fastai/callback/core.py\", line 62, in __call__\n",
      "    except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/fastai/callback/core.py\", line 60, in __call__\n",
      "    try: res = getcallable(self, event_name)()\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/callbacks.py\", line 107, in after_fit\n",
      "    self.learn.load(f\"{self.fname}\", with_opt=self.with_opt)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/fastai/learner.py\", line 420, in load\n",
      "    load_model(file, self.model, self.opt, device=device, **kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/fastai/learner.py\", line 51, in load_model\n",
      "    state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/serialization.py\", line 771, in load\n",
      "    with _open_file_like(f, 'rb') as opened_file:\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/serialization.py\", line 270, in _open_file_like\n",
      "    return _open_file(name_or_buffer, mode)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/serialization.py\", line 251, in __init__\n",
      "    super(_open_file, self).__init__(open(name, mode))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/var/tmp/tmpbtqq73km/models/model.pth'\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 8357.49s of the 13159.62s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "2023-10-05 08:53:02,577\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2023-10-05 08:53:02,579\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2023-10-05 08:53:02,584\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2023-10-05 08:53:02,586\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2023-10-05 08:53:02,587\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2023-10-05 08:53:02,588\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2023-10-05 08:53:02,591\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\t-81.3198\t = Validation score   (-mean_absolute_error)\n",
      "\t457.84s\t = Training   runtime\n",
      "\t135.37s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 7879.02s of the 12681.15s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-80.7424\t = Validation score   (-mean_absolute_error)\n",
      "\t386.96s\t = Training   runtime\n",
      "\t0.97s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 7490.1s of the 12292.23s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-66.3872\t = Validation score   (-mean_absolute_error)\n",
      "\t297.22s\t = Training   runtime\n",
      "\t190.79s\t = Validation runtime\n",
      "Repeating k-fold bagging: 2/20\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 7160.66s of the 11962.79s of remaining time.\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-68.7333\t = Validation score   (-mean_absolute_error)\n",
      "\t220.38s\t = Training   runtime\n",
      "\t278.56s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 7028.64s of the 11830.77s of remaining time.\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-69.6326\t = Validation score   (-mean_absolute_error)\n",
      "\t252.9s\t = Training   runtime\n",
      "\t273.7s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 6881.43s of the 11683.56s of remaining time.\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-79.0736\t = Validation score   (-mean_absolute_error)\n",
      "\t1493.79s\t = Training   runtime\n",
      "\t0.7s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 6129.77s of the 10931.9s of remaining time.\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-77.6253\t = Validation score   (-mean_absolute_error)\n",
      "\t914.26s\t = Training   runtime\n",
      "\t262.2s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 5651.19s of the 10453.32s of remaining time.\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-78.7208\t = Validation score   (-mean_absolute_error)\n",
      "\t724.56s\t = Training   runtime\n",
      "\t1.86s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 5311.61s of the 10113.74s of remaining time.\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-65.1182\t = Validation score   (-mean_absolute_error)\n",
      "\t598.04s\t = Training   runtime\n",
      "\t379.84s\t = Validation runtime\n",
      "Repeating k-fold bagging: 3/20\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 4983.71s of the 9785.84s of remaining time.\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-68.0389\t = Validation score   (-mean_absolute_error)\n",
      "\t334.88s\t = Training   runtime\n",
      "\t413.74s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 4851.92s of the 9654.05s of remaining time.\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-68.6704\t = Validation score   (-mean_absolute_error)\n",
      "\t379.1s\t = Training   runtime\n",
      "\t406.73s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 4705.4s of the 9507.53s of remaining time.\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-78.2481\t = Validation score   (-mean_absolute_error)\n",
      "\t2245.44s\t = Training   runtime\n",
      "\t1.05s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 3951.82s of the 8753.95s of remaining time.\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-76.2136\t = Validation score   (-mean_absolute_error)\n",
      "\t1408.72s\t = Training   runtime\n",
      "\t455.37s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 3428.23s of the 8230.36s of remaining time.\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-77.871\t = Validation score   (-mean_absolute_error)\n",
      "\t1060.44s\t = Training   runtime\n",
      "\t2.93s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 3090.38s of the 7892.51s of remaining time.\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-64.645\t = Validation score   (-mean_absolute_error)\n",
      "\t893.43s\t = Training   runtime\n",
      "\t571.98s\t = Validation runtime\n",
      "Completed 3/20 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 959.71s of the 7565.8s of remaining time.\n",
      "\t-64.1106\t = Validation score   (-mean_absolute_error)\n",
      "\t1.39s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 9 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 7564.39s of the 7564.34s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-49.2301\t = Validation score   (-mean_absolute_error)\n",
      "\t127.61s\t = Training   runtime\n",
      "\t128.88s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 7417.47s of the 7417.45s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-49.7129\t = Validation score   (-mean_absolute_error)\n",
      "\t98.83s\t = Training   runtime\n",
      "\t15.28s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 7312.26s of the 7312.24s of remaining time.\n",
      "\t-51.1884\t = Validation score   (-mean_absolute_error)\n",
      "\t103.28s\t = Training   runtime\n",
      "\t5.1s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 7202.72s of the 7202.7s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-50.7776\t = Validation score   (-mean_absolute_error)\n",
      "\t761.95s\t = Training   runtime\n",
      "\t0.33s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 6438.75s of the 6438.72s of remaining time.\n",
      "\t-54.3873\t = Validation score   (-mean_absolute_error)\n",
      "\t18.2s\t = Training   runtime\n",
      "\t5.07s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 6414.3s of the 6414.28s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=46640, ip=10.128.0.5)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 829, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 327, in _fit\n",
      "    self.model.fit_one_cycle(epochs, params[\"lr\"], cbs=callbacks)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/fastai/callback/schedule.py\", line 119, in fit_one_cycle\n",
      "    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/fastai/learner.py\", line 264, in fit\n",
      "    self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/fastai/learner.py\", line 201, in _with_events\n",
      "    self(f'after_{event_type}');  final()\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/fastai/learner.py\", line 172, in __call__\n",
      "    def __call__(self, event_name): L(event_name).map(self._call_one)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/fastcore/foundation.py\", line 156, in map\n",
      "    def map(self, f, *args, **kwargs): return self._new(map_ex(self, f, *args, gen=False, **kwargs))\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/fastcore/basics.py\", line 840, in map_ex\n",
      "    return list(res)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/fastcore/basics.py\", line 825, in __call__\n",
      "    return self.func(*fargs, **kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/fastai/learner.py\", line 176, in _call_one\n",
      "    for cb in self.cbs.sorted('order'): cb(event_name)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/fastai/callback/core.py\", line 62, in __call__\n",
      "    except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/fastai/callback/core.py\", line 60, in __call__\n",
      "    try: res = getcallable(self, event_name)()\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/callbacks.py\", line 107, in after_fit\n",
      "    self.learn.load(f\"{self.fname}\", with_opt=self.with_opt)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/fastai/learner.py\", line 420, in load\n",
      "    load_model(file, self.model, self.opt, device=device, **kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/fastai/learner.py\", line 51, in load_model\n",
      "    state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/serialization.py\", line 771, in load\n",
      "    with _open_file_like(f, 'rb') as opened_file:\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/serialization.py\", line 270, in _open_file_like\n",
      "    return _open_file(name_or_buffer, mode)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/serialization.py\", line 251, in __init__\n",
      "    super(_open_file, self).__init__(open(name, mode))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/var/tmp/tmpltd6fy4h/models/model.pth'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1733, in _train_and_save\n",
      "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1684, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 829, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 169, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 266, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 592, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 581, in after_all_folds_scheduled\n",
      "    raise processed_exception\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 541, in after_all_folds_scheduled\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time = self.ray.get(finished)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 105, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/ray/_private/worker.py\", line 2380, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(FileNotFoundError): \u001b[36mray::_ray_fit()\u001b[39m (pid=46640, ip=10.128.0.5)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 829, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 327, in _fit\n",
      "    self.model.fit_one_cycle(epochs, params[\"lr\"], cbs=callbacks)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/fastai/callback/schedule.py\", line 119, in fit_one_cycle\n",
      "    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/fastai/learner.py\", line 264, in fit\n",
      "    self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/fastai/learner.py\", line 201, in _with_events\n",
      "    self(f'after_{event_type}');  final()\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/fastai/learner.py\", line 172, in __call__\n",
      "    def __call__(self, event_name): L(event_name).map(self._call_one)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/fastcore/foundation.py\", line 156, in map\n",
      "    def map(self, f, *args, **kwargs): return self._new(map_ex(self, f, *args, gen=False, **kwargs))\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/fastcore/basics.py\", line 840, in map_ex\n",
      "    return list(res)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/fastcore/basics.py\", line 825, in __call__\n",
      "    return self.func(*fargs, **kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/fastai/learner.py\", line 176, in _call_one\n",
      "    for cb in self.cbs.sorted('order'): cb(event_name)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/fastai/callback/core.py\", line 62, in __call__\n",
      "    except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/fastai/callback/core.py\", line 60, in __call__\n",
      "    try: res = getcallable(self, event_name)()\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/callbacks.py\", line 107, in after_fit\n",
      "    self.learn.load(f\"{self.fname}\", with_opt=self.with_opt)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/fastai/learner.py\", line 420, in load\n",
      "    load_model(file, self.model, self.opt, device=device, **kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/fastai/learner.py\", line 51, in load_model\n",
      "    state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/serialization.py\", line 771, in load\n",
      "    with _open_file_like(f, 'rb') as opened_file:\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/serialization.py\", line 270, in _open_file_like\n",
      "    return _open_file(name_or_buffer, mode)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/serialization.py\", line 251, in __init__\n",
      "    super(_open_file, self).__init__(open(name, mode))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/var/tmp/tmpltd6fy4h/models/model.pth'\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 6309.71s of the 6309.68s of remaining time.\n",
      "2023-10-05 10:47:07,002\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2023-10-05 10:47:07,004\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2023-10-05 10:47:07,017\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2023-10-05 10:47:07,019\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2023-10-05 10:47:07,021\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2023-10-05 10:47:07,022\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2023-10-05 10:47:07,024\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-50.7853\t = Validation score   (-mean_absolute_error)\n",
      "\t41.09s\t = Training   runtime\n",
      "\t1.66s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 6266.36s of the 6266.34s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-60.1549\t = Validation score   (-mean_absolute_error)\n",
      "\t150.97s\t = Training   runtime\n",
      "\t1.27s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 6113.34s of the 6113.31s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-48.1664\t = Validation score   (-mean_absolute_error)\n",
      "\t349.96s\t = Training   runtime\n",
      "\t178.49s\t = Validation runtime\n",
      "Repeating k-fold bagging: 2/20\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 5735.79s of the 5735.76s of remaining time.\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-48.1086\t = Validation score   (-mean_absolute_error)\n",
      "\t257.04s\t = Training   runtime\n",
      "\t256.67s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 5587.16s of the 5587.14s of remaining time.\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-48.9595\t = Validation score   (-mean_absolute_error)\n",
      "\t183.54s\t = Training   runtime\n",
      "\t25.42s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 5496.47s of the 5496.45s of remaining time.\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-49.7919\t = Validation score   (-mean_absolute_error)\n",
      "\t1549.7s\t = Training   runtime\n",
      "\t0.67s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 4706.91s of the 4706.89s of remaining time.\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-49.82\t = Validation score   (-mean_absolute_error)\n",
      "\t103.82s\t = Training   runtime\n",
      "\t4.45s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 4641.73s of the 4641.7s of remaining time.\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-58.9766\t = Validation score   (-mean_absolute_error)\n",
      "\t292.4s\t = Training   runtime\n",
      "\t2.54s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 4498.22s of the 4498.2s of remaining time.\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-47.4441\t = Validation score   (-mean_absolute_error)\n",
      "\t707.04s\t = Training   runtime\n",
      "\t356.0s\t = Validation runtime\n",
      "Repeating k-fold bagging: 3/20\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 4115.63s of the 4115.6s of remaining time.\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-47.7616\t = Validation score   (-mean_absolute_error)\n",
      "\t388.68s\t = Training   runtime\n",
      "\t381.01s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 3966.64s of the 3966.62s of remaining time.\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-48.6744\t = Validation score   (-mean_absolute_error)\n",
      "\t247.62s\t = Training   runtime\n",
      "\t34.24s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 3897.86s of the 3897.83s of remaining time.\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-49.4277\t = Validation score   (-mean_absolute_error)\n",
      "\t2338.57s\t = Training   runtime\n",
      "\t1.02s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 3107.18s of the 3107.15s of remaining time.\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-49.5088\t = Validation score   (-mean_absolute_error)\n",
      "\t152.03s\t = Training   runtime\n",
      "\t6.31s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 3056.75s of the 3056.73s of remaining time.\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-58.5595\t = Validation score   (-mean_absolute_error)\n",
      "\t441.78s\t = Training   runtime\n",
      "\t3.81s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 2905.42s of the 2905.39s of remaining time.\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-47.1313\t = Validation score   (-mean_absolute_error)\n",
      "\t1052.44s\t = Training   runtime\n",
      "\t533.78s\t = Validation runtime\n",
      "Repeating k-fold bagging: 4/20\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 2531.55s of the 2531.52s of remaining time.\n",
      "\tFitting 8 child models (S4F1 - S4F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-47.588\t = Validation score   (-mean_absolute_error)\n",
      "\t519.86s\t = Training   runtime\n",
      "\t508.71s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 2381.01s of the 2380.98s of remaining time.\n",
      "\tFitting 8 child models (S4F1 - S4F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-48.4458\t = Validation score   (-mean_absolute_error)\n",
      "\t339.2s\t = Training   runtime\n",
      "\t47.04s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 2283.29s of the 2283.27s of remaining time.\n",
      "\tFitting 8 child models (S4F1 - S4F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-49.225\t = Validation score   (-mean_absolute_error)\n",
      "\t3062.76s\t = Training   runtime\n",
      "\t1.34s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 1557.15s of the 1557.12s of remaining time.\n",
      "\tFitting 8 child models (S4F1 - S4F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-49.2819\t = Validation score   (-mean_absolute_error)\n",
      "\t201.62s\t = Training   runtime\n",
      "\t8.58s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 1505.39s of the 1505.37s of remaining time.\n",
      "\tFitting 8 child models (S4F1 - S4F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-58.3628\t = Validation score   (-mean_absolute_error)\n",
      "\t584.27s\t = Training   runtime\n",
      "\t5.18s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 1360.81s of the 1360.78s of remaining time.\n",
      "\tFitting 8 child models (S4F1 - S4F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-46.9923\t = Validation score   (-mean_absolute_error)\n",
      "\t1408.72s\t = Training   runtime\n",
      "\t707.37s\t = Validation runtime\n",
      "Completed 4/20 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 756.44s of the 979.83s of remaining time.\n",
      "\t-46.4558\t = Validation score   (-mean_absolute_error)\n",
      "\t1.22s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 13421.47s ... Best model: \"WeightedEnsemble_L3\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20231005_083216/\")\n"
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "train_data = TabularDataset('X_train.csv')\n",
    "test_data = TabularDataset('X_test_raw.csv')\n",
    "label = 'y'\n",
    "\n",
    "# mean absolute error\n",
    "metric = 'mean_absolute_error'\n",
    "time_limit = 60*60*4\n",
    "predictor = TabularPredictor(label=label, eval_metric=metric).fit(train_data, time_limit=time_limit, presets='best_quality')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          prediction   id\n",
      "location                 \n",
      "A                720  720\n",
      "B                720  720\n",
      "C                720  720\n"
     ]
    }
   ],
   "source": [
    "# pull in test.csv and create submission.csv\n",
    "submission_ids_df = pd.read_csv('test.csv', index_col=0)\n",
    "# submission_ids_df has Id, Time, Location\n",
    "# X_test has location_A, location_B, location_C\n",
    "# we have to make sure that submission_ids_df has same dates as X_test, then predict y for each row in submission_ids_df, then save as submission.csv (using correct Id from submission_ids_df)\n",
    "\n",
    "# convert index to datetime\n",
    "submission_ids_df[\"id\"] = submission_ids_df.index\n",
    "submission_ids_df.set_index('time', inplace=True)\n",
    "submission_ids_df.index = pd.to_datetime(submission_ids_df.index).astype('datetime64[ns]')\n",
    "submission_ids_df\n",
    "\n",
    "# split submission_ids_df into each location\n",
    "print(submission_ids_df.groupby('location').count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded data from: X_test_raw.csv | Columns = 48 / 48 | Rows = 4608 -> 4608\n",
      "Loaded data from: test.csv | Columns = 4 / 4 | Rows = 2160 -> 2160\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>absolute_humidity_2m:gm3</th>\n",
       "      <th>air_density_2m:kgm3</th>\n",
       "      <th>ceiling_height_agl:m</th>\n",
       "      <th>clear_sky_energy_1h:J</th>\n",
       "      <th>clear_sky_rad:W</th>\n",
       "      <th>cloud_base_agl:m</th>\n",
       "      <th>dew_or_rime:idx</th>\n",
       "      <th>dew_point_2m:K</th>\n",
       "      <th>diffuse_rad:W</th>\n",
       "      <th>...</th>\n",
       "      <th>visibility:m</th>\n",
       "      <th>wind_speed_10m:ms</th>\n",
       "      <th>wind_speed_u_10m:ms</th>\n",
       "      <th>wind_speed_v_10m:ms</th>\n",
       "      <th>wind_speed_w_1000hPa:ms</th>\n",
       "      <th>estimated_diff_hours</th>\n",
       "      <th>location</th>\n",
       "      <th>id</th>\n",
       "      <th>time</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-05-01 00:00:00</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.286</td>\n",
       "      <td>912.700012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1041.199951</td>\n",
       "      <td>0.0</td>\n",
       "      <td>271.700012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>30210.699219</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>16.998889</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-05-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-05-01 01:00:00</td>\n",
       "      <td>4.3</td>\n",
       "      <td>1.287</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1095.400024</td>\n",
       "      <td>0.0</td>\n",
       "      <td>271.600006</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>29507.500000</td>\n",
       "      <td>3.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.3</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>17.998889</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-05-01 01:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-05-01 02:00:00</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.284</td>\n",
       "      <td>1482.099976</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1041.300049</td>\n",
       "      <td>0.0</td>\n",
       "      <td>271.200012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>29463.099609</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.8</td>\n",
       "      <td>3.2</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>18.998889</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-05-01 02:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-05-01 03:00:00</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.282</td>\n",
       "      <td>2306.699951</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1465.599976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>270.799988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>33727.101562</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.6</td>\n",
       "      <td>3.2</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>19.998889</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-05-01 03:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-05-01 04:00:00</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.282</td>\n",
       "      <td>2323.199951</td>\n",
       "      <td>59774.500000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>703.599976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>270.299988</td>\n",
       "      <td>31.400000</td>\n",
       "      <td>...</td>\n",
       "      <td>35927.601562</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.3</td>\n",
       "      <td>3.1</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>20.998889</td>\n",
       "      <td>A</td>\n",
       "      <td>4</td>\n",
       "      <td>2023-05-01 04:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2155</th>\n",
       "      <td>2023-07-03 19:00:00</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1.196</td>\n",
       "      <td>3532.399902</td>\n",
       "      <td>615338.812500</td>\n",
       "      <td>117.199997</td>\n",
       "      <td>2239.600098</td>\n",
       "      <td>0.0</td>\n",
       "      <td>281.600006</td>\n",
       "      <td>41.299999</td>\n",
       "      <td>...</td>\n",
       "      <td>41536.398438</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1.9</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.991389</td>\n",
       "      <td>C</td>\n",
       "      <td>2155</td>\n",
       "      <td>2023-07-03 19:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2156</th>\n",
       "      <td>2023-07-03 20:00:00</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1.199</td>\n",
       "      <td>3429.000000</td>\n",
       "      <td>269582.406250</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>1513.699951</td>\n",
       "      <td>0.0</td>\n",
       "      <td>281.899994</td>\n",
       "      <td>19.700001</td>\n",
       "      <td>...</td>\n",
       "      <td>40136.500000</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.991389</td>\n",
       "      <td>C</td>\n",
       "      <td>2156</td>\n",
       "      <td>2023-07-03 20:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2157</th>\n",
       "      <td>2023-07-03 21:00:00</td>\n",
       "      <td>8.8</td>\n",
       "      <td>1.202</td>\n",
       "      <td>2495.000000</td>\n",
       "      <td>71999.601562</td>\n",
       "      <td>4.900000</td>\n",
       "      <td>1342.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>282.200012</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>43266.101562</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.2</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.991389</td>\n",
       "      <td>C</td>\n",
       "      <td>2157</td>\n",
       "      <td>2023-07-03 21:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2158</th>\n",
       "      <td>2023-07-03 22:00:00</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.206</td>\n",
       "      <td>1997.400024</td>\n",
       "      <td>1378.300049</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1878.900024</td>\n",
       "      <td>0.0</td>\n",
       "      <td>282.600006</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>39017.898438</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.991389</td>\n",
       "      <td>C</td>\n",
       "      <td>2158</td>\n",
       "      <td>2023-07-03 22:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2159</th>\n",
       "      <td>2023-07-03 23:00:00</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.207</td>\n",
       "      <td>2005.599976</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1471.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>282.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>39026.000000</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.6</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.991389</td>\n",
       "      <td>C</td>\n",
       "      <td>2159</td>\n",
       "      <td>2023-07-03 23:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2160 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       ds  absolute_humidity_2m:gm3  air_density_2m:kgm3  \\\n",
       "0     2023-05-01 00:00:00                       4.4                1.286   \n",
       "1     2023-05-01 01:00:00                       4.3                1.287   \n",
       "2     2023-05-01 02:00:00                       4.2                1.284   \n",
       "3     2023-05-01 03:00:00                       4.1                1.282   \n",
       "4     2023-05-01 04:00:00                       3.9                1.282   \n",
       "...                   ...                       ...                  ...   \n",
       "2155  2023-07-03 19:00:00                       8.3                1.196   \n",
       "2156  2023-07-03 20:00:00                       8.5                1.199   \n",
       "2157  2023-07-03 21:00:00                       8.8                1.202   \n",
       "2158  2023-07-03 22:00:00                       9.0                1.206   \n",
       "2159  2023-07-03 23:00:00                       9.0                1.207   \n",
       "\n",
       "      ceiling_height_agl:m  clear_sky_energy_1h:J  clear_sky_rad:W  \\\n",
       "0               912.700012               0.000000         0.000000   \n",
       "1                      NaN               0.000000         0.000000   \n",
       "2              1482.099976               0.000000         0.000000   \n",
       "3              2306.699951               0.000000         0.000000   \n",
       "4              2323.199951           59774.500000        43.000000   \n",
       "...                    ...                    ...              ...   \n",
       "2155           3532.399902          615338.812500       117.199997   \n",
       "2156           3429.000000          269582.406250        40.000000   \n",
       "2157           2495.000000           71999.601562         4.900000   \n",
       "2158           1997.400024            1378.300049         0.000000   \n",
       "2159           2005.599976               0.000000         0.000000   \n",
       "\n",
       "      cloud_base_agl:m  dew_or_rime:idx  dew_point_2m:K  diffuse_rad:W  ...  \\\n",
       "0          1041.199951              0.0      271.700012       0.000000  ...   \n",
       "1          1095.400024              0.0      271.600006       0.000000  ...   \n",
       "2          1041.300049              0.0      271.200012       0.000000  ...   \n",
       "3          1465.599976              0.0      270.799988       0.000000  ...   \n",
       "4           703.599976              0.0      270.299988      31.400000  ...   \n",
       "...                ...              ...             ...            ...  ...   \n",
       "2155       2239.600098              0.0      281.600006      41.299999  ...   \n",
       "2156       1513.699951              0.0      281.899994      19.700001  ...   \n",
       "2157       1342.500000              0.0      282.200012       5.000000  ...   \n",
       "2158       1878.900024              0.0      282.600006       0.000000  ...   \n",
       "2159       1471.000000              0.0      282.500000       0.000000  ...   \n",
       "\n",
       "      visibility:m  wind_speed_10m:ms  wind_speed_u_10m:ms  \\\n",
       "0     30210.699219                4.0                  2.2   \n",
       "1     29507.500000                3.9                  2.0   \n",
       "2     29463.099609                3.7                  1.8   \n",
       "3     33727.101562                3.6                  1.6   \n",
       "4     35927.601562                3.4                  1.3   \n",
       "...            ...                ...                  ...   \n",
       "2155  41536.398438                2.2                  1.9   \n",
       "2156  40136.500000                2.1                  1.9   \n",
       "2157  43266.101562                2.4                  2.2   \n",
       "2158  39017.898438                2.0                  1.8   \n",
       "2159  39026.000000                1.7                  1.6   \n",
       "\n",
       "      wind_speed_v_10m:ms  wind_speed_w_1000hPa:ms  estimated_diff_hours  \\\n",
       "0                     3.4                     -0.0             16.998889   \n",
       "1                     3.3                     -0.0             17.998889   \n",
       "2                     3.2                     -0.0             18.998889   \n",
       "3                     3.2                     -0.0             19.998889   \n",
       "4                     3.1                     -0.0             20.998889   \n",
       "...                   ...                      ...                   ...   \n",
       "2155                 -1.2                      0.0             35.991389   \n",
       "2156                 -0.9                      0.0             36.991389   \n",
       "2157                 -1.0                      0.0             37.991389   \n",
       "2158                 -0.8                      0.0             38.991389   \n",
       "2159                 -0.7                      0.0             39.991389   \n",
       "\n",
       "      location    id                 time  prediction  \n",
       "0            A     0  2023-05-01 00:00:00           0  \n",
       "1            A     1  2023-05-01 01:00:00           0  \n",
       "2            A     2  2023-05-01 02:00:00           0  \n",
       "3            A     3  2023-05-01 03:00:00           0  \n",
       "4            A     4  2023-05-01 04:00:00           0  \n",
       "...        ...   ...                  ...         ...  \n",
       "2155         C  2155  2023-07-03 19:00:00           0  \n",
       "2156         C  2156  2023-07-03 20:00:00           0  \n",
       "2157         C  2157  2023-07-03 21:00:00           0  \n",
       "2158         C  2158  2023-07-03 22:00:00           0  \n",
       "2159         C  2159  2023-07-03 23:00:00           0  \n",
       "\n",
       "[2160 rows x 51 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming X_test and model are already defined\n",
    "# Assuming 'time' is also a datetime64[ns] column in X_test\n",
    "\n",
    "\n",
    "test_data = TabularDataset('X_test_raw.csv')\n",
    "test_ids = TabularDataset('test.csv')\n",
    "# merge test_data with test_ids\n",
    "test_data = pd.merge(test_data, test_ids, how=\"inner\", right_on=[\"time\", \"location\"], left_on=[\"ds\", \"location\"])\n",
    "\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "y_pred = predictor.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0.049316\n",
       "1        0.054726\n",
       "2        0.315490\n",
       "3       13.609836\n",
       "4       54.420658\n",
       "          ...    \n",
       "2155    24.931078\n",
       "2156    21.207546\n",
       "2157    17.164587\n",
       "2158     9.467060\n",
       "2159     6.092224\n",
       "Name: y, Length: 2160, dtype: float32"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.049316</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.054726</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.315490</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.609836</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54.420658</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2155</th>\n",
       "      <td>24.931078</td>\n",
       "      <td>2155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2156</th>\n",
       "      <td>21.207546</td>\n",
       "      <td>2156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2157</th>\n",
       "      <td>17.164587</td>\n",
       "      <td>2157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2158</th>\n",
       "      <td>9.467060</td>\n",
       "      <td>2158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2159</th>\n",
       "      <td>6.092224</td>\n",
       "      <td>2159</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2160 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      prediction    id\n",
       "0       0.049316     0\n",
       "1       0.054726     1\n",
       "2       0.315490     2\n",
       "3      13.609836     3\n",
       "4      54.420658     4\n",
       "...          ...   ...\n",
       "2155   24.931078  2155\n",
       "2156   21.207546  2156\n",
       "2157   17.164587  2157\n",
       "2158    9.467060  2158\n",
       "2159    6.092224  2159\n",
       "\n",
       "[2160 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save y_pred series to csv as \"id\", \"prediction\"\n",
    "y_pred_df = pd.DataFrame(y_pred)\n",
    "y_pred_df.rename(columns={\"y\": \"prediction\"}, inplace=True)\n",
    "y_pred_df[\"id\"] = y_pred_df.index\n",
    "y_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last submission number: 62\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Save the submission DataFrame to submissions folder, create new name based on last submission, format is submission_<last_submission_number + 1>.csv\n",
    "# Get the last submission number\n",
    "last_submission_number = int(max([int(filename.split('_')[1].split('.')[0]) for filename in os.listdir('submissions') if \"submission\" in filename]))\n",
    "print(\"Last submission number:\", last_submission_number)\n",
    "\n",
    "# Create the new filename\n",
    "new_filename = f'submission_{last_submission_number + 1}.csv'\n",
    "\n",
    "# Save the submission\n",
    "y_pred_df.to_csv(os.path.join('submissions', new_filename), index=False)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m111"
  },
  "kernelspec": {
   "display_name": "ag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
