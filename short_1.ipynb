{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install neccessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: autogluon in /opt/homebrew/anaconda3/envs/ag/lib/python3.10/site-packages (0.8.2)\n",
      "Requirement already satisfied: matplotlib in /opt/homebrew/anaconda3/envs/ag/lib/python3.10/site-packages (3.6.3)\n",
      "Collecting autogluon.core[all]==0.8.2 (from autogluon)\n",
      "  Obtaining dependency information for autogluon.core[all]==0.8.2 from https://files.pythonhosted.org/packages/e0/56/545adb1d388e78591cd7e36de0c8b889c1944de362bdaeec0f31d01890df/autogluon.core-0.8.2-py3-none-any.whl.metadata\n",
      "  Downloading autogluon.core-0.8.2-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting autogluon.features==0.8.2 (from autogluon)\n",
      "  Obtaining dependency information for autogluon.features==0.8.2 from https://files.pythonhosted.org/packages/bb/ea/7892719f78a30aee1bf42c4a0540fbae98bfbdf56b85fab79ffc437eb687/autogluon.features-0.8.2-py3-none-any.whl.metadata\n",
      "  Downloading autogluon.features-0.8.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting autogluon.tabular[all]==0.8.2 (from autogluon)\n",
      "  Obtaining dependency information for autogluon.tabular[all]==0.8.2 from https://files.pythonhosted.org/packages/f3/dc/0bd8cadb9a5e2f3e5b12caaa6745357d912ffc7b8b75fb4e426a38331028/autogluon.tabular-0.8.2-py3-none-any.whl.metadata\n",
      "  Downloading autogluon.tabular-0.8.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting autogluon.multimodal==0.8.2 (from autogluon)\n",
      "  Obtaining dependency information for autogluon.multimodal==0.8.2 from https://files.pythonhosted.org/packages/f4/df/a3921edb866555154d5a53adbcd2268fd3b53071820b6382f2619ad439f3/autogluon.multimodal-0.8.2-py3-none-any.whl.metadata\n",
      "  Downloading autogluon.multimodal-0.8.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting autogluon.timeseries[all]==0.8.2 (from autogluon)\n",
      "  Obtaining dependency information for autogluon.timeseries[all]==0.8.2 from https://files.pythonhosted.org/packages/50/38/1768d30684292d064dad4457355d6928581c5b832a3f65a6a023df2ff4ff/autogluon.timeseries-0.8.2-py3-none-any.whl.metadata\n",
      "  Downloading autogluon.timeseries-0.8.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy<1.27,>=1.21 in /opt/homebrew/anaconda3/envs/ag/lib/python3.10/site-packages (from autogluon.core[all]==0.8.2->autogluon) (1.24.4)\n",
      "Requirement already satisfied: scipy<1.12,>=1.5.4 in /opt/homebrew/anaconda3/envs/ag/lib/python3.10/site-packages (from autogluon.core[all]==0.8.2->autogluon) (1.11.3)\n",
      "Requirement already satisfied: scikit-learn<1.3,>=1.0 in /opt/homebrew/anaconda3/envs/ag/lib/python3.10/site-packages (from autogluon.core[all]==0.8.2->autogluon) (1.2.2)\n",
      "Requirement already satisfied: networkx<4,>=3.0 in /opt/homebrew/anaconda3/envs/ag/lib/python3.10/site-packages (from autogluon.core[all]==0.8.2->autogluon) (3.1)\n",
      "Requirement already satisfied: pandas<1.6,>=1.4.1 in /opt/homebrew/anaconda3/envs/ag/lib/python3.10/site-packages (from autogluon.core[all]==0.8.2->autogluon) (1.5.3)\n",
      "Requirement already satisfied: tqdm<5,>=4.38 in /opt/homebrew/anaconda3/envs/ag/lib/python3.10/site-packages (from autogluon.core[all]==0.8.2->autogluon) (4.66.1)\n",
      "Requirement already satisfied: requests in /opt/homebrew/anaconda3/envs/ag/lib/python3.10/site-packages (from autogluon.core[all]==0.8.2->autogluon) (2.31.0)\n",
      "Requirement already satisfied: boto3<2,>=1.10 in /opt/homebrew/anaconda3/envs/ag/lib/python3.10/site-packages (from autogluon.core[all]==0.8.2->autogluon) (1.28.60)\n",
      "Collecting autogluon.common==0.8.2 (from autogluon.core[all]==0.8.2->autogluon)\n",
      "  Obtaining dependency information for autogluon.common==0.8.2 from https://files.pythonhosted.org/packages/67/c7/aa2bd5708c9329a29245aa0cd955cace0662b92a82eb7605cf46b6e7e9d5/autogluon.common-0.8.2-py3-none-any.whl.metadata\n",
      "  Downloading autogluon.common-0.8.2-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: hyperopt<0.2.8,>=0.2.7 in /opt/homebrew/anaconda3/envs/ag/lib/python3.10/site-packages (from autogluon.core[all]==0.8.2->autogluon) (0.2.7)\n",
      "Collecting ray[default]<2.4,>=2.3 (from autogluon.core[all]==0.8.2->autogluon)\n",
      "  Downloading ray-2.3.1-cp310-cp310-macosx_11_0_arm64.whl (28.6 MB)\n",
      "\u001b[2K     \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━\u001b[0m \u001b[32m27.3/28.6 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m^C\n",
      "\u001b[2K     \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m \u001b[32m28.2/28.6 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install autogluon matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "\n",
    "label = 'y'\n",
    "metric = 'mean_absolute_error'\n",
    "time_limit = 60*60\n",
    "presets = \"experimental_zeroshot_hpo_hybrid\"#'best_quality'\n",
    "\n",
    "use_is_estimated_attr = True\n",
    "\n",
    "drop_night_outliers = True\n",
    "\n",
    "# to_drop = [\"snow_drift:idx\", \"snow_density:kgm3\", \"wind_speed_w_1000hPa:ms\", \"dew_or_rime:idx\", \"prob_rime:p\", \"fresh_snow_12h:cm\", \"fresh_snow_24h:cm\", \"wind_speed_u_10m:ms\", \"wind_speed_v_10m:ms\", \"snow_melt_10min:mm\", \"rain_water:kgm2\", \"dew_point_2m:K\", \"precip_5min:mm\", \"absolute_humidity_2m:gm3\", \"air_density_2m:kgm3\"]#, \"msl_pressure:hPa\", \"pressure_50m:hPa\",  \"pressure_100m:hPa\"]\n",
    "to_drop = [\"wind_speed_w_1000hPa:ms\", \"wind_speed_u_10m:ms\", \"wind_speed_v_10m:ms\"]\n",
    "\n",
    "excluded_model_types = ['CAT', 'XGB', 'RF']\n",
    "\n",
    "num_stack_levels = 0\n",
    "num_bag_folds = None# 8\n",
    "num_bag_sets = None#20\n",
    "\n",
    "use_tune_data = True\n",
    "use_test_data = True\n",
    "use_bag_holdout = True \n",
    "\n",
    "clip_predictions = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing location A...\n",
      "Processing location B...\n",
      "Processing location C...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def feature_engineering(X):\n",
    "    # shift columns with them by 1 hour, so that for index 16:00, we have the values from 17:00\n",
    "    columns = ['clear_sky_energy_1h:J', 'diffuse_rad_1h:J', 'direct_rad_1h:J',\n",
    "               'fresh_snow_12h:cm', 'fresh_snow_1h:cm', 'fresh_snow_24h:cm',\n",
    "               'fresh_snow_3h:cm', 'fresh_snow_6h:cm']\n",
    "\n",
    "    # Filter rows where index.minute == 0\n",
    "    X_shifted = X[X.index.minute == 0][columns].copy()\n",
    "\n",
    "    # Create a set for constant-time lookup\n",
    "    index_set = set(X.index)\n",
    "\n",
    "    # Vectorized time shifting\n",
    "    one_hour = pd.Timedelta('1 hour')\n",
    "    shifted_indices = X_shifted.index #+ one_hour\n",
    "    X_shifted.loc[shifted_indices.isin(index_set)] = X.loc[shifted_indices[shifted_indices.isin(index_set)]][columns]\n",
    "\n",
    "    # set last row to same as second last row\n",
    "    X_shifted.iloc[-1] = X_shifted.iloc[-2]\n",
    "\n",
    "\n",
    "    # Rename columns\n",
    "    X_old_unshifted = X_shifted.copy()\n",
    "    X_old_unshifted.columns = [f\"{col}_not_shifted\" for col in X_old_unshifted.columns]\n",
    "    \n",
    "    date_calc = None\n",
    "    # If 'date_calc' is present, handle it\n",
    "    if 'date_calc' in X.columns:\n",
    "        date_calc = X[X.index.minute == 0]['date_calc']\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "    # resample to hourly\n",
    "    X = X.resample('H').mean()\n",
    "\n",
    "    # overwrite columns with shifted columns\n",
    "    X[columns] = X_shifted[columns]\n",
    "\n",
    "    if date_calc is not None:\n",
    "        X['date_calc'] = date_calc\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fix_X(X, name):\n",
    "    # Convert 'date_forecast' to datetime format and replace original column with 'ds'\n",
    "    X['ds'] = pd.to_datetime(X['date_forecast'])\n",
    "    X.drop(columns=['date_forecast'], inplace=True, errors='ignore')\n",
    "    X.sort_values(by='ds', inplace=True)\n",
    "    X.set_index('ds', inplace=True)\n",
    "\n",
    "    \n",
    "    X = feature_engineering(X)\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "\n",
    "def handle_features(X_train_observed, X_train_estimated, X_test, y_train):\n",
    "    X_train_observed = fix_X(X_train_observed, \"X_train_observed\")\n",
    "    X_train_estimated = fix_X(X_train_estimated, \"X_train_estimated\")\n",
    "    X_test = fix_X(X_test, \"X_test\")\n",
    "\n",
    "\n",
    "    y_train['ds'] = pd.to_datetime(y_train['time'])\n",
    "    y_train.drop(columns=['time'], inplace=True)\n",
    "    y_train.sort_values(by='ds', inplace=True)\n",
    "    y_train.set_index('ds', inplace=True)\n",
    "\n",
    "    return X_train_observed, X_train_estimated, X_test, y_train\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_data(X_train_observed, X_train_estimated, X_test, y_train, location):\n",
    "    # convert to datetime\n",
    "    X_train_observed, X_train_estimated, X_test, y_train = handle_features(X_train_observed, X_train_estimated, X_test, y_train)\n",
    "\n",
    "    if use_is_estimated_attr:\n",
    "        X_train_observed[\"is_estimated\"] = 0\n",
    "        X_train_estimated[\"is_estimated\"] = 1\n",
    "        X_test[\"is_estimated\"] = 1\n",
    "\n",
    "    # drop date_calc\n",
    "    X_train_estimated.drop(columns=['date_calc'], inplace=True)\n",
    "    X_test.drop(columns=['date_calc'], inplace=True)\n",
    "\n",
    "\n",
    "    y_train[\"y\"] = y_train[\"pv_measurement\"].astype('float64')\n",
    "    y_train.drop(columns=['pv_measurement'], inplace=True)\n",
    "    X_train = pd.concat([X_train_observed, X_train_estimated])\n",
    "    \n",
    "    # clip all y values to 0 if negative\n",
    "    y_train[\"y\"] = y_train[\"y\"].clip(lower=0)\n",
    "    \n",
    "    X_train = pd.merge(X_train, y_train, how=\"inner\", left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "    X_train[\"location\"] = location\n",
    "    X_test[\"location\"] = location\n",
    "    \n",
    "    return X_train, X_test\n",
    "\n",
    "# Define locations\n",
    "locations = ['A', 'B', 'C']\n",
    "\n",
    "X_trains = []\n",
    "X_tests = []\n",
    "# Loop through locations\n",
    "for loc in locations:\n",
    "    print(f\"Processing location {loc}...\")\n",
    "    # Read target training data\n",
    "    y_train = pd.read_parquet(f'{loc}/train_targets.parquet')\n",
    "    \n",
    "    # Read estimated training data and add location feature\n",
    "    X_train_estimated = pd.read_parquet(f'{loc}/X_train_estimated.parquet')\n",
    "    \n",
    "    # Read observed training data and add location feature\n",
    "    X_train_observed= pd.read_parquet(f'{loc}/X_train_observed.parquet')\n",
    "\n",
    "    # Read estimated test data and add location feature\n",
    "    X_test_estimated = pd.read_parquet(f'{loc}/X_test_estimated.parquet')\n",
    "\n",
    "    # Preprocess data\n",
    "    X_train, X_test = preprocess_data(X_train_observed, X_train_estimated, X_test_estimated, y_train, loc)\n",
    "\n",
    "    X_trains.append(X_train)\n",
    "    X_tests.append(X_test)\n",
    "\n",
    "# Concatenate all data and save to csv\n",
    "X_train = pd.concat(X_trains)\n",
    "X_test = pd.concat(X_tests)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Feature enginering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found streaks for location A: {}\n",
      "Found streaks for location B: {3.45: 28, 6.9: 7, 12.9375: 5, 13.8: 8, 276.0: 78, 18.975: 58, 0.8625: 4, 118.1625: 33, 34.5: 11, 183.7125: 1058, 87.1125: 7, 79.35: 34, 7.7625: 12, 27.6: 448, 273.41249999999997: 72, 264.78749999999997: 55, 169.05: 33, 375.1875: 56, 314.8125: 66, 76.7625: 10, 135.4125: 216, 81.9375: 202, 2.5875: 12, 81.075: 210}\n",
      "Found streaks for location C: {9.8: 4, 29.400000000000002: 4, 19.6: 4}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def replace_streaks_with_nan(df, max_streak_length, column=\"y\"):\n",
    "    for location in df[\"location\"].unique():\n",
    "        x = df[df[\"location\"] == location][column].copy()\n",
    "\n",
    "        last_val = None\n",
    "        streak_length = 1\n",
    "        streak_indices = []\n",
    "        allowed = [0]\n",
    "        found_streaks = {}\n",
    "\n",
    "        for idx in x.index:\n",
    "            value = x[idx]\n",
    "\n",
    "            if value == last_val and value not in allowed:\n",
    "                streak_length += 1\n",
    "                streak_indices.append(idx)\n",
    "            else:\n",
    "                streak_length = 1\n",
    "                last_val = value\n",
    "                streak_indices.clear()\n",
    "\n",
    "            if streak_length > max_streak_length:\n",
    "                found_streaks[value] = streak_length\n",
    "\n",
    "                for streak_idx in streak_indices:\n",
    "                    x[idx] = np.nan\n",
    "                streak_indices.clear()  # clear after setting to NaN to avoid setting multiple times\n",
    "        df.loc[df[\"location\"] == location, column] = x\n",
    "\n",
    "        print(f\"Found streaks for location {location}: {found_streaks}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "X_train = replace_streaks_with_nan(X_train.copy(), 3, \"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped rows:  9285\n"
     ]
    }
   ],
   "source": [
    "# print num rows\n",
    "temprows = len(X_train)\n",
    "X_train.dropna(subset=['y', 'direct_rad_1h:J', 'diffuse_rad_1h:J'], inplace=True)\n",
    "print(\"Dropped rows: \", temprows - len(X_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 0.1\n",
    "mask = (X_train[\"direct_rad_1h:J\"] <= thresh) & (X_train[\"diffuse_rad_1h:J\"] <= thresh) & (X_train[\"y\"] >= 0.1)\n",
    "\n",
    "if drop_night_outliers:\n",
    "    X_train.loc[mask, \"y\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped rows:  1876\n"
     ]
    }
   ],
   "source": [
    "temprows = len(X_train)\n",
    "X_train.dropna(subset=['y', 'direct_rad_1h:J', 'diffuse_rad_1h:J'], inplace=True)\n",
    "print(\"Dropped rows: \", temprows - len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop(columns=to_drop, inplace=True)\n",
    "X_test.drop(columns=to_drop, inplace=True)\n",
    "\n",
    "X_train.to_csv('X_train_raw.csv', index=True)\n",
    "X_test.to_csv('X_test_raw.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_shuffle_data(input_data, num_bins, frac1):\n",
    "    # Validate the input fraction\n",
    "    if frac1 < 0 or frac1 > 1:\n",
    "        raise ValueError(\"frac1 must be between 0 and 1.\")\n",
    "    \n",
    "    if frac1==1:\n",
    "        return input_data, pd.DataFrame()\n",
    "\n",
    "    # Calculate the fraction for the second output set\n",
    "    frac2 = 1 - frac1\n",
    "    \n",
    "    # Calculate bin size\n",
    "    bin_size = len(input_data) // num_bins\n",
    "\n",
    "    # Initialize empty DataFrames for output\n",
    "    output_data1 = pd.DataFrame()\n",
    "    output_data2 = pd.DataFrame()\n",
    "\n",
    "    for i in range(num_bins):\n",
    "        # Shuffle the data in the current bin\n",
    "        np.random.seed(i)\n",
    "        current_bin = input_data.iloc[i * bin_size: (i + 1) * bin_size].sample(frac=1)\n",
    "        \n",
    "        # Calculate the sizes for each output set\n",
    "        size1 = int(len(current_bin) * frac1)\n",
    "        \n",
    "        # Split and append to output DataFrames\n",
    "        output_data1 = pd.concat([output_data1, current_bin.iloc[:size1]])\n",
    "        output_data2 = pd.concat([output_data2, current_bin.iloc[size1:]])\n",
    "\n",
    "    # Shuffle and split the remaining data\n",
    "    remaining_data = input_data.iloc[num_bins * bin_size:].sample(frac=1)\n",
    "\n",
    "    remaining_size1 = int(len(remaining_data) * frac1)\n",
    "    \n",
    "    output_data1 = pd.concat([output_data1, remaining_data.iloc[:remaining_size1]])\n",
    "    output_data2 = pd.concat([output_data2, remaining_data.iloc[remaining_size1:]])\n",
    "\n",
    "    return output_data1, output_data2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "data = TabularDataset('X_train_raw.csv')\n",
    "data['ds'] = pd.to_datetime(data['ds'])\n",
    "data = data.sort_values(by='ds')\n",
    "\n",
    "split_time = pd.to_datetime(\"2022-10-28 22:00:00\")\n",
    "train_set = TabularDataset(data[data[\"ds\"] < split_time])\n",
    "estimated_set = TabularDataset(data[data[\"ds\"] >= split_time]) # only estimated\n",
    "\n",
    "test_set = pd.DataFrame()\n",
    "tune_set = pd.DataFrame()\n",
    "new_train_set = pd.DataFrame()\n",
    "\n",
    "for location in locations:\n",
    "    loc_data = data[data[\"location\"] == location]\n",
    "    num_train_rows = len(loc_data)\n",
    "    \n",
    "    tune_rows = 1500.0 # 2500.0\n",
    "    if use_test_data:\n",
    "        tune_rows = 1880.0#max(3000.0, len(estimated_set[estimated_set[\"location\"] == location]))\n",
    "\n",
    "    holdout_frac = max(0.01, min(0.1, tune_rows / num_train_rows)) * num_train_rows / len(estimated_set[estimated_set[\"location\"] == location])\n",
    "\n",
    "    # shuffle and split data\n",
    "    loc_tune_set, loc_new_train_set = split_and_shuffle_data(estimated_set[estimated_set['location'] == location], 40, holdout_frac)\n",
    "\n",
    "    new_train_set = pd.concat([new_train_set, loc_new_train_set])\n",
    "\n",
    "    if use_test_data:\n",
    "        loc_test_set, loc_tune_set = split_and_shuffle_data(loc_tune_set, 40, 0.2)\n",
    "        test_set = pd.concat([test_set, loc_test_set])\n",
    "\n",
    "    tune_set = pd.concat([tune_set, loc_tune_set])\n",
    "\n",
    "# add rest to train_set\n",
    "train_set = pd.concat([train_set, new_train_set])\n",
    "tuning_data = tune_set\n",
    "if use_test_data:\n",
    "    test_data = test_set\n",
    "\n",
    "train_data = train_set\n",
    "\n",
    "train_data = TabularDataset(train_data)\n",
    "tuning_data = TabularDataset(tuning_data)\n",
    "\n",
    "if use_test_data:\n",
    "    test_data = TabularDataset(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last submission number: 132\n",
      "Now creating submission number: 133\n",
      "New filename: submission_133\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# if submissions folder does not exist, create it\n",
    "if not os.path.exists('submissions'):\n",
    "    os.makedirs('submissions')\n",
    "\n",
    "# Get the last submission number\n",
    "last_submission_number = int(max([int(filename.split('_')[1].split('.')[0]) for filename in os.listdir('submissions') if \"submission\" in filename]))\n",
    "print(\"Last submission number:\", last_submission_number)\n",
    "print(\"Now creating submission number:\", last_submission_number + 1)\n",
    "\n",
    "# Create the new filename\n",
    "new_filename = f'submission_{last_submission_number + 1}'\n",
    "\n",
    "print(\"New filename:\", new_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = [None, None, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Presets specified: ['experimental_zeroshot_hpo_hybrid']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=0, num_bag_folds=8, num_bag_sets=20\n",
      "Beginning AutoGluon training ... Time limit = 3600s\n",
      "AutoGluon will save models to \"AutogluonModels/submission_133_A\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.10.12\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 22.1.0: Sun Oct  9 20:15:09 PDT 2022; root:xnu-8792.41.9~2/RELEASE_ARM64_T6000\n",
      "Disk Space Avail:   115.18 GB / 494.38 GB (23.3%)\n",
      "Train Data Rows:    30936\n",
      "Train Data Columns: 43\n",
      "Tuning Data Rows:    1486\n",
      "Tuning Data Columns: 43\n",
      "Label Column: y\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (5733.42, 0.0, 673.20656, 1195.31332)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2492.39 MB\n",
      "\tTrain Data (Original)  Memory Usage: 12.77 MB (0.5% of available memory)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 3): ['elevation:m', 'snow_drift:idx', 'location']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 40 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 39 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', ...]\n",
      "\t\t('int', ['bool']) :  1 | ['snow_density:kgm3']\n",
      "\t0.2s = Fit runtime\n",
      "\t40 features in original data used to generate 40 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 10.15 MB (0.4% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.19s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "use_bag_holdout=True, will use tuning_data as holdout (will not be used for early stopping).\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}, {'min_samples_leaf': 1, 'max_leaf_nodes': 15000, 'max_features': 0.5, 'ag_args': {'name_suffix': '_r19', 'priority': 20}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}, {'min_samples_leaf': 5, 'max_leaf_nodes': 50000, 'max_features': 0.5, 'ag_args': {'name_suffix': '_r5', 'priority': 19}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge', {'extra_trees': False, 'feature_fraction': 0.7248284762542815, 'learning_rate': 0.07947286942946127, 'min_data_in_leaf': 50, 'num_leaves': 89, 'ag_args': {'name_suffix': '_r158', 'priority': 18}}, {'extra_trees': True, 'feature_fraction': 0.7832570544199176, 'learning_rate': 0.021720607471727896, 'min_data_in_leaf': 3, 'num_leaves': 21, 'ag_args': {'name_suffix': '_r118', 'priority': 17}}, {'extra_trees': True, 'feature_fraction': 0.7113010892989156, 'learning_rate': 0.012535427424259274, 'min_data_in_leaf': 16, 'num_leaves': 48, 'ag_args': {'name_suffix': '_r97', 'priority': 16}}, {'extra_trees': True, 'feature_fraction': 0.45555769907110816, 'learning_rate': 0.009591347321206594, 'min_data_in_leaf': 50, 'num_leaves': 110, 'ag_args': {'name_suffix': '_r71', 'priority': 15}}, {'extra_trees': False, 'feature_fraction': 0.40979710161022476, 'learning_rate': 0.008708890211023034, 'min_data_in_leaf': 3, 'num_leaves': 80, 'ag_args': {'name_suffix': '_r111', 'priority': 14}}],\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': [{}, {'bs': 1024, 'emb_drop': 0.6167722379778131, 'epochs': 44, 'layers': [200, 100, 50], 'lr': 0.053440377855629266, 'ps': 0.48477211305443607, 'ag_args': {'name_suffix': '_r25', 'priority': 13}}, {'bs': 1024, 'emb_drop': 0.6046989241462619, 'epochs': 48, 'layers': [200, 100, 50], 'lr': 0.00775309042164966, 'ps': 0.09244767444160731, 'ag_args': {'name_suffix': '_r51', 'priority': 12}}, {'bs': 512, 'emb_drop': 0.6557225316526186, 'epochs': 49, 'layers': [200, 100], 'lr': 0.023627682025564638, 'ps': 0.519566584552178, 'ag_args': {'name_suffix': '_r82', 'priority': 11}}, {'bs': 2048, 'emb_drop': 0.4066210919034579, 'epochs': 43, 'layers': [400, 200], 'lr': 0.0029598312717673434, 'ps': 0.4378695797438974, 'ag_args': {'name_suffix': '_r121', 'priority': 10}}, {'bs': 128, 'emb_drop': 0.44339037504795686, 'epochs': 31, 'layers': [400, 200, 100], 'lr': 0.008615195908919904, 'ps': 0.19220253419114286, 'ag_args': {'name_suffix': '_r145', 'priority': 9}}, {'bs': 128, 'emb_drop': 0.12106594798980945, 'epochs': 38, 'layers': [200, 100, 50], 'lr': 0.037991970245029975, 'ps': 0.33120008492595093, 'ag_args': {'name_suffix': '_r173', 'priority': 8}}, {'bs': 128, 'emb_drop': 0.4599138419358, 'epochs': 47, 'layers': [200, 100], 'lr': 0.03888383281136287, 'ps': 0.28193673177122863, 'ag_args': {'name_suffix': '_r128', 'priority': 7}}],\n",
      "\t'CAT': [{}, {'depth': 5, 'l2_leaf_reg': 4.774992314058497, 'learning_rate': 0.038551267822920274, 'ag_args': {'name_suffix': '_r16', 'priority': 6}}, {'depth': 4, 'l2_leaf_reg': 1.9950125740798321, 'learning_rate': 0.028091050379971633, 'ag_args': {'name_suffix': '_r42', 'priority': 5}}, {'depth': 6, 'l2_leaf_reg': 1.8298803017644376, 'learning_rate': 0.017844259810823604, 'ag_args': {'name_suffix': '_r93', 'priority': 4}}, {'depth': 7, 'l2_leaf_reg': 4.81099604606794, 'learning_rate': 0.019085060180573103, 'ag_args': {'name_suffix': '_r44', 'priority': 3}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Excluded models: ['RF', 'XGB', 'CAT'] (Specified by `excluded_model_types`)\n",
      "Fitting 21 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 3599.81s of the 3599.81s of remaining time.\n",
      "\t-186.9466\t = Validation score   (-mean_absolute_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t260.99s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 3314.15s of the 3314.15s of remaining time.\n"
     ]
    }
   ],
   "source": [
    "def fit_predictor_for_location(loc):\n",
    "    predictor = TabularPredictor(\n",
    "        label=label, \n",
    "        eval_metric=metric, \n",
    "        path=f\"AutogluonModels/{new_filename}_{loc}\", \n",
    "    ).fit(\n",
    "        train_data=train_data[train_data[\"location\"] == loc].drop(columns=[\"ds\"]),\n",
    "        time_limit=time_limit, \n",
    "        presets=presets, \n",
    "        num_stack_levels=num_stack_levels,\n",
    "        num_bag_folds=num_bag_folds,\n",
    "        num_bag_sets=num_bag_sets,\n",
    "        tuning_data=tuning_data[tuning_data[\"location\"] == loc].reset_index(drop=True).drop(columns=[\"ds\"]),\n",
    "        use_bag_holdout=use_bag_holdout,\n",
    "        excluded_model_types=excluded_model_types\n",
    "    )\n",
    "\n",
    "    # evaluate on test data\n",
    "    if use_test_data:\n",
    "        t = test_data[test_data[\"location\"] == loc]\n",
    "        perf = predictor.evaluate(t)\n",
    "        print(\"Evaluation on test data:\")\n",
    "        print(perf[predictor.eval_metric.name])\n",
    "\n",
    "    return predictor\n",
    "\n",
    "loc = \"A\"\n",
    "predictors[0] = fit_predictor_for_location(loc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "leaderboards = [None, None, None]\n",
    "def leaderboard_for_location(i, loc):\n",
    "    plt.scatter(train_data[(train_data[\"location\"] == loc) & (train_data[\"is_estimated\"]==True)][\"y\"].index, train_data[(train_data[\"location\"] == loc) & (train_data[\"is_estimated\"]==True)][\"y\"])\n",
    "    plt.scatter(tuning_data[tuning_data[\"location\"] == loc][\"y\"].index, tuning_data[tuning_data[\"location\"] == loc][\"y\"])\n",
    "    plt.title(\"Val and Train\")\n",
    "    plt.show()\n",
    "\n",
    "    if use_test_data:\n",
    "        lb = predictors[i].leaderboard(test_data[test_data[\"location\"] == loc])\n",
    "        lb[\"location\"] = loc\n",
    "        plt.scatter(test_data[test_data[\"location\"] == loc][\"y\"].index, test_data[test_data[\"location\"] == loc][\"y\"])\n",
    "        plt.title(\"Test\")\n",
    "        \n",
    "        return lb\n",
    "    \n",
    "    return pd.DataFrame()\n",
    "\n",
    "leaderboards[0] = leaderboard_for_location(0, loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = \"B\"\n",
    "predictors[1] = fit_predictor_for_location(loc)\n",
    "leaderboards[1] = leaderboard_for_location(1, loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = \"C\"\n",
    "predictors[2] = fit_predictor_for_location(loc)\n",
    "leaderboards[2] = leaderboard_for_location(2, loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save leaderboards to csv\n",
    "pd.concat(leaderboards).to_csv(f\"leaderboards/{new_filename}.csv\")\n",
    "\n",
    "\n",
    "for i in range(len(predictors)):\n",
    "    print(f\"Predictor {i}:\")\n",
    "    print(predictors[i].info()[\"model_info\"][\"WeightedEnsemble_L2\"][\"children_info\"][\"S1F1\"][\"model_weights\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "future_test_data = TabularDataset('X_test_raw.csv')\n",
    "future_test_data[\"ds\"] = pd.to_datetime(future_test_data[\"ds\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = TabularDataset('test.csv')\n",
    "test_ids[\"time\"] = pd.to_datetime(test_ids[\"time\"])\n",
    "# merge test_data with test_ids\n",
    "future_test_data_merged = pd.merge(future_test_data, test_ids, how=\"inner\", right_on=[\"time\", \"location\"], left_on=[\"ds\", \"location\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict, grouped by location\n",
    "predictions = []\n",
    "location_map = {\n",
    "    \"A\": 0,\n",
    "    \"B\": 1,\n",
    "    \"C\": 2\n",
    "}\n",
    "for loc, group in future_test_data.groupby('location'):\n",
    "    i = location_map[loc]\n",
    "    subset = future_test_data_merged[future_test_data_merged[\"location\"] == loc].reset_index(drop=True)\n",
    "    pred = predictors[i].predict(subset)\n",
    "    subset[\"prediction\"] = pred\n",
    "    predictions.append(subset)\n",
    "\n",
    "    # get past predictions\n",
    "    tuning_data.loc[tuning_data[\"location\"] == loc, \"prediction\"] =  predictors[i].predict(tuning_data[tuning_data[\"location\"] == loc])\n",
    "    if use_test_data:\n",
    "        # get predictions for local test_data\n",
    "        test_data.loc[test_data[\"location\"] == loc, \"prediction\"] =  predictors[i].predict(test_data[test_data[\"location\"] == loc])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for loc, idx in location_map.items():\n",
    "    fig, ax = plt.subplots(figsize=(20, 10))\n",
    "    # plot train data\n",
    "    train_data[train_data[\"location\"]==loc].plot(x='ds', y='y', ax=ax, label=\"train data\")\n",
    "    tuning_data[tuning_data[\"location\"]==loc].plot(x='ds', y='y', ax=ax, label=\"tune data\")\n",
    "    if use_test_data:\n",
    "        test_data[test_data[\"location\"]==loc].plot(x='ds', y='y', ax=ax, label=\"test data\")\n",
    "\n",
    "    # plot predictions\n",
    "    predictions[idx].plot(x='ds', y='prediction', ax=ax, label=\"predictions\")\n",
    "\n",
    "    # plot past predictions\n",
    "    tuning_data[tuning_data[\"location\"]==loc].plot(x='ds', y='prediction', ax=ax, label=\"past predictions tune\")\n",
    "    if use_test_data:\n",
    "        test_data[test_data[\"location\"]==loc].plot(x='ds', y='prediction', ax=ax, label=\"past predictions test\")\n",
    "\n",
    "\n",
    "    ax.set_title(f\"Predictions for location {loc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_predictions = [prediction.copy() for prediction in predictions]\n",
    "if clip_predictions:\n",
    "    # clip predictions smaller than 0 to 0\n",
    "    for pred in temp_predictions:\n",
    "        # print smallest prediction\n",
    "        print(\"Smallest prediction:\", pred[\"prediction\"].min())\n",
    "        pred.loc[pred[\"prediction\"] < 0, \"prediction\"] = 0\n",
    "        print(\"Smallest prediction after clipping:\", pred[\"prediction\"].min())\n",
    "\n",
    "\n",
    "# concatenate predictions\n",
    "submissions_df = pd.concat(temp_predictions)\n",
    "submissions_df = submissions_df[[\"id\", \"prediction\"]]\n",
    "submissions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the submission\n",
    "print(f\"Saving submission to submissions/{new_filename}.csv\")\n",
    "submissions_df.to_csv(os.path.join('submissions', f\"{new_filename}.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importance\n",
    "print(\"\\033[1m\" + \"Calculating feature importance for location A...\" + \"\\033[0m\")\n",
    "print(predictors[0].feature_importance(feature_stage=\"original\", data=test_data[test_data[\"location\"] == \"A\"], time_limit=60*10))\n",
    "print(\"\\033[1m\" + \"Calculating feature importance for location B...\" + \"\\033[0m\")\n",
    "print(predictors[1].feature_importance(feature_stage=\"original\", data=test_data[test_data[\"location\"] == \"B\"], time_limit=60*10))\n",
    "print(\"\\033[1m\" + \"Calculating feature importance for location C...\" + \"\\033[0m\")\n",
    "print(predictors[2].feature_importance(feature_stage=\"original\", data=test_data[test_data[\"location\"] == \"C\"], time_limit=60*10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save this notebook to submissions folder\n",
    "import subprocess\n",
    "import os\n",
    "subprocess.run([\"jupyter\", \"nbconvert\", \"--to\", \"pdf\", \"--output\", os.path.join('notebook_pdfs', f\"{new_filename}.pdf\"), \"autogluon_each_location.ipynb\"])"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m111"
  },
  "kernelspec": {
   "display_name": "ag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
