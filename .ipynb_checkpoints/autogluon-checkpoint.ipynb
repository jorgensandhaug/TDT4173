{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Processing location A...\n",
      "Shape of X_train_observed before dropping in-between hour rows:  (118669, 45)\n",
      "HEIHEI: X_train_observed gaps in dates:  0\n",
      "HEIHEI: X_train_observed first gap in dates:  DatetimeIndex([], dtype='datetime64[us]', name='ds', freq=None)\n",
      "HEIHEI: X_train_observed list of size (in days) of each gap:  []\n",
      "HEIHEI: X_train_observed gaps in dates after filling missing dates:  0\n",
      "Shape of X_train_observed after dropping in-between hour rows:  (29668, 45)\n",
      "Shape of X_train_estimated before dropping in-between hour rows:  (17576, 46)\n",
      "HEIHEI: X_train_estimated gaps in dates:  1\n",
      "HEIHEI: X_train_estimated first gap in dates:  DatetimeIndex(['2023-01-27'], dtype='datetime64[us]', name='ds', freq=None)\n",
      "HEIHEI: X_train_estimated list of size (in days) of each gap:  [1.01041667]\n",
      "HEIHEI: X_train_estimated gaps in dates after filling missing dates:  0\n",
      "Shape of X_train_estimated after dropping in-between hour rows:  (4418, 46)\n",
      "Shape of X_test before dropping in-between hour rows:  (2880, 46)\n",
      "HEIHEI: X_test gaps in dates:  17\n",
      "HEIHEI: X_test first gap in dates:  DatetimeIndex(['2023-05-06'], dtype='datetime64[us]', name='ds', freq=None)\n",
      "HEIHEI: X_test list of size (in days) of each gap:  [4.01041667 7.01041667 3.01041667 1.01041667 1.01041667 1.01041667\n",
      " 1.01041667 1.01041667 1.01041667 2.01041667 1.01041667 1.01041667\n",
      " 3.01041667 2.01041667 3.01041667 1.01041667 1.01041667]\n",
      "HEIHEI: X_test gaps in dates after filling missing dates:  0\n",
      "Shape of X_test after dropping in-between hour rows:  (1536, 46)\n",
      "X_train_observed shape: (29668, 46)\n",
      "X_train_estimated shape: (4418, 46)\n",
      "X_test shape: (1536, 46)\n",
      "y_train shape: (34085, 1)\n",
      "y_train columns:  Index(['y'], dtype='object')\n",
      "Shape of y_train before filling missing dates:  (34085, 1)\n",
      "Shape of y_train after filling missing dates:  (34274, 1)\n",
      "LOOK: Number of gaps in X_train_observed plus number of gaps in X_train_estimated before:  0\n",
      "LOOK: Number of gaps in X_train_observed plus number of gaps in X_train_estimated after:  1\n",
      "LOOK: list of size (in days) of each gap:  [7.875]\n",
      "if the number is bigger after than before that means there is a gap in time between the observed and estimated training sets\n",
      "X_train dates info:  2019-06-02 22:00:00 2023-04-30 23:00:00 1428 days 01:00:00\n",
      "X_test dates info:  2023-05-01 00:00:00 2023-07-03 23:00:00 63 days 23:00:00\n",
      "y_train dates info:  2019-06-02 22:00:00 2023-04-30 23:00:00 1428 days 01:00:00\n",
      "X_train gaps in dates:  1\n",
      "X_test gaps in dates:  0\n",
      "y_train gaps in dates:  0\n",
      "X_train gaps in dates after filling missing dates:  0\n",
      "X_test gaps in dates after filling missing dates:  0\n",
      "Number of missing values in X_train:  53521\n",
      "Number of missing values in X_test:  38573\n",
      "Number of missing values in y_train:  189\n",
      "Number of missing values in X_train after merging with y_train:  53521\n",
      "Final shape of X_train for location A:  (34274, 48)\n",
      "Final shape of X_test for location A:  (1536, 47)\n",
      "\n",
      "\n",
      "\n",
      "Processing location B...\n",
      "Shape of X_train_observed before dropping in-between hour rows:  (116929, 45)\n",
      "HEIHEI: X_train_observed gaps in dates:  0\n",
      "HEIHEI: X_train_observed first gap in dates:  DatetimeIndex([], dtype='datetime64[us]', name='ds', freq=None)\n",
      "HEIHEI: X_train_observed list of size (in days) of each gap:  []\n",
      "HEIHEI: X_train_observed gaps in dates after filling missing dates:  0\n",
      "Shape of X_train_observed after dropping in-between hour rows:  (29233, 45)\n",
      "Shape of X_train_estimated before dropping in-between hour rows:  (17576, 46)\n",
      "HEIHEI: X_train_estimated gaps in dates:  1\n",
      "HEIHEI: X_train_estimated first gap in dates:  DatetimeIndex(['2023-01-27'], dtype='datetime64[us]', name='ds', freq=None)\n",
      "HEIHEI: X_train_estimated list of size (in days) of each gap:  [1.01041667]\n",
      "HEIHEI: X_train_estimated gaps in dates after filling missing dates:  0\n",
      "Shape of X_train_estimated after dropping in-between hour rows:  (4418, 46)\n",
      "Shape of X_test before dropping in-between hour rows:  (2880, 46)\n",
      "HEIHEI: X_test gaps in dates:  17\n",
      "HEIHEI: X_test first gap in dates:  DatetimeIndex(['2023-05-06'], dtype='datetime64[us]', name='ds', freq=None)\n",
      "HEIHEI: X_test list of size (in days) of each gap:  [4.01041667 7.01041667 3.01041667 1.01041667 1.01041667 1.01041667\n",
      " 1.01041667 1.01041667 1.01041667 2.01041667 1.01041667 1.01041667\n",
      " 3.01041667 2.01041667 3.01041667 1.01041667 1.01041667]\n",
      "HEIHEI: X_test gaps in dates after filling missing dates:  0\n",
      "Shape of X_test after dropping in-between hour rows:  (1536, 46)\n",
      "X_train_observed shape: (29233, 46)\n",
      "X_train_estimated shape: (4418, 46)\n",
      "X_test shape: (1536, 46)\n",
      "y_train shape: (32848, 1)\n",
      "y_train columns:  Index(['y'], dtype='object')\n",
      "Shape of y_train before filling missing dates:  (32848, 1)\n",
      "Shape of y_train after filling missing dates:  (37945, 1)\n",
      "LOOK: Number of gaps in X_train_observed plus number of gaps in X_train_estimated before:  0\n",
      "LOOK: Number of gaps in X_train_observed plus number of gaps in X_train_estimated after:  1\n",
      "LOOK: list of size (in days) of each gap:  [178.91666667]\n",
      "if the number is bigger after than before that means there is a gap in time between the observed and estimated training sets\n",
      "X_train dates info:  2019-01-01 00:00:00 2023-04-30 23:00:00 1580 days 23:00:00\n",
      "X_test dates info:  2023-05-01 00:00:00 2023-07-03 23:00:00 63 days 23:00:00\n",
      "y_train dates info:  2018-12-31 23:00:00 2023-04-30 23:00:00 1581 days 00:00:00\n",
      "X_train gaps in dates:  1\n",
      "X_test gaps in dates:  0\n",
      "y_train gaps in dates:  0\n",
      "X_train gaps in dates after filling missing dates:  0\n",
      "X_test gaps in dates after filling missing dates:  0\n",
      "Number of missing values in X_train:  239726\n",
      "Number of missing values in X_test:  38553\n",
      "Number of missing values in y_train:  5101\n",
      "Number of missing values in X_train after merging with y_train:  239772\n",
      "Final shape of X_train for location B:  (37945, 48)\n",
      "Final shape of X_test for location B:  (1536, 47)\n",
      "\n",
      "\n",
      "\n",
      "Processing location C...\n",
      "Shape of X_train_observed before dropping in-between hour rows:  (116825, 45)\n",
      "HEIHEI: X_train_observed gaps in dates:  0\n",
      "HEIHEI: X_train_observed first gap in dates:  DatetimeIndex([], dtype='datetime64[us]', name='ds', freq=None)\n",
      "HEIHEI: X_train_observed list of size (in days) of each gap:  []\n",
      "HEIHEI: X_train_observed gaps in dates after filling missing dates:  0\n",
      "Shape of X_train_observed after dropping in-between hour rows:  (29207, 45)\n",
      "Shape of X_train_estimated before dropping in-between hour rows:  (17576, 46)\n",
      "HEIHEI: X_train_estimated gaps in dates:  1\n",
      "HEIHEI: X_train_estimated first gap in dates:  DatetimeIndex(['2023-01-27'], dtype='datetime64[us]', name='ds', freq=None)\n",
      "HEIHEI: X_train_estimated list of size (in days) of each gap:  [1.01041667]\n",
      "HEIHEI: X_train_estimated gaps in dates after filling missing dates:  0\n",
      "Shape of X_train_estimated after dropping in-between hour rows:  (4418, 46)\n",
      "Shape of X_test before dropping in-between hour rows:  (2880, 46)\n",
      "HEIHEI: X_test gaps in dates:  17\n",
      "HEIHEI: X_test first gap in dates:  DatetimeIndex(['2023-05-06'], dtype='datetime64[us]', name='ds', freq=None)\n",
      "HEIHEI: X_test list of size (in days) of each gap:  [4.01041667 7.01041667 3.01041667 1.01041667 1.01041667 1.01041667\n",
      " 1.01041667 1.01041667 1.01041667 2.01041667 1.01041667 1.01041667\n",
      " 3.01041667 2.01041667 3.01041667 1.01041667 1.01041667]\n",
      "HEIHEI: X_test gaps in dates after filling missing dates:  0\n",
      "Shape of X_test after dropping in-between hour rows:  (1536, 46)\n",
      "X_train_observed shape: (29207, 46)\n",
      "X_train_estimated shape: (4418, 46)\n",
      "X_test shape: (1536, 46)\n",
      "y_train shape: (32155, 1)\n",
      "y_train columns:  Index(['y'], dtype='object')\n",
      "Shape of y_train before filling missing dates:  (32155, 1)\n",
      "Shape of y_train after filling missing dates:  (37945, 1)\n",
      "LOOK: Number of gaps in X_train_observed plus number of gaps in X_train_estimated before:  0\n",
      "LOOK: Number of gaps in X_train_observed plus number of gaps in X_train_estimated after:  1\n",
      "LOOK: list of size (in days) of each gap:  [180.]\n",
      "if the number is bigger after than before that means there is a gap in time between the observed and estimated training sets\n",
      "X_train dates info:  2019-01-01 00:00:00 2023-04-30 23:00:00 1580 days 23:00:00\n",
      "X_test dates info:  2023-05-01 00:00:00 2023-07-03 23:00:00 63 days 23:00:00\n",
      "y_train dates info:  2018-12-31 23:00:00 2023-04-30 23:00:00 1581 days 00:00:00\n",
      "X_train gaps in dates:  1\n",
      "X_test gaps in dates:  0\n",
      "y_train gaps in dates:  0\n",
      "X_train gaps in dates after filling missing dates:  0\n",
      "X_test gaps in dates after filling missing dates:  0\n",
      "Number of missing values in X_train:  240647\n",
      "Number of missing values in X_test:  38610\n",
      "Number of missing values in y_train:  11850\n",
      "Number of missing values in X_train after merging with y_train:  240693\n",
      "Final shape of X_train for location C:  (37945, 48)\n",
      "Final shape of X_test for location C:  (1536, 47)\n",
      "Final shape of X_train:  (110164, 48)\n",
      "Final shape of X_test:  (4608, 47)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from darts import TimeSeries\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def fix_datetime(X, name):\n",
    "    \"\"\"\n",
    "    Function to fix and standardize datetime in the given DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: DataFrame to be modified.\n",
    "    - name: String representing the name of the DataFrame, used for logging.\n",
    "    \n",
    "    Returns:\n",
    "    - Modified DataFrame with standardized datetime.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert 'date_forecast' to datetime format and replace original column with 'ds'\n",
    "    X['ds'] = pd.to_datetime(X['date_forecast'])\n",
    "    X.drop(columns=['date_forecast'], inplace=True, errors='ignore')\n",
    "\n",
    "    # Sort DataFrame by the new datetime column ('ds') and set it as the index\n",
    "    X.sort_values(by='ds', inplace=True)\n",
    "    X.set_index('ds', inplace=True)\n",
    "\n",
    "    # Log the shape of the DataFrame before dropping rows with in-between minutes\n",
    "    print(f\"Shape of {name} before dropping in-between hour rows: \", X.shape)\n",
    "\n",
    "    # Identify and log gaps in the date sequence\n",
    "    print(f\"HEIHEI: {name} gaps in dates: \", X.index.to_series().diff().dt.total_seconds().gt(60*15).sum())\n",
    "    print(f\"HEIHEI: {name} first gap in dates: \", X[X.index.to_series().diff().dt.total_seconds().gt(60*15)==True].index[:1])\n",
    "\n",
    "    # Calculate and log the size of each gap in the date sequence\n",
    "    temp = X.index.to_series().diff().dt.total_seconds()\n",
    "    if temp.shape[0] > 0:\n",
    "        print(f\"HEIHEI: {name} list of size (in days) of each gap: \", temp[temp.gt(60*15)].values / (60*60*24))\n",
    "    \n",
    "    # temporarily transform into darts time series to fill missing dates\n",
    "    # get date_calc if date_calc is column in X\n",
    "    temp_calc = None\n",
    "    if \"date_calc\" in X.columns:\n",
    "        temp_calc = X[\"date_calc\"]\n",
    "        X.drop(columns=['date_calc'], inplace=True)\n",
    "    X = TimeSeries.from_dataframe(df=X, freq=\"15T\", fill_missing_dates=True, fillna_value=None).pd_dataframe()\n",
    "    if temp_calc is not None:\n",
    "        X[\"date_calc\"] = temp_calc\n",
    "\n",
    "    print(f\"HEIHEI: {name} gaps in dates after filling missing dates: \", X.index.to_series().diff().dt.total_seconds().gt(60*15).sum())\n",
    "\n",
    "\n",
    "    # Drop rows where the minute part of the time is not 0\n",
    "    X = X[X.index.minute == 0]\n",
    "\n",
    "    # Log the shape of the DataFrame after dropping rows with in-between minutes\n",
    "    print(f\"Shape of {name} after dropping in-between hour rows: \", X.shape)\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "\n",
    "def convert_to_datetime(X_train_observed, X_train_estimated, X_test, y_train):\n",
    "    X_train_observed = fix_datetime(X_train_observed, \"X_train_observed\")\n",
    "    X_train_estimated = fix_datetime(X_train_estimated, \"X_train_estimated\")\n",
    "    X_test = fix_datetime(X_test, \"X_test\")\n",
    "\n",
    "\n",
    "    X_train_observed[\"estimated_diff_hours\"] = 0\n",
    "    X_train_estimated[\"estimated_diff_hours\"] = (X_train_estimated.index - pd.to_datetime(X_train_estimated[\"date_calc\"])).dt.total_seconds() / 3600.0\n",
    "    X_test[\"estimated_diff_hours\"] = (X_test.index - pd.to_datetime(X_test[\"date_calc\"])).dt.total_seconds() / 3600.0\n",
    "\n",
    "    X_train_estimated.drop(columns=['date_calc'], inplace=True)\n",
    "    X_test.drop(columns=['date_calc'], inplace=True)\n",
    "\n",
    "    y_train['ds'] = pd.to_datetime(y_train['time'])\n",
    "    y_train.drop(columns=['time'], inplace=True)\n",
    "    y_train.sort_values(by='ds', inplace=True)\n",
    "    y_train.set_index('ds', inplace=True)\n",
    "\n",
    "    return X_train_observed, X_train_estimated, X_test, y_train\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# location_map = {\n",
    "#     \"A\": 0,\n",
    "#     \"B\": 1,\n",
    "#     \"C\": 2\n",
    "# }\n",
    "\n",
    "\n",
    "def preprocess_data(X_train_observed, X_train_estimated, X_test, y_train, location):\n",
    "    # convert to datetime\n",
    "    X_train_observed, X_train_estimated, X_test, y_train = convert_to_datetime(X_train_observed, X_train_estimated, X_test, y_train)\n",
    "\n",
    "\n",
    "    # # cast all columns to float64\n",
    "    # X_train = X_train.astype('float64')\n",
    "    # X_test = X_test.astype('float64')\n",
    "\n",
    "\n",
    "    print(f\"X_train_observed shape: {X_train_observed.shape}\")\n",
    "    print(f\"X_train_estimated shape: {X_train_estimated.shape}\")\n",
    "    print(f\"X_test shape: {X_test.shape}\")\n",
    "    print(f\"y_train shape: {y_train.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "    y_train[\"y\"] = y_train[\"pv_measurement\"].astype('float64')\n",
    "    y_train.drop(columns=['pv_measurement'], inplace=True)\n",
    "    print(\"y_train columns: \", y_train.columns)\n",
    "\n",
    "    # temporarily transform into darts time series to fill missing dates\n",
    "    print(\"Shape of y_train before filling missing dates: \", y_train.shape)\n",
    "    y_train = TimeSeries.from_dataframe(df=y_train, freq=\"H\", fill_missing_dates=True, fillna_value=None).pd_dataframe()\n",
    "    print(\"Shape of y_train after filling missing dates: \", y_train.shape)\n",
    "\n",
    "\n",
    "    # number of gaps in X_train_observed + X_train_estimated before\n",
    "    print(f\"LOOK: Number of gaps in X_train_observed plus number of gaps in X_train_estimated before: \", X_train_observed.index.to_series().diff().dt.total_seconds().gt(3600).sum() + X_train_estimated.index.to_series().diff().dt.total_seconds().gt(3600).sum())\n",
    "    X_train = pd.concat([X_train_observed, X_train_estimated])\n",
    "    print(f\"LOOK: Number of gaps in X_train_observed plus number of gaps in X_train_estimated after: \", X_train.index.to_series().diff().dt.total_seconds().gt(3600).sum())\n",
    "    # print size of gaps in X_train\n",
    "    temp = X_train.index.to_series().diff().dt.total_seconds()\n",
    "    if temp.shape[0] > 0:\n",
    "        print(\"LOOK: list of size (in days) of each gap: \", temp[temp.gt(3600)].values / (60*60*24))\n",
    "    print(\"if the number is bigger after than before that means there is a gap in time between the observed and estimated training sets\")\n",
    "\n",
    "    # print info on dates in X_train, and if there are any missing dates\n",
    "    print(\"X_train dates info: \", X_train.index.min(), X_train.index.max(), X_train.index.max() - X_train.index.min())\n",
    "    print(\"X_test dates info: \", X_test.index.min(), X_test.index.max(), X_test.index.max() - X_test.index.min())\n",
    "    print(\"y_train dates info: \", y_train.index.min(), y_train.index.max(), y_train.index.max() - y_train.index.min())\n",
    "\n",
    "    # any gaps in dates?\n",
    "    print(\"X_train gaps in dates: \", X_train.index.to_series().diff().dt.total_seconds().gt(3600).sum())\n",
    "    print(\"X_test gaps in dates: \", X_test.index.to_series().diff().dt.total_seconds().gt(3600).sum())\n",
    "    print(\"y_train gaps in dates: \", y_train.index.to_series().diff().dt.total_seconds().gt(3600).sum())\n",
    "\n",
    "    # temporarily transform into darts time series to fill missing dates\n",
    "    X_train = TimeSeries.from_dataframe(df=X_train, freq=\"H\", fill_missing_dates=True, fillna_value=None).pd_dataframe()\n",
    "    X_test = TimeSeries.from_dataframe(df=X_test, freq=\"H\", fill_missing_dates=True, fillna_value=None).pd_dataframe()\n",
    "    print(\"X_train gaps in dates after filling missing dates: \", X_train.index.to_series().diff().dt.total_seconds().gt(3600).sum())\n",
    "    print(\"X_test gaps in dates after filling missing dates: \", X_test.index.to_series().diff().dt.total_seconds().gt(3600).sum())\n",
    "\n",
    "    \n",
    "\n",
    "    # clip all y values to 0 if negative\n",
    "    y_train[\"y\"] = y_train[\"y\"].clip(lower=0)\n",
    "    \n",
    "    # print Number of missing values in X train\n",
    "    print(\"Number of missing values in X_train: \", X_train.isnull().sum().sum())\n",
    "    print(\"Number of missing values in X_test: \", X_test.isnull().sum().sum())\n",
    "    # y_train missing values\n",
    "    print(\"Number of missing values in y_train: \", y_train.isnull().sum().sum())\n",
    "    X_train = pd.merge(X_train, y_train, how=\"outer\", left_index=True, right_index=True)\n",
    "    print(\"Number of missing values in X_train after merging with y_train: \", X_train.drop(columns=['y']).isnull().sum().sum())\n",
    "\n",
    "\n",
    "\n",
    "    X_train[\"location\"] = location\n",
    "    X_test[\"location\"] = location\n",
    "    \n",
    "    return X_train, X_test\n",
    "    \n",
    "\n",
    "\n",
    "# Define locations\n",
    "locations = ['A', 'B', 'C']\n",
    "\n",
    "X_trains = []\n",
    "X_tests = []\n",
    "y_trains = []\n",
    "# Loop through locations\n",
    "for loc in locations:\n",
    "    print(\"\\n\\n\")\n",
    "    print(f\"Processing location {loc}...\")\n",
    "    # Read target training data\n",
    "    y_train = pd.read_parquet(f'{loc}/train_targets.parquet')\n",
    "    \n",
    "    # Read estimated training data and add location feature\n",
    "    X_train_estimated = pd.read_parquet(f'{loc}/X_train_estimated.parquet')\n",
    "    \n",
    "    # Read observed training data and add location feature\n",
    "    X_train_observed= pd.read_parquet(f'{loc}/X_train_observed.parquet')\n",
    "\n",
    "    # Read estimated test data and add location feature\n",
    "    X_test_estimated = pd.read_parquet(f'{loc}/X_test_estimated.parquet')\n",
    "    \n",
    "    # Concatenate observed and estimated datasets for each location\n",
    "    #X_train = pd.concat([X_train_estimated, X_train_observed])\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # Preprocess data\n",
    "    X_train, X_test = preprocess_data(X_train_observed, X_train_estimated, X_test_estimated, y_train, loc)\n",
    "\n",
    "    print(f\"Final shape of X_train for location {loc}: \", X_train.shape)\n",
    "    print(f\"Final shape of X_test for location {loc}: \", X_test.shape)\n",
    "\n",
    "    # print(y_train.head(), y_train.shape)\n",
    "    # print(X_train.head(), X_train.shape)\n",
    "    # print(X_train.head(), X_train.shape)\n",
    "    # print(type(X_train['y']))\n",
    "\n",
    "    # Save data to csv\n",
    "    X_train.to_csv(f'{loc}/X_train.csv', index=True)\n",
    "    X_test.to_csv(f'{loc}/X_test.csv', index=True)\n",
    "\n",
    "\n",
    "    X_trains.append(X_train)\n",
    "    X_tests.append(X_test)\n",
    "\n",
    "# Concatenate all data and save to csv\n",
    "X_train = pd.concat(X_trains)\n",
    "X_test = pd.concat(X_tests)\n",
    "\n",
    "print(f\"Final shape of X_train: \", X_train.shape)\n",
    "print(f\"Final shape of X_test: \", X_test.shape)\n",
    "\n",
    "X_train.to_csv('X_train_raw.csv', index=True)\n",
    "X_test.to_csv('X_test_raw.csv', index=True)\n",
    "\n",
    "\n",
    "# save where nan y values are dropped\n",
    "X_train.dropna(subset=['y'], inplace=True)\n",
    "X_train.to_csv('X_train.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded data from: X_train.csv | Columns = 49 / 49 | Rows = 93024 -> 93024\n",
      "Loaded data from: X_test_raw.csv | Columns = 48 / 48 | Rows = 4608 -> 4608\n",
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20231005_081934\"\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=20\n",
      "Beginning AutoGluon training ... Time limit = 60s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20231005_081934\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.10.12\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 22.1.0: Sun Oct  9 20:15:09 PDT 2022; root:xnu-8792.41.9~2/RELEASE_ARM64_T6000\n",
      "Disk Space Avail:   34.05 GB / 494.38 GB (6.9%)\n",
      "Train Data Rows:    93024\n",
      "Train Data Columns: 48\n",
      "Label Column: y\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (5733.42, -0.0, 287.01965, 766.40778)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    6058.73 MB\n",
      "\tTrain Data (Original)  Memory Usage: 46.7 MB (0.8% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                      : 46 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', ...]\n",
      "\t\t('object', [])                     :  1 | ['location']\n",
      "\t\t('object', ['datetime_as_object']) :  1 | ['ds']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])             :  1 | ['location']\n",
      "\t\t('float', [])                : 45 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', ...]\n",
      "\t\t('int', ['bool'])            :  1 | ['snow_density:kgm3']\n",
      "\t\t('int', ['datetime_as_int']) :  5 | ['ds', 'ds.year', 'ds.month', 'ds.day', 'ds.dayofweek']\n",
      "\t0.3s = Fit runtime\n",
      "\t48 features in original data used to generate 52 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 37.4 MB (0.6% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.35s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 39.76s of the 59.65s of remaining time.\n",
      "\t-285.3275\t = Validation score   (-mean_absolute_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t6.57s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 33.02s of the 52.91s of remaining time.\n",
      "\t-337.9007\t = Validation score   (-mean_absolute_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t6.1s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 26.75s of the 46.65s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-73.8274\t = Validation score   (-mean_absolute_error)\n",
      "\t23.2s\t = Training   runtime\n",
      "\t80.5s\t = Validation runtime\n",
      "Completed 1/20 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 59.65s of the 10.37s of remaining time.\n",
      "\t-73.8274\t = Validation score   (-mean_absolute_error)\n",
      "\t0.38s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 9 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 9.97s of the 9.96s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-56.3873\t = Validation score   (-mean_absolute_error)\n",
      "\t8.82s\t = Training   runtime\n",
      "\t6.75s\t = Validation runtime\n",
      "Completed 1/20 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 59.65s of the -2.27s of remaining time.\n",
      "\t-56.3873\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 62.32s ... Best model: \"WeightedEnsemble_L3\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20231005_081934\")\n"
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "train_data = TabularDataset('X_train.csv')\n",
    "test_data = TabularDataset('X_test_raw.csv')\n",
    "label = 'y'\n",
    "\n",
    "# mean absolute error\n",
    "metric = 'mean_absolute_error'\n",
    "time_limit = 60\n",
    "predictor = TabularPredictor(label=label, eval_metric=metric).fit(train_data, time_limit=time_limit, presets='best_quality')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          prediction   id\n",
      "location                 \n",
      "A                720  720\n",
      "B                720  720\n",
      "C                720  720\n"
     ]
    }
   ],
   "source": [
    "# pull in test.csv and create submission.csv\n",
    "submission_ids_df = pd.read_csv('test.csv', index_col=0)\n",
    "# submission_ids_df has Id, Time, Location\n",
    "# X_test has location_A, location_B, location_C\n",
    "# we have to make sure that submission_ids_df has same dates as X_test, then predict y for each row in submission_ids_df, then save as submission.csv (using correct Id from submission_ids_df)\n",
    "\n",
    "# convert index to datetime\n",
    "submission_ids_df[\"id\"] = submission_ids_df.index\n",
    "submission_ids_df.set_index('time', inplace=True)\n",
    "submission_ids_df.index = pd.to_datetime(submission_ids_df.index).astype('datetime64[ns]')\n",
    "submission_ids_df\n",
    "\n",
    "# split submission_ids_df into each location\n",
    "print(submission_ids_df.groupby('location').count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded data from: X_test_raw.csv | Columns = 48 / 48 | Rows = 4608 -> 4608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded data from: test.csv | Columns = 4 / 4 | Rows = 2160 -> 2160\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>absolute_humidity_2m:gm3</th>\n",
       "      <th>air_density_2m:kgm3</th>\n",
       "      <th>ceiling_height_agl:m</th>\n",
       "      <th>clear_sky_energy_1h:J</th>\n",
       "      <th>clear_sky_rad:W</th>\n",
       "      <th>cloud_base_agl:m</th>\n",
       "      <th>dew_or_rime:idx</th>\n",
       "      <th>dew_point_2m:K</th>\n",
       "      <th>diffuse_rad:W</th>\n",
       "      <th>...</th>\n",
       "      <th>visibility:m</th>\n",
       "      <th>wind_speed_10m:ms</th>\n",
       "      <th>wind_speed_u_10m:ms</th>\n",
       "      <th>wind_speed_v_10m:ms</th>\n",
       "      <th>wind_speed_w_1000hPa:ms</th>\n",
       "      <th>estimated_diff_hours</th>\n",
       "      <th>location</th>\n",
       "      <th>id</th>\n",
       "      <th>time</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-05-01 00:00:00</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.286</td>\n",
       "      <td>912.700012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1041.199951</td>\n",
       "      <td>0.0</td>\n",
       "      <td>271.700012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>30210.699219</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>16.998889</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-05-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-05-01 01:00:00</td>\n",
       "      <td>4.3</td>\n",
       "      <td>1.287</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1095.400024</td>\n",
       "      <td>0.0</td>\n",
       "      <td>271.600006</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>29507.500000</td>\n",
       "      <td>3.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.3</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>17.998889</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-05-01 01:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-05-01 02:00:00</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.284</td>\n",
       "      <td>1482.099976</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1041.300049</td>\n",
       "      <td>0.0</td>\n",
       "      <td>271.200012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>29463.099609</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.8</td>\n",
       "      <td>3.2</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>18.998889</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-05-01 02:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-05-01 03:00:00</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.282</td>\n",
       "      <td>2306.699951</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1465.599976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>270.799988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>33727.101562</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.6</td>\n",
       "      <td>3.2</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>19.998889</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-05-01 03:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-05-01 04:00:00</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.282</td>\n",
       "      <td>2323.199951</td>\n",
       "      <td>59774.500000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>703.599976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>270.299988</td>\n",
       "      <td>31.400000</td>\n",
       "      <td>...</td>\n",
       "      <td>35927.601562</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.3</td>\n",
       "      <td>3.1</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>20.998889</td>\n",
       "      <td>A</td>\n",
       "      <td>4</td>\n",
       "      <td>2023-05-01 04:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2155</th>\n",
       "      <td>2023-07-03 19:00:00</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1.196</td>\n",
       "      <td>3532.399902</td>\n",
       "      <td>615338.812500</td>\n",
       "      <td>117.199997</td>\n",
       "      <td>2239.600098</td>\n",
       "      <td>0.0</td>\n",
       "      <td>281.600006</td>\n",
       "      <td>41.299999</td>\n",
       "      <td>...</td>\n",
       "      <td>41536.398438</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1.9</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.991389</td>\n",
       "      <td>C</td>\n",
       "      <td>2155</td>\n",
       "      <td>2023-07-03 19:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2156</th>\n",
       "      <td>2023-07-03 20:00:00</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1.199</td>\n",
       "      <td>3429.000000</td>\n",
       "      <td>269582.406250</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>1513.699951</td>\n",
       "      <td>0.0</td>\n",
       "      <td>281.899994</td>\n",
       "      <td>19.700001</td>\n",
       "      <td>...</td>\n",
       "      <td>40136.500000</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.991389</td>\n",
       "      <td>C</td>\n",
       "      <td>2156</td>\n",
       "      <td>2023-07-03 20:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2157</th>\n",
       "      <td>2023-07-03 21:00:00</td>\n",
       "      <td>8.8</td>\n",
       "      <td>1.202</td>\n",
       "      <td>2495.000000</td>\n",
       "      <td>71999.601562</td>\n",
       "      <td>4.900000</td>\n",
       "      <td>1342.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>282.200012</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>43266.101562</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.2</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.991389</td>\n",
       "      <td>C</td>\n",
       "      <td>2157</td>\n",
       "      <td>2023-07-03 21:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2158</th>\n",
       "      <td>2023-07-03 22:00:00</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.206</td>\n",
       "      <td>1997.400024</td>\n",
       "      <td>1378.300049</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1878.900024</td>\n",
       "      <td>0.0</td>\n",
       "      <td>282.600006</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>39017.898438</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.991389</td>\n",
       "      <td>C</td>\n",
       "      <td>2158</td>\n",
       "      <td>2023-07-03 22:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2159</th>\n",
       "      <td>2023-07-03 23:00:00</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.207</td>\n",
       "      <td>2005.599976</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1471.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>282.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>39026.000000</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.6</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.991389</td>\n",
       "      <td>C</td>\n",
       "      <td>2159</td>\n",
       "      <td>2023-07-03 23:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2160 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       ds  absolute_humidity_2m:gm3  air_density_2m:kgm3  \\\n",
       "0     2023-05-01 00:00:00                       4.4                1.286   \n",
       "1     2023-05-01 01:00:00                       4.3                1.287   \n",
       "2     2023-05-01 02:00:00                       4.2                1.284   \n",
       "3     2023-05-01 03:00:00                       4.1                1.282   \n",
       "4     2023-05-01 04:00:00                       3.9                1.282   \n",
       "...                   ...                       ...                  ...   \n",
       "2155  2023-07-03 19:00:00                       8.3                1.196   \n",
       "2156  2023-07-03 20:00:00                       8.5                1.199   \n",
       "2157  2023-07-03 21:00:00                       8.8                1.202   \n",
       "2158  2023-07-03 22:00:00                       9.0                1.206   \n",
       "2159  2023-07-03 23:00:00                       9.0                1.207   \n",
       "\n",
       "      ceiling_height_agl:m  clear_sky_energy_1h:J  clear_sky_rad:W  \\\n",
       "0               912.700012               0.000000         0.000000   \n",
       "1                      NaN               0.000000         0.000000   \n",
       "2              1482.099976               0.000000         0.000000   \n",
       "3              2306.699951               0.000000         0.000000   \n",
       "4              2323.199951           59774.500000        43.000000   \n",
       "...                    ...                    ...              ...   \n",
       "2155           3532.399902          615338.812500       117.199997   \n",
       "2156           3429.000000          269582.406250        40.000000   \n",
       "2157           2495.000000           71999.601562         4.900000   \n",
       "2158           1997.400024            1378.300049         0.000000   \n",
       "2159           2005.599976               0.000000         0.000000   \n",
       "\n",
       "      cloud_base_agl:m  dew_or_rime:idx  dew_point_2m:K  diffuse_rad:W  ...  \\\n",
       "0          1041.199951              0.0      271.700012       0.000000  ...   \n",
       "1          1095.400024              0.0      271.600006       0.000000  ...   \n",
       "2          1041.300049              0.0      271.200012       0.000000  ...   \n",
       "3          1465.599976              0.0      270.799988       0.000000  ...   \n",
       "4           703.599976              0.0      270.299988      31.400000  ...   \n",
       "...                ...              ...             ...            ...  ...   \n",
       "2155       2239.600098              0.0      281.600006      41.299999  ...   \n",
       "2156       1513.699951              0.0      281.899994      19.700001  ...   \n",
       "2157       1342.500000              0.0      282.200012       5.000000  ...   \n",
       "2158       1878.900024              0.0      282.600006       0.000000  ...   \n",
       "2159       1471.000000              0.0      282.500000       0.000000  ...   \n",
       "\n",
       "      visibility:m  wind_speed_10m:ms  wind_speed_u_10m:ms  \\\n",
       "0     30210.699219                4.0                  2.2   \n",
       "1     29507.500000                3.9                  2.0   \n",
       "2     29463.099609                3.7                  1.8   \n",
       "3     33727.101562                3.6                  1.6   \n",
       "4     35927.601562                3.4                  1.3   \n",
       "...            ...                ...                  ...   \n",
       "2155  41536.398438                2.2                  1.9   \n",
       "2156  40136.500000                2.1                  1.9   \n",
       "2157  43266.101562                2.4                  2.2   \n",
       "2158  39017.898438                2.0                  1.8   \n",
       "2159  39026.000000                1.7                  1.6   \n",
       "\n",
       "      wind_speed_v_10m:ms  wind_speed_w_1000hPa:ms  estimated_diff_hours  \\\n",
       "0                     3.4                     -0.0             16.998889   \n",
       "1                     3.3                     -0.0             17.998889   \n",
       "2                     3.2                     -0.0             18.998889   \n",
       "3                     3.2                     -0.0             19.998889   \n",
       "4                     3.1                     -0.0             20.998889   \n",
       "...                   ...                      ...                   ...   \n",
       "2155                 -1.2                      0.0             35.991389   \n",
       "2156                 -0.9                      0.0             36.991389   \n",
       "2157                 -1.0                      0.0             37.991389   \n",
       "2158                 -0.8                      0.0             38.991389   \n",
       "2159                 -0.7                      0.0             39.991389   \n",
       "\n",
       "      location    id                 time  prediction  \n",
       "0            A     0  2023-05-01 00:00:00           0  \n",
       "1            A     1  2023-05-01 01:00:00           0  \n",
       "2            A     2  2023-05-01 02:00:00           0  \n",
       "3            A     3  2023-05-01 03:00:00           0  \n",
       "4            A     4  2023-05-01 04:00:00           0  \n",
       "...        ...   ...                  ...         ...  \n",
       "2155         C  2155  2023-07-03 19:00:00           0  \n",
       "2156         C  2156  2023-07-03 20:00:00           0  \n",
       "2157         C  2157  2023-07-03 21:00:00           0  \n",
       "2158         C  2158  2023-07-03 22:00:00           0  \n",
       "2159         C  2159  2023-07-03 23:00:00           0  \n",
       "\n",
       "[2160 rows x 51 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming X_test and model are already defined\n",
    "# Assuming 'time' is also a datetime64[ns] column in X_test\n",
    "\n",
    "\n",
    "test_data = TabularDataset('X_test_raw.csv')\n",
    "test_ids = TabularDataset('test.csv')\n",
    "# merge test_data with test_ids\n",
    "test_data = pd.merge(test_data, test_ids, how=\"inner\", right_on=[\"time\", \"location\"], left_on=[\"ds\", \"location\"])\n",
    "\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "y_pred = predictor.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        -1.192794\n",
       "1        -0.823539\n",
       "2        -0.486877\n",
       "3        14.557883\n",
       "4       131.272003\n",
       "           ...    \n",
       "2155      5.461376\n",
       "2156      3.115988\n",
       "2157      1.281675\n",
       "2158     -0.930885\n",
       "2159     -0.764861\n",
       "Name: y, Length: 2160, dtype: float32"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.192794</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.823539</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.486877</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.557883</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>131.272003</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2155</th>\n",
       "      <td>5.461376</td>\n",
       "      <td>2155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2156</th>\n",
       "      <td>3.115988</td>\n",
       "      <td>2156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2157</th>\n",
       "      <td>1.281675</td>\n",
       "      <td>2157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2158</th>\n",
       "      <td>-0.930885</td>\n",
       "      <td>2158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2159</th>\n",
       "      <td>-0.764861</td>\n",
       "      <td>2159</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2160 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      prediction    id\n",
       "0      -1.192794     0\n",
       "1      -0.823539     1\n",
       "2      -0.486877     2\n",
       "3      14.557883     3\n",
       "4     131.272003     4\n",
       "...          ...   ...\n",
       "2155    5.461376  2155\n",
       "2156    3.115988  2156\n",
       "2157    1.281675  2157\n",
       "2158   -0.930885  2158\n",
       "2159   -0.764861  2159\n",
       "\n",
       "[2160 rows x 2 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save y_pred series to csv as \"id\", \"prediction\"\n",
    "y_pred_df = pd.DataFrame(y_pred)\n",
    "y_pred_df.rename(columns={\"y\": \"prediction\"}, inplace=True)\n",
    "y_pred_df[\"id\"] = y_pred_df.index\n",
    "y_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last submission number: 60\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Save the submission DataFrame to submissions folder, create new name based on last submission, format is submission_<last_submission_number + 1>.csv\n",
    "# Get the last submission number\n",
    "last_submission_number = int(max([int(filename.split('_')[1].split('.')[0]) for filename in os.listdir('submissions') if \"submission\" in filename]))\n",
    "print(\"Last submission number:\", last_submission_number)\n",
    "\n",
    "# Create the new filename\n",
    "new_filename = f'submission_{last_submission_number + 1}.csv'\n",
    "\n",
    "# Save the submission\n",
    "y_pred_df.to_csv(os.path.join('submissions', new_filename), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
