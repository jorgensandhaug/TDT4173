{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Processing location A...\n",
      "Shape of X_train_observed before dropping in-between hour rows:  (118669, 45)\n",
      "HEIHEI: X_train_observed gaps in dates:  0\n",
      "HEIHEI: X_train_observed first gap in dates:  DatetimeIndex([], dtype='datetime64[us]', name='ds', freq=None)\n",
      "HEIHEI: X_train_observed list of size (in days) of each gap:  []\n",
      "HEIHEI: X_train_observed gaps in dates after filling missing dates:  0\n",
      "Shape of X_train_observed after dropping in-between hour rows:  (29668, 45)\n",
      "Shape of X_train_estimated before dropping in-between hour rows:  (17576, 46)\n",
      "HEIHEI: X_train_estimated gaps in dates:  1\n",
      "HEIHEI: X_train_estimated first gap in dates:  DatetimeIndex(['2023-01-27'], dtype='datetime64[us]', name='ds', freq=None)\n",
      "HEIHEI: X_train_estimated list of size (in days) of each gap:  [1.01041667]\n",
      "HEIHEI: X_train_estimated gaps in dates after filling missing dates:  0\n",
      "Shape of X_train_estimated after dropping in-between hour rows:  (4418, 46)\n",
      "Shape of X_test before dropping in-between hour rows:  (2880, 46)\n",
      "HEIHEI: X_test gaps in dates:  17\n",
      "HEIHEI: X_test first gap in dates:  DatetimeIndex(['2023-05-06'], dtype='datetime64[us]', name='ds', freq=None)\n",
      "HEIHEI: X_test list of size (in days) of each gap:  [4.01041667 7.01041667 3.01041667 1.01041667 1.01041667 1.01041667\n",
      " 1.01041667 1.01041667 1.01041667 2.01041667 1.01041667 1.01041667\n",
      " 3.01041667 2.01041667 3.01041667 1.01041667 1.01041667]\n",
      "HEIHEI: X_test gaps in dates after filling missing dates:  0\n",
      "Shape of X_test after dropping in-between hour rows:  (1536, 46)\n",
      "X_train_observed shape: (29668, 46)\n",
      "X_train_estimated shape: (4418, 46)\n",
      "X_test shape: (1536, 46)\n",
      "y_train shape: (34085, 1)\n",
      "y_train columns:  Index(['y'], dtype='object')\n",
      "Shape of y_train before filling missing dates:  (34085, 1)\n",
      "Shape of y_train after filling missing dates:  (34274, 1)\n",
      "LOOK: Number of gaps in X_train_observed plus number of gaps in X_train_estimated before:  0\n",
      "LOOK: Number of gaps in X_train_observed plus number of gaps in X_train_estimated after:  1\n",
      "LOOK: list of size (in days) of each gap:  [7.875]\n",
      "if the number is bigger after than before that means there is a gap in time between the observed and estimated training sets\n",
      "X_train dates info:  2019-06-02 22:00:00 2023-04-30 23:00:00 1428 days 01:00:00\n",
      "X_test dates info:  2023-05-01 00:00:00 2023-07-03 23:00:00 63 days 23:00:00\n",
      "y_train dates info:  2019-06-02 22:00:00 2023-04-30 23:00:00 1428 days 01:00:00\n",
      "X_train gaps in dates:  1\n",
      "X_test gaps in dates:  0\n",
      "y_train gaps in dates:  0\n",
      "X_train gaps in dates after filling missing dates:  0\n",
      "X_test gaps in dates after filling missing dates:  0\n",
      "Number of missing values in X_train:  53521\n",
      "Number of missing values in X_test:  38573\n",
      "Number of missing values in y_train:  189\n",
      "Number of missing values in X_train after merging with y_train:  53521\n",
      "Final shape of X_train for location A:  (34274, 48)\n",
      "Final shape of X_test for location A:  (1536, 47)\n",
      "\n",
      "\n",
      "\n",
      "Processing location B...\n",
      "Shape of X_train_observed before dropping in-between hour rows:  (116929, 45)\n",
      "HEIHEI: X_train_observed gaps in dates:  0\n",
      "HEIHEI: X_train_observed first gap in dates:  DatetimeIndex([], dtype='datetime64[us]', name='ds', freq=None)\n",
      "HEIHEI: X_train_observed list of size (in days) of each gap:  []\n",
      "HEIHEI: X_train_observed gaps in dates after filling missing dates:  0\n",
      "Shape of X_train_observed after dropping in-between hour rows:  (29233, 45)\n",
      "Shape of X_train_estimated before dropping in-between hour rows:  (17576, 46)\n",
      "HEIHEI: X_train_estimated gaps in dates:  1\n",
      "HEIHEI: X_train_estimated first gap in dates:  DatetimeIndex(['2023-01-27'], dtype='datetime64[us]', name='ds', freq=None)\n",
      "HEIHEI: X_train_estimated list of size (in days) of each gap:  [1.01041667]\n",
      "HEIHEI: X_train_estimated gaps in dates after filling missing dates:  0\n",
      "Shape of X_train_estimated after dropping in-between hour rows:  (4418, 46)\n",
      "Shape of X_test before dropping in-between hour rows:  (2880, 46)\n",
      "HEIHEI: X_test gaps in dates:  17\n",
      "HEIHEI: X_test first gap in dates:  DatetimeIndex(['2023-05-06'], dtype='datetime64[us]', name='ds', freq=None)\n",
      "HEIHEI: X_test list of size (in days) of each gap:  [4.01041667 7.01041667 3.01041667 1.01041667 1.01041667 1.01041667\n",
      " 1.01041667 1.01041667 1.01041667 2.01041667 1.01041667 1.01041667\n",
      " 3.01041667 2.01041667 3.01041667 1.01041667 1.01041667]\n",
      "HEIHEI: X_test gaps in dates after filling missing dates:  0\n",
      "Shape of X_test after dropping in-between hour rows:  (1536, 46)\n",
      "X_train_observed shape: (29233, 46)\n",
      "X_train_estimated shape: (4418, 46)\n",
      "X_test shape: (1536, 46)\n",
      "y_train shape: (32848, 1)\n",
      "y_train columns:  Index(['y'], dtype='object')\n",
      "Shape of y_train before filling missing dates:  (32848, 1)\n",
      "Shape of y_train after filling missing dates:  (37945, 1)\n",
      "LOOK: Number of gaps in X_train_observed plus number of gaps in X_train_estimated before:  0\n",
      "LOOK: Number of gaps in X_train_observed plus number of gaps in X_train_estimated after:  1\n",
      "LOOK: list of size (in days) of each gap:  [178.91666667]\n",
      "if the number is bigger after than before that means there is a gap in time between the observed and estimated training sets\n",
      "X_train dates info:  2019-01-01 00:00:00 2023-04-30 23:00:00 1580 days 23:00:00\n",
      "X_test dates info:  2023-05-01 00:00:00 2023-07-03 23:00:00 63 days 23:00:00\n",
      "y_train dates info:  2018-12-31 23:00:00 2023-04-30 23:00:00 1581 days 00:00:00\n",
      "X_train gaps in dates:  1\n",
      "X_test gaps in dates:  0\n",
      "y_train gaps in dates:  0\n",
      "X_train gaps in dates after filling missing dates:  0\n",
      "X_test gaps in dates after filling missing dates:  0\n",
      "Number of missing values in X_train:  239726\n",
      "Number of missing values in X_test:  38553\n",
      "Number of missing values in y_train:  5101\n",
      "Number of missing values in X_train after merging with y_train:  239772\n",
      "Final shape of X_train for location B:  (37945, 48)\n",
      "Final shape of X_test for location B:  (1536, 47)\n",
      "\n",
      "\n",
      "\n",
      "Processing location C...\n",
      "Shape of X_train_observed before dropping in-between hour rows:  (116825, 45)\n",
      "HEIHEI: X_train_observed gaps in dates:  0\n",
      "HEIHEI: X_train_observed first gap in dates:  DatetimeIndex([], dtype='datetime64[us]', name='ds', freq=None)\n",
      "HEIHEI: X_train_observed list of size (in days) of each gap:  []\n",
      "HEIHEI: X_train_observed gaps in dates after filling missing dates:  0\n",
      "Shape of X_train_observed after dropping in-between hour rows:  (29207, 45)\n",
      "Shape of X_train_estimated before dropping in-between hour rows:  (17576, 46)\n",
      "HEIHEI: X_train_estimated gaps in dates:  1\n",
      "HEIHEI: X_train_estimated first gap in dates:  DatetimeIndex(['2023-01-27'], dtype='datetime64[us]', name='ds', freq=None)\n",
      "HEIHEI: X_train_estimated list of size (in days) of each gap:  [1.01041667]\n",
      "HEIHEI: X_train_estimated gaps in dates after filling missing dates:  0\n",
      "Shape of X_train_estimated after dropping in-between hour rows:  (4418, 46)\n",
      "Shape of X_test before dropping in-between hour rows:  (2880, 46)\n",
      "HEIHEI: X_test gaps in dates:  17\n",
      "HEIHEI: X_test first gap in dates:  DatetimeIndex(['2023-05-06'], dtype='datetime64[us]', name='ds', freq=None)\n",
      "HEIHEI: X_test list of size (in days) of each gap:  [4.01041667 7.01041667 3.01041667 1.01041667 1.01041667 1.01041667\n",
      " 1.01041667 1.01041667 1.01041667 2.01041667 1.01041667 1.01041667\n",
      " 3.01041667 2.01041667 3.01041667 1.01041667 1.01041667]\n",
      "HEIHEI: X_test gaps in dates after filling missing dates:  0\n",
      "Shape of X_test after dropping in-between hour rows:  (1536, 46)\n",
      "X_train_observed shape: (29207, 46)\n",
      "X_train_estimated shape: (4418, 46)\n",
      "X_test shape: (1536, 46)\n",
      "y_train shape: (32155, 1)\n",
      "y_train columns:  Index(['y'], dtype='object')\n",
      "Shape of y_train before filling missing dates:  (32155, 1)\n",
      "Shape of y_train after filling missing dates:  (37945, 1)\n",
      "LOOK: Number of gaps in X_train_observed plus number of gaps in X_train_estimated before:  0\n",
      "LOOK: Number of gaps in X_train_observed plus number of gaps in X_train_estimated after:  1\n",
      "LOOK: list of size (in days) of each gap:  [180.]\n",
      "if the number is bigger after than before that means there is a gap in time between the observed and estimated training sets\n",
      "X_train dates info:  2019-01-01 00:00:00 2023-04-30 23:00:00 1580 days 23:00:00\n",
      "X_test dates info:  2023-05-01 00:00:00 2023-07-03 23:00:00 63 days 23:00:00\n",
      "y_train dates info:  2018-12-31 23:00:00 2023-04-30 23:00:00 1581 days 00:00:00\n",
      "X_train gaps in dates:  1\n",
      "X_test gaps in dates:  0\n",
      "y_train gaps in dates:  0\n",
      "X_train gaps in dates after filling missing dates:  0\n",
      "X_test gaps in dates after filling missing dates:  0\n",
      "Number of missing values in X_train:  240647\n",
      "Number of missing values in X_test:  38610\n",
      "Number of missing values in y_train:  11850\n",
      "Number of missing values in X_train after merging with y_train:  240693\n",
      "Final shape of X_train for location C:  (37945, 48)\n",
      "Final shape of X_test for location C:  (1536, 47)\n",
      "Final shape of X_train:  (110164, 48)\n",
      "Final shape of X_test:  (4608, 47)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from darts import TimeSeries\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def fix_datetime(X, name):\n",
    "    \"\"\"\n",
    "    Function to fix and standardize datetime in the given DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: DataFrame to be modified.\n",
    "    - name: String representing the name of the DataFrame, used for logging.\n",
    "    \n",
    "    Returns:\n",
    "    - Modified DataFrame with standardized datetime.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert 'date_forecast' to datetime format and replace original column with 'ds'\n",
    "    X['ds'] = pd.to_datetime(X['date_forecast'])\n",
    "    X.drop(columns=['date_forecast'], inplace=True, errors='ignore')\n",
    "\n",
    "    # Sort DataFrame by the new datetime column ('ds') and set it as the index\n",
    "    X.sort_values(by='ds', inplace=True)\n",
    "    X.set_index('ds', inplace=True)\n",
    "\n",
    "    # Log the shape of the DataFrame before dropping rows with in-between minutes\n",
    "    print(f\"Shape of {name} before dropping in-between hour rows: \", X.shape)\n",
    "\n",
    "    # Identify and log gaps in the date sequence\n",
    "    print(f\"HEIHEI: {name} gaps in dates: \", X.index.to_series().diff().dt.total_seconds().gt(60*15).sum())\n",
    "    print(f\"HEIHEI: {name} first gap in dates: \", X[X.index.to_series().diff().dt.total_seconds().gt(60*15)==True].index[:1])\n",
    "\n",
    "    # Calculate and log the size of each gap in the date sequence\n",
    "    temp = X.index.to_series().diff().dt.total_seconds()\n",
    "    if temp.shape[0] > 0:\n",
    "        print(f\"HEIHEI: {name} list of size (in days) of each gap: \", temp[temp.gt(60*15)].values / (60*60*24))\n",
    "    \n",
    "    # temporarily transform into darts time series to fill missing dates\n",
    "    # get date_calc if date_calc is column in X\n",
    "    temp_calc = None\n",
    "    if \"date_calc\" in X.columns:\n",
    "        temp_calc = X[\"date_calc\"]\n",
    "        X.drop(columns=['date_calc'], inplace=True)\n",
    "    X = TimeSeries.from_dataframe(df=X, freq=\"15T\", fill_missing_dates=True, fillna_value=None).pd_dataframe()\n",
    "    if temp_calc is not None:\n",
    "        X[\"date_calc\"] = temp_calc\n",
    "\n",
    "    print(f\"HEIHEI: {name} gaps in dates after filling missing dates: \", X.index.to_series().diff().dt.total_seconds().gt(60*15).sum())\n",
    "\n",
    "\n",
    "    # Drop rows where the minute part of the time is not 0\n",
    "    X = X[X.index.minute == 0]\n",
    "\n",
    "    # Log the shape of the DataFrame after dropping rows with in-between minutes\n",
    "    print(f\"Shape of {name} after dropping in-between hour rows: \", X.shape)\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "\n",
    "def convert_to_datetime(X_train_observed, X_train_estimated, X_test, y_train):\n",
    "    X_train_observed = fix_datetime(X_train_observed, \"X_train_observed\")\n",
    "    X_train_estimated = fix_datetime(X_train_estimated, \"X_train_estimated\")\n",
    "    X_test = fix_datetime(X_test, \"X_test\")\n",
    "\n",
    "\n",
    "    X_train_observed[\"estimated_diff_hours\"] = 0\n",
    "    X_train_estimated[\"estimated_diff_hours\"] = (X_train_estimated.index - pd.to_datetime(X_train_estimated[\"date_calc\"])).dt.total_seconds() / 3600\n",
    "    X_test[\"estimated_diff_hours\"] = (X_test.index - pd.to_datetime(X_test[\"date_calc\"])).dt.total_seconds() / 3600\n",
    "\n",
    "    X_train_estimated.drop(columns=['date_calc'], inplace=True)\n",
    "    X_test.drop(columns=['date_calc'], inplace=True)\n",
    "\n",
    "    y_train['ds'] = pd.to_datetime(y_train['time'])\n",
    "    y_train.drop(columns=['time'], inplace=True)\n",
    "    y_train.sort_values(by='ds', inplace=True)\n",
    "    y_train.set_index('ds', inplace=True)\n",
    "\n",
    "    return X_train_observed, X_train_estimated, X_test, y_train\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# location_map = {\n",
    "#     \"A\": 0,\n",
    "#     \"B\": 1,\n",
    "#     \"C\": 2\n",
    "# }\n",
    "\n",
    "\n",
    "def preprocess_data(X_train_observed, X_train_estimated, X_test, y_train, location):\n",
    "    # convert to datetime\n",
    "    X_train_observed, X_train_estimated, X_test, y_train = convert_to_datetime(X_train_observed, X_train_estimated, X_test, y_train)\n",
    "\n",
    "\n",
    "    # # cast all columns to float64\n",
    "    # X_train = X_train.astype('float64')\n",
    "    # X_test = X_test.astype('float64')\n",
    "\n",
    "\n",
    "    print(f\"X_train_observed shape: {X_train_observed.shape}\")\n",
    "    print(f\"X_train_estimated shape: {X_train_estimated.shape}\")\n",
    "    print(f\"X_test shape: {X_test.shape}\")\n",
    "    print(f\"y_train shape: {y_train.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "    y_train[\"y\"] = y_train[\"pv_measurement\"].astype('float64')\n",
    "    y_train.drop(columns=['pv_measurement'], inplace=True)\n",
    "    print(\"y_train columns: \", y_train.columns)\n",
    "\n",
    "    # temporarily transform into darts time series to fill missing dates\n",
    "    print(\"Shape of y_train before filling missing dates: \", y_train.shape)\n",
    "    y_train = TimeSeries.from_dataframe(df=y_train, freq=\"H\", fill_missing_dates=True, fillna_value=None).pd_dataframe()\n",
    "    print(\"Shape of y_train after filling missing dates: \", y_train.shape)\n",
    "\n",
    "\n",
    "    # number of gaps in X_train_observed + X_train_estimated before\n",
    "    print(f\"LOOK: Number of gaps in X_train_observed plus number of gaps in X_train_estimated before: \", X_train_observed.index.to_series().diff().dt.total_seconds().gt(3600).sum() + X_train_estimated.index.to_series().diff().dt.total_seconds().gt(3600).sum())\n",
    "    X_train = pd.concat([X_train_observed, X_train_estimated])\n",
    "    print(f\"LOOK: Number of gaps in X_train_observed plus number of gaps in X_train_estimated after: \", X_train.index.to_series().diff().dt.total_seconds().gt(3600).sum())\n",
    "    # print size of gaps in X_train\n",
    "    temp = X_train.index.to_series().diff().dt.total_seconds()\n",
    "    if temp.shape[0] > 0:\n",
    "        print(\"LOOK: list of size (in days) of each gap: \", temp[temp.gt(3600)].values / (60*60*24))\n",
    "    print(\"if the number is bigger after than before that means there is a gap in time between the observed and estimated training sets\")\n",
    "\n",
    "    # print info on dates in X_train, and if there are any missing dates\n",
    "    print(\"X_train dates info: \", X_train.index.min(), X_train.index.max(), X_train.index.max() - X_train.index.min())\n",
    "    print(\"X_test dates info: \", X_test.index.min(), X_test.index.max(), X_test.index.max() - X_test.index.min())\n",
    "    print(\"y_train dates info: \", y_train.index.min(), y_train.index.max(), y_train.index.max() - y_train.index.min())\n",
    "\n",
    "    # any gaps in dates?\n",
    "    print(\"X_train gaps in dates: \", X_train.index.to_series().diff().dt.total_seconds().gt(3600).sum())\n",
    "    print(\"X_test gaps in dates: \", X_test.index.to_series().diff().dt.total_seconds().gt(3600).sum())\n",
    "    print(\"y_train gaps in dates: \", y_train.index.to_series().diff().dt.total_seconds().gt(3600).sum())\n",
    "\n",
    "    # temporarily transform into darts time series to fill missing dates\n",
    "    X_train = TimeSeries.from_dataframe(df=X_train, freq=\"H\", fill_missing_dates=True, fillna_value=None).pd_dataframe()\n",
    "    X_test = TimeSeries.from_dataframe(df=X_test, freq=\"H\", fill_missing_dates=True, fillna_value=None).pd_dataframe()\n",
    "    print(\"X_train gaps in dates after filling missing dates: \", X_train.index.to_series().diff().dt.total_seconds().gt(3600).sum())\n",
    "    print(\"X_test gaps in dates after filling missing dates: \", X_test.index.to_series().diff().dt.total_seconds().gt(3600).sum())\n",
    "\n",
    "    \n",
    "\n",
    "    # clip all y values to 0 if negative\n",
    "    y_train[\"y\"] = y_train[\"y\"].clip(lower=0)\n",
    "    \n",
    "    # print Number of missing values in X train\n",
    "    print(\"Number of missing values in X_train: \", X_train.isnull().sum().sum())\n",
    "    print(\"Number of missing values in X_test: \", X_test.isnull().sum().sum())\n",
    "    # y_train missing values\n",
    "    print(\"Number of missing values in y_train: \", y_train.isnull().sum().sum())\n",
    "    X_train = pd.merge(X_train, y_train, how=\"outer\", left_index=True, right_index=True)\n",
    "    print(\"Number of missing values in X_train after merging with y_train: \", X_train.drop(columns=['y']).isnull().sum().sum())\n",
    "\n",
    "\n",
    "\n",
    "    X_train[\"location\"] = location\n",
    "    X_test[\"location\"] = location\n",
    "    \n",
    "    return X_train, X_test\n",
    "    \n",
    "\n",
    "\n",
    "# Define locations\n",
    "locations = ['A', 'B', 'C']\n",
    "\n",
    "X_trains = []\n",
    "X_tests = []\n",
    "y_trains = []\n",
    "# Loop through locations\n",
    "for loc in locations:\n",
    "    print(\"\\n\\n\")\n",
    "    print(f\"Processing location {loc}...\")\n",
    "    # Read target training data\n",
    "    y_train = pd.read_parquet(f'{loc}/train_targets.parquet')\n",
    "    \n",
    "    # Read estimated training data and add location feature\n",
    "    X_train_estimated = pd.read_parquet(f'{loc}/X_train_estimated.parquet')\n",
    "    \n",
    "    # Read observed training data and add location feature\n",
    "    X_train_observed= pd.read_parquet(f'{loc}/X_train_observed.parquet')\n",
    "\n",
    "    # Read estimated test data and add location feature\n",
    "    X_test_estimated = pd.read_parquet(f'{loc}/X_test_estimated.parquet')\n",
    "    \n",
    "    # Concatenate observed and estimated datasets for each location\n",
    "    #X_train = pd.concat([X_train_estimated, X_train_observed])\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # Preprocess data\n",
    "    X_train, X_test = preprocess_data(X_train_observed, X_train_estimated, X_test_estimated, y_train, loc)\n",
    "\n",
    "    print(f\"Final shape of X_train for location {loc}: \", X_train.shape)\n",
    "    print(f\"Final shape of X_test for location {loc}: \", X_test.shape)\n",
    "\n",
    "    # print(y_train.head(), y_train.shape)\n",
    "    # print(X_train.head(), X_train.shape)\n",
    "    # print(X_train.head(), X_train.shape)\n",
    "    # print(type(X_train['y']))\n",
    "\n",
    "    # Save data to csv\n",
    "    X_train.to_csv(f'{loc}/X_train.csv', index=True)\n",
    "    X_test.to_csv(f'{loc}/X_test.csv', index=True)\n",
    "\n",
    "\n",
    "    X_trains.append(X_train)\n",
    "    X_tests.append(X_test)\n",
    "\n",
    "# Concatenate all data and save to csv\n",
    "X_train = pd.concat(X_trains)\n",
    "X_test = pd.concat(X_tests)\n",
    "\n",
    "print(f\"Final shape of X_train: \", X_train.shape)\n",
    "print(f\"Final shape of X_test: \", X_test.shape)\n",
    "\n",
    "X_train.dropna(subset=['y'], inplace=True)\n",
    "X_train.to_csv('X_train_raw.csv', index=True)\n",
    "X_test.to_csv('X_test_raw.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = X_train.copy()\n",
    "test_df = X_test.copy()\n",
    "\n",
    "\n",
    "# add sin and cos of sun_elevation:d and sun_azimuth:d\n",
    "df['sin_sun_elevation'] = np.sin(np.deg2rad(df['sun_elevation:d']))\n",
    "\n",
    "test_df['sin_sun_elevation'] = np.sin(np.deg2rad(test_df['sun_elevation:d']))\n",
    "\n",
    "# add global_rad_1h:J = diffuse_rad_1h:J + direct_rad_1h:J\n",
    "df['global_rad_1h:J'] = df['diffuse_rad_1h:J'] + df['direct_rad_1h:J']\n",
    "test_df['global_rad_1h:J'] = test_df['diffuse_rad_1h:J'] + test_df['direct_rad_1h:J']\n",
    "\n",
    "# dew_or_rime:idx, Change this to one variable for is_dew and one variable for is_rime (dew:1, rime:-1)\n",
    "df['is_dew'] = df['dew_or_rime:idx'].apply(lambda x: 1 if x == 1 else 0)\n",
    "df['is_rime'] = df['dew_or_rime:idx'].apply(lambda x: 1 if x == -1 else 0)\n",
    "\n",
    "test_df['is_dew'] = test_df['dew_or_rime:idx'].apply(lambda x: 1 if x == 1 else 0)\n",
    "test_df['is_rime'] = test_df['dew_or_rime:idx'].apply(lambda x: 1 if x == -1 else 0)\n",
    "\n",
    "\n",
    "EXOGENOUS = [\n",
    "    'estimated_diff_hours',\n",
    "    \"absolute_humidity_2m:gm3\",\n",
    "    \"air_density_2m:kgm3\",\n",
    "    \"dew_point_2m:K\",\n",
    "    \"diffuse_rad_1h:J\",\n",
    "    \"direct_rad_1h:J\",\n",
    "    \"effective_cloud_cover:p\",\n",
    "    \"fresh_snow_1h:cm\",\n",
    "    \"snow_depth:cm\",\n",
    "    \"sun_elevation:d\",\n",
    "    \"sun_azimuth:d\",\n",
    "    \"t_1000hPa:K\",\n",
    "    \"visibility:m\",\n",
    "    \"wind_speed_10m:ms\",\n",
    "    \"is_dew\",\n",
    "    \"is_rime\",\n",
    "    \"sin_sun_elevation\",\n",
    "    \"global_rad_1h:J\",\n",
    "    ]\n",
    "#additional_features_for_testing = \n",
    "\n",
    "df = df[EXOGENOUS + [\"y\", \"location\"]]\n",
    "test_df = test_df[EXOGENOUS+ [\"location\"]]\n",
    "\n",
    "# save to X_train_feature_engineered.csv\n",
    "df.to_csv('X_train_feature_engineered.csv', index=True)\n",
    "test_df.to_csv('X_test_feature_engineered.csv', index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last submission number: 70\n",
      "Now creating submission number: 71\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Get the last submission number\n",
    "last_submission_number = int(max([int(filename.split('_')[1].split('.')[0]) for filename in os.listdir('submissions') if \"submission\" in filename]))\n",
    "print(\"Last submission number:\", last_submission_number)\n",
    "print(\"Now creating submission number:\", last_submission_number + 1)\n",
    "\n",
    "# Create the new filename\n",
    "new_filename = f'submission_{last_submission_number + 1}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "train_data = TabularDataset('X_train_raw.csv')\n",
    "train_data.drop(columns=['ds'], inplace=True)\n",
    "\n",
    "label = 'y'\n",
    "metric = 'mean_absolute_error'\n",
    "time_limit = 60*60*1.5\n",
    "presets = 'best_quality'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = [None, None, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=20\n",
      "Beginning AutoGluon training ... Time limit = 7200s\n",
      "AutoGluon will save models to \"AutogluonModels/submission_71_A\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.10.12\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 22.1.0: Sun Oct  9 20:15:09 PDT 2022; root:xnu-8792.41.9~2/RELEASE_ARM64_T6000\n",
      "Disk Space Avail:   13.09 GB / 494.38 GB (2.6%)\n",
      "Train Data Rows:    34085\n",
      "Train Data Columns: 47\n",
      "Label Column: y\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (5733.42, 0.0, 630.59471, 1165.90242)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    3740.05 MB\n",
      "\tTrain Data (Original)  Memory Usage: 14.52 MB (0.4% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for location A...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tUseless Original Features (Count: 1): ['location']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['snow_drift:idx']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 1 | ['snow_drift:idx']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 45 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 43 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', ...]\n",
      "\t\t('int', ['bool']) :  2 | ['elevation:m', 'snow_density:kgm3']\n",
      "\t0.4s = Fit runtime\n",
      "\t45 features in original data used to generate 45 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 11.79 MB (0.3% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.41s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 4798.53s of the 7199.59s of remaining time.\n",
      "\t-299.7196\t = Validation score   (-mean_absolute_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.84s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 4796.3s of the 7197.36s of remaining time.\n",
      "\t-300.7579\t = Validation score   (-mean_absolute_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.9s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 4795.32s of the 7196.38s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-170.8807\t = Validation score   (-mean_absolute_error)\n",
      "\t27.81s\t = Training   runtime\n",
      "\t63.37s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 4751.79s of the 7152.85s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-177.9357\t = Validation score   (-mean_absolute_error)\n",
      "\t34.28s\t = Training   runtime\n",
      "\t34.96s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 4707.21s of the 7108.28s of remaining time.\n",
      "\t-189.7909\t = Validation score   (-mean_absolute_error)\n",
      "\t31.82s\t = Training   runtime\n",
      "\t0.98s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 4673.89s of the 7074.96s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining model for location \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m predictor \u001b[38;5;241m=\u001b[39m \u001b[43mTabularPredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAutogluonModels/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mnew_filename\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mloc\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlocation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpresets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m predictors[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m predictor\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/core/utils/decorators.py:31\u001b[0m, in \u001b[0;36munpack.<locals>._unpack_inner.<locals>._call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     30\u001b[0m     gargs, gkwargs \u001b[38;5;241m=\u001b[39m g(\u001b[38;5;241m*\u001b[39mother_args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/predictor/predictor.py:986\u001b[0m, in \u001b[0;36mTabularPredictor.fit\u001b[0;34m(self, train_data, tuning_data, time_limit, presets, hyperparameters, feature_metadata, infer_limit, infer_limit_batch_size, fit_weighted_ensemble, calibrate_decision_threshold, num_cpus, num_gpus, **kwargs)\u001b[0m\n\u001b[1;32m    984\u001b[0m     aux_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_weighted_ensemble\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave(silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Save predictor to disk to enable prediction and training after interrupt\u001b[39;00m\n\u001b[0;32m--> 986\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_learner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    987\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    988\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtuning_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    989\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munlabeled_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    990\u001b[0m \u001b[43m    \u001b[49m\u001b[43mholdout_frac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mholdout_frac\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    991\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_bag_folds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_bag_folds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_bag_sets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_bag_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    993\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_stack_levels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_stack_levels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    994\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    995\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcore_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcore_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    996\u001b[0m \u001b[43m    \u001b[49m\u001b[43maux_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maux_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    997\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    998\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfer_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    999\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1000\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_bag_holdout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_bag_holdout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1002\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_post_fit_vars()\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_fit(\n\u001b[1;32m   1006\u001b[0m     keep_only_best\u001b[38;5;241m=\u001b[39mkwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeep_only_best\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   1007\u001b[0m     refit_full\u001b[38;5;241m=\u001b[39mkwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrefit_full\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     infer_limit\u001b[38;5;241m=\u001b[39minfer_limit,\n\u001b[1;32m   1013\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/learner/abstract_learner.py:159\u001b[0m, in \u001b[0;36mAbstractTabularLearner.fit\u001b[0;34m(self, X, X_val, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLearner is already fit.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_fit_input(X\u001b[38;5;241m=\u001b[39mX, X_val\u001b[38;5;241m=\u001b[39mX_val, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 159\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/learner/default_learner.py:157\u001b[0m, in \u001b[0;36mDefaultLearner._fit\u001b[0;34m(self, X, X_val, X_unlabeled, holdout_frac, num_bag_folds, num_bag_sets, time_limit, infer_limit, infer_limit_batch_size, verbosity, **trainer_fit_kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_metric \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39meval_metric\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave()\n\u001b[0;32m--> 157\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mholdout_frac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mholdout_frac\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_limit_trainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfer_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrainer_fit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_trainer(trainer\u001b[38;5;241m=\u001b[39mtrainer)\n\u001b[1;32m    171\u001b[0m time_end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/trainer/auto_trainer.py:114\u001b[0m, in \u001b[0;36mAutoTrainer.fit\u001b[0;34m(self, X, y, hyperparameters, X_val, y_val, X_unlabeled, holdout_frac, num_stack_levels, core_kwargs, aux_kwargs, time_limit, infer_limit, infer_limit_batch_size, use_bag_holdout, groups, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m log_str \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    112\u001b[0m logger\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m20\u001b[39m, log_str)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_multi_and_ensemble\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_stack_levels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_stack_levels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcore_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcore_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43maux_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maux_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfer_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py:2419\u001b[0m, in \u001b[0;36mAbstractTrainer._train_multi_and_ensemble\u001b[0;34m(self, X, y, X_val, y_val, hyperparameters, X_unlabeled, num_stack_levels, time_limit, groups, **kwargs)\u001b[0m\n\u001b[1;32m   2417\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_rows_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(X_val)\n\u001b[1;32m   2418\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_cols_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(X\u001b[38;5;241m.\u001b[39mcolumns))\n\u001b[0;32m-> 2419\u001b[0m model_names_fit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_multi_levels\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2420\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2421\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2422\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2423\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2424\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2425\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2426\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2427\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_stack_levels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2428\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2429\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2430\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_model_names()) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   2432\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoGluon did not successfully train any models\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py:383\u001b[0m, in \u001b[0;36mAbstractTrainer.train_multi_levels\u001b[0;34m(self, X, y, hyperparameters, X_val, y_val, X_unlabeled, base_model_names, core_kwargs, aux_kwargs, level_start, level_end, time_limit, name_suffix, relative_stack, level_time_modifier, infer_limit, infer_limit_batch_size)\u001b[0m\n\u001b[1;32m    381\u001b[0m         core_kwargs_level[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m core_kwargs_level\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m, time_limit_core)\n\u001b[1;32m    382\u001b[0m         aux_kwargs_level[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m aux_kwargs_level\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m, time_limit_aux)\n\u001b[0;32m--> 383\u001b[0m     base_model_names, aux_models \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack_new_level\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_model_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_model_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcore_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcore_kwargs_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m        \u001b[49m\u001b[43maux_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maux_kwargs_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_suffix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname_suffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m        \u001b[49m\u001b[43minfer_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m        \u001b[49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m     model_names_fit \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m base_model_names \u001b[38;5;241m+\u001b[39m aux_models\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_best \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(model_names_fit) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py:527\u001b[0m, in \u001b[0;36mAbstractTrainer.stack_new_level\u001b[0;34m(self, X, y, models, X_val, y_val, X_unlabeled, level, base_model_names, core_kwargs, aux_kwargs, name_suffix, infer_limit, infer_limit_batch_size)\u001b[0m\n\u001b[1;32m    525\u001b[0m     core_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname_suffix\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m core_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname_suffix\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m name_suffix\n\u001b[1;32m    526\u001b[0m     aux_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname_suffix\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m aux_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname_suffix\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m name_suffix\n\u001b[0;32m--> 527\u001b[0m core_models \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack_new_level_core\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfer_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_model_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_model_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcore_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X_val \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    542\u001b[0m     aux_models \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack_new_level_aux(\n\u001b[1;32m    543\u001b[0m         X\u001b[38;5;241m=\u001b[39mX, y\u001b[38;5;241m=\u001b[39my, base_model_names\u001b[38;5;241m=\u001b[39mcore_models, level\u001b[38;5;241m=\u001b[39mlevel \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, infer_limit\u001b[38;5;241m=\u001b[39minfer_limit, infer_limit_batch_size\u001b[38;5;241m=\u001b[39minfer_limit_batch_size, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39maux_kwargs\n\u001b[1;32m    544\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py:661\u001b[0m, in \u001b[0;36mAbstractTrainer.stack_new_level_core\u001b[0;34m(self, X, y, models, X_val, y_val, X_unlabeled, level, base_model_names, stack_name, ag_args, ag_args_fit, ag_args_ensemble, included_model_types, excluded_model_types, ensemble_type, name_suffix, get_models_func, refit_full, infer_limit, infer_limit_batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    658\u001b[0m fit_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes)\n\u001b[1;32m    660\u001b[0m \u001b[38;5;66;03m# FIXME: TODO: v0.1 X_unlabeled isn't cached so it won't be available during refit_full or fit_extra.\u001b[39;00m\n\u001b[0;32m--> 661\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_multi\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstack_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstack_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py:2369\u001b[0m, in \u001b[0;36mAbstractTrainer._train_multi\u001b[0;34m(self, X, y, models, hyperparameter_tune_kwargs, feature_prune_kwargs, k_fold, n_repeats, n_repeat_start, time_limit, **kwargs)\u001b[0m\n\u001b[1;32m   2367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_repeat_start \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   2368\u001b[0m     time_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m-> 2369\u001b[0m     model_names_trained \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_multi_initial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2370\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2371\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2372\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2373\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk_fold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk_fold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2374\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_repeats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_repeats_initial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhyperparameter_tune_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameter_tune_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2376\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_prune_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_prune_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2377\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2378\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2379\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2380\u001b[0m     n_repeat_start \u001b[38;5;241m=\u001b[39m n_repeats_initial\n\u001b[1;32m   2381\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m time_limit \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py:2218\u001b[0m, in \u001b[0;36mAbstractTrainer._train_multi_initial\u001b[0;34m(self, X, y, models, k_fold, n_repeats, hyperparameter_tune_kwargs, time_limit, feature_prune_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m   2216\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2217\u001b[0m     time_ratio \u001b[38;5;241m=\u001b[39m hpo_time_ratio \u001b[38;5;28;01mif\u001b[39;00m hpo_enabled \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 2218\u001b[0m     models \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_multi_fold\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhyperparameter_tune_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameter_tune_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk_fold_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk_fold_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk_fold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_repeats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_repeats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_repeat_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtime_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtime_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2228\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2229\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2231\u001b[0m multi_fold_time_elapsed \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m multi_fold_time_start\n\u001b[1;32m   2232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time_limit \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py:2326\u001b[0m, in \u001b[0;36mAbstractTrainer._train_multi_fold\u001b[0;34m(self, X, y, models, time_limit, time_split, time_ratio, hyperparameter_tune_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m   2324\u001b[0m         time_start_model \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   2325\u001b[0m         time_left \u001b[38;5;241m=\u001b[39m time_limit \u001b[38;5;241m-\u001b[39m (time_start_model \u001b[38;5;241m-\u001b[39m time_start)\n\u001b[0;32m-> 2326\u001b[0m model_name_trained_lst \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_single_full\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_left\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparameter_tune_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameter_tune_kwargs_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   2328\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m   2331\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m model\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py:2099\u001b[0m, in \u001b[0;36mAbstractTrainer._train_single_full\u001b[0;34m(self, X, y, model, X_unlabeled, X_val, y_val, X_pseudo, y_pseudo, feature_prune, hyperparameter_tune_kwargs, stack_name, k_fold, k_fold_start, k_fold_end, n_repeats, n_repeat_start, level, time_limit, fit_kwargs, compute_score, total_resources, **kwargs)\u001b[0m\n\u001b[1;32m   2095\u001b[0m         bagged_model_fit_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_bagged_model_fit_kwargs(\n\u001b[1;32m   2096\u001b[0m             k_fold\u001b[38;5;241m=\u001b[39mk_fold, k_fold_start\u001b[38;5;241m=\u001b[39mk_fold_start, k_fold_end\u001b[38;5;241m=\u001b[39mk_fold_end, n_repeats\u001b[38;5;241m=\u001b[39mn_repeats, n_repeat_start\u001b[38;5;241m=\u001b[39mn_repeat_start\n\u001b[1;32m   2097\u001b[0m         )\n\u001b[1;32m   2098\u001b[0m         model_fit_kwargs\u001b[38;5;241m.\u001b[39mupdate(bagged_model_fit_kwargs)\n\u001b[0;32m-> 2099\u001b[0m     model_names_trained \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_and_save\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2101\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2104\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstack_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstack_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2107\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompute_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_resources\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_resources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2110\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_fit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2111\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m   2113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_names_trained\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py:1761\u001b[0m, in \u001b[0;36mAbstractTrainer._train_and_save\u001b[0;34m(self, X, y, model, X_val, y_val, stack_name, level, compute_score, total_resources, **model_fit_kwargs)\u001b[0m\n\u001b[1;32m   1759\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_single(X_w_pseudo, y_w_pseudo, model, X_val, y_val, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_fit_kwargs)\n\u001b[1;32m   1760\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1761\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_resources\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_resources\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_fit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m fit_end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   1764\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_evaluation:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py:1712\u001b[0m, in \u001b[0;36mAbstractTrainer._train_single\u001b[0;34m(self, X, y, model, X_val, y_val, total_resources, **model_fit_kwargs)\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_train_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, model: AbstractModel, X_val\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, y_val\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, total_resources\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_fit_kwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AbstractModel:\n\u001b[1;32m   1708\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1709\u001b[0m \u001b[38;5;124;03m    Trains model but does not add the trained model to this Trainer.\u001b[39;00m\n\u001b[1;32m   1710\u001b[0m \u001b[38;5;124;03m    Returns trained model object.\u001b[39;00m\n\u001b[1;32m   1711\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1712\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_resources\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_resources\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_fit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1713\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py:838\u001b[0m, in \u001b[0;36mAbstractModel.fit\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    836\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate_fit_resources(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_fit_memory_usage(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 838\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    840\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py:165\u001b[0m, in \u001b[0;36mStackerEnsembleModel._fit\u001b[0;34m(self, X, y, compute_base_preds, time_limit, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time_limit \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    164\u001b[0m     time_limit \u001b[38;5;241m=\u001b[39m time_limit \u001b[38;5;241m-\u001b[39m (time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time)\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py:266\u001b[0m, in \u001b[0;36mBaggedEnsembleModel._fit\u001b[0;34m(self, X, y, X_val, y_val, X_pseudo, y_pseudo, k_fold, k_fold_start, k_fold_end, n_repeats, n_repeat_start, groups, _skip_oof, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[38;5;66;03m# Reserve time for final refit model\u001b[39;00m\n\u001b[1;32m    265\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m*\u001b[39m folds_to_fit \u001b[38;5;241m/\u001b[39m (folds_to_fit \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1.2\u001b[39m)\n\u001b[0;32m--> 266\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_folds\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_pseudo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_pseudo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_pseudo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_pseudo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk_fold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk_fold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk_fold_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk_fold_start\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk_fold_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk_fold_end\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_repeats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_repeats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_repeat_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_repeat_start\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_folds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_bag_folds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;66;03m# FIXME: Cleanup self\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;66;03m# FIXME: Support `can_refit_full=False` models\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m refit_folds:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py:592\u001b[0m, in \u001b[0;36mBaggedEnsembleModel._fit_folds\u001b[0;34m(self, X, y, model_base, X_pseudo, y_pseudo, k_fold, k_fold_start, k_fold_end, n_repeats, n_repeat_start, time_limit, sample_weight, save_folds, groups, num_cpus, num_gpus, **kwargs)\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fold_fit_args \u001b[38;5;129;01min\u001b[39;00m fold_fit_args_list:\n\u001b[1;32m    591\u001b[0m     fold_fitting_strategy\u001b[38;5;241m.\u001b[39mschedule_fold_model_fit(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfold_fit_args)\n\u001b[0;32m--> 592\u001b[0m \u001b[43mfold_fitting_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mafter_all_folds_scheduled\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m models:\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;66;03m# No need to add child times or save child here as this already occurred in the fold_fitting_strategy\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_child(model\u001b[38;5;241m=\u001b[39mmodel, add_child_times\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py:538\u001b[0m, in \u001b[0;36mParallelFoldFittingStrategy.after_all_folds_scheduled\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    536\u001b[0m unfinished \u001b[38;5;241m=\u001b[39m job_refs\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m unfinished:\n\u001b[0;32m--> 538\u001b[0m     finished, unfinished \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43munfinished\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_returns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    539\u001b[0m     finished \u001b[38;5;241m=\u001b[39m finished[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    540\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ag/lib/python3.10/site-packages/ray/_private/auto_init_hook.py:24\u001b[0m, in \u001b[0;36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mauto_init_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     23\u001b[0m     auto_init_ray()\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ag/lib/python3.10/site-packages/ray/_private/client_mode_hook.py:103\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ag/lib/python3.10/site-packages/ray/_private/worker.py:2732\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_refs, num_returns, timeout, fetch_local)\u001b[0m\n\u001b[1;32m   2730\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeout \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m6\u001b[39m\n\u001b[1;32m   2731\u001b[0m timeout_milliseconds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m-> 2732\u001b[0m ready_ids, remaining_ids \u001b[38;5;241m=\u001b[39m \u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobject_refs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2734\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_returns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout_milliseconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_task_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfetch_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2738\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2739\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ready_ids, remaining_ids\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:3012\u001b[0m, in \u001b[0;36mray._raylet.CoreWorker.wait\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:400\u001b[0m, in \u001b[0;36mray._raylet.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loc = \"A\"\n",
    "print(f\"Training model for location {loc}...\")\n",
    "predictor = TabularPredictor(label=label, eval_metric=metric, path=f\"AutogluonModels/{new_filename}_{loc}\").fit(train_data[train_data[\"location\"] == loc], time_limit=time_limit, presets=presets)\n",
    "predictors[0] = predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loc = \"B\"\n",
    "print(f\"Training model for location {loc}...\")\n",
    "predictor = TabularPredictor(label=label, eval_metric=metric, path=f\"AutogluonModels/{new_filename}_{loc}\").fit(train_data[train_data[\"location\"] == loc], time_limit=time_limit, presets=presets)\n",
    "predictors[1] = predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = \"C\"\n",
    "print(f\"Training model for location {loc}...\")\n",
    "predictor = TabularPredictor(label=label, eval_metric=metric, path=f\"AutogluonModels/{new_filename}_{loc}\").fit(train_data[train_data[\"location\"] == loc], time_limit=time_limit, presets=presets)\n",
    "predictors[2] = predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_data_with_dates = TabularDataset('X_train_raw.csv')\n",
    "train_data_with_dates[\"ds\"] = pd.to_datetime(train_data_with_dates[\"ds\"])\n",
    "\n",
    "test_data = TabularDataset('X_test_raw.csv')\n",
    "test_data[\"ds\"] = pd.to_datetime(test_data[\"ds\"])\n",
    "#test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = TabularDataset('test.csv')\n",
    "test_ids[\"time\"] = pd.to_datetime(test_ids[\"time\"])\n",
    "# merge test_data with test_ids\n",
    "test_data_merged = pd.merge(test_data, test_ids, how=\"inner\", right_on=[\"time\", \"location\"], left_on=[\"ds\", \"location\"])\n",
    "\n",
    "#test_data_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict, grouped by location\n",
    "predictions = []\n",
    "location_map = {\n",
    "    \"A\": 0,\n",
    "    \"B\": 1,\n",
    "    \"C\": 2\n",
    "}\n",
    "for loc, group in test_data.groupby('location'):\n",
    "    i = location_map[loc]\n",
    "    subset = test_data_merged[test_data_merged[\"location\"] == loc].reset_index(drop=True)\n",
    "    #print(subset)\n",
    "    pred = predictors[i].predict(subset)\n",
    "    subset[\"prediction\"] = pred\n",
    "    predictions.append(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot predictions for location A, in addition to train data for A\n",
    "for loc, idx in location_map.items():\n",
    "    fig, ax = plt.subplots(figsize=(20, 10))\n",
    "    # plot train data\n",
    "    train_data_with_dates[train_data_with_dates[\"location\"]==loc].plot(x='ds', y='y', ax=ax, label=\"train data\")\n",
    "\n",
    "    # plot predictions\n",
    "    predictions[idx].plot(x='ds', y='prediction', ax=ax, label=\"predictions\")\n",
    "\n",
    "    # title\n",
    "    ax.set_title(f\"Predictions for location {loc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate predictions\n",
    "submissions_df = pd.concat(predictions)\n",
    "submissions_df = submissions_df[[\"id\", \"prediction\"]]\n",
    "submissions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Save the submission DataFrame to submissions folder, create new name based on last submission, format is submission_<last_submission_number + 1>.csv\n",
    "\n",
    "# Save the submission\n",
    "print(f\"Saving submission to submissions/{new_filename}.csv\")\n",
    "submissions_df.to_csv(os.path.join('submissions', f\"{new_filename}.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
