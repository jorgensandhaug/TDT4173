{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Processing location A...\n",
      "Shape of X_train_observed before dropping in-between hour rows:  (118669, 45)\n",
      "HEIHEI: X_train_observed gaps in dates:  0\n",
      "HEIHEI: X_train_observed first gap in dates:  DatetimeIndex([], dtype='datetime64[us]', name='ds', freq=None)\n",
      "HEIHEI: X_train_observed list of size (in days) of each gap:  []\n",
      "HEIHEI: X_train_observed gaps in dates after filling missing dates:  0\n",
      "Shape of X_train_observed after dropping in-between hour rows:  (29668, 45)\n",
      "Shape of X_train_estimated before dropping in-between hour rows:  (17576, 46)\n",
      "HEIHEI: X_train_estimated gaps in dates:  1\n",
      "HEIHEI: X_train_estimated first gap in dates:  DatetimeIndex(['2023-01-27'], dtype='datetime64[us]', name='ds', freq=None)\n",
      "HEIHEI: X_train_estimated list of size (in days) of each gap:  [1.01041667]\n",
      "HEIHEI: X_train_estimated gaps in dates after filling missing dates:  0\n",
      "Shape of X_train_estimated after dropping in-between hour rows:  (4418, 46)\n",
      "Shape of X_test before dropping in-between hour rows:  (2880, 46)\n",
      "HEIHEI: X_test gaps in dates:  17\n",
      "HEIHEI: X_test first gap in dates:  DatetimeIndex(['2023-05-06'], dtype='datetime64[us]', name='ds', freq=None)\n",
      "HEIHEI: X_test list of size (in days) of each gap:  [4.01041667 7.01041667 3.01041667 1.01041667 1.01041667 1.01041667\n",
      " 1.01041667 1.01041667 1.01041667 2.01041667 1.01041667 1.01041667\n",
      " 3.01041667 2.01041667 3.01041667 1.01041667 1.01041667]\n",
      "HEIHEI: X_test gaps in dates after filling missing dates:  0\n",
      "Shape of X_test after dropping in-between hour rows:  (1536, 46)\n",
      "X_train_observed shape: (29668, 46)\n",
      "X_train_estimated shape: (4418, 46)\n",
      "X_test shape: (1536, 46)\n",
      "y_train shape: (34085, 1)\n",
      "y_train columns:  Index(['y'], dtype='object')\n",
      "Shape of y_train before filling missing dates:  (34085, 1)\n",
      "Shape of y_train after filling missing dates:  (34274, 1)\n",
      "LOOK: Number of gaps in X_train_observed plus number of gaps in X_train_estimated before:  0\n",
      "LOOK: Number of gaps in X_train_observed plus number of gaps in X_train_estimated after:  1\n",
      "LOOK: list of size (in days) of each gap:  [7.875]\n",
      "if the number is bigger after than before that means there is a gap in time between the observed and estimated training sets\n",
      "X_train dates info:  2019-06-02 22:00:00 2023-04-30 23:00:00 1428 days 01:00:00\n",
      "X_test dates info:  2023-05-01 00:00:00 2023-07-03 23:00:00 63 days 23:00:00\n",
      "y_train dates info:  2019-06-02 22:00:00 2023-04-30 23:00:00 1428 days 01:00:00\n",
      "X_train gaps in dates:  1\n",
      "X_test gaps in dates:  0\n",
      "y_train gaps in dates:  0\n",
      "X_train gaps in dates after filling missing dates:  0\n",
      "X_test gaps in dates after filling missing dates:  0\n",
      "Number of missing values in X_train:  53521\n",
      "Number of missing values in X_test:  38573\n",
      "Number of missing values in y_train:  189\n",
      "Number of missing values in X_train after merging with y_train:  53521\n",
      "Final shape of X_train for location A:  (34274, 48)\n",
      "Final shape of X_test for location A:  (1536, 47)\n",
      "\n",
      "\n",
      "\n",
      "Processing location B...\n",
      "Shape of X_train_observed before dropping in-between hour rows:  (116929, 45)\n",
      "HEIHEI: X_train_observed gaps in dates:  0\n",
      "HEIHEI: X_train_observed first gap in dates:  DatetimeIndex([], dtype='datetime64[us]', name='ds', freq=None)\n",
      "HEIHEI: X_train_observed list of size (in days) of each gap:  []\n",
      "HEIHEI: X_train_observed gaps in dates after filling missing dates:  0\n",
      "Shape of X_train_observed after dropping in-between hour rows:  (29233, 45)\n",
      "Shape of X_train_estimated before dropping in-between hour rows:  (17576, 46)\n",
      "HEIHEI: X_train_estimated gaps in dates:  1\n",
      "HEIHEI: X_train_estimated first gap in dates:  DatetimeIndex(['2023-01-27'], dtype='datetime64[us]', name='ds', freq=None)\n",
      "HEIHEI: X_train_estimated list of size (in days) of each gap:  [1.01041667]\n",
      "HEIHEI: X_train_estimated gaps in dates after filling missing dates:  0\n",
      "Shape of X_train_estimated after dropping in-between hour rows:  (4418, 46)\n",
      "Shape of X_test before dropping in-between hour rows:  (2880, 46)\n",
      "HEIHEI: X_test gaps in dates:  17\n",
      "HEIHEI: X_test first gap in dates:  DatetimeIndex(['2023-05-06'], dtype='datetime64[us]', name='ds', freq=None)\n",
      "HEIHEI: X_test list of size (in days) of each gap:  [4.01041667 7.01041667 3.01041667 1.01041667 1.01041667 1.01041667\n",
      " 1.01041667 1.01041667 1.01041667 2.01041667 1.01041667 1.01041667\n",
      " 3.01041667 2.01041667 3.01041667 1.01041667 1.01041667]\n",
      "HEIHEI: X_test gaps in dates after filling missing dates:  0\n",
      "Shape of X_test after dropping in-between hour rows:  (1536, 46)\n",
      "X_train_observed shape: (29233, 46)\n",
      "X_train_estimated shape: (4418, 46)\n",
      "X_test shape: (1536, 46)\n",
      "y_train shape: (32848, 1)\n",
      "y_train columns:  Index(['y'], dtype='object')\n",
      "Shape of y_train before filling missing dates:  (32848, 1)\n",
      "Shape of y_train after filling missing dates:  (37945, 1)\n",
      "LOOK: Number of gaps in X_train_observed plus number of gaps in X_train_estimated before:  0\n",
      "LOOK: Number of gaps in X_train_observed plus number of gaps in X_train_estimated after:  1\n",
      "LOOK: list of size (in days) of each gap:  [178.91666667]\n",
      "if the number is bigger after than before that means there is a gap in time between the observed and estimated training sets\n",
      "X_train dates info:  2019-01-01 00:00:00 2023-04-30 23:00:00 1580 days 23:00:00\n",
      "X_test dates info:  2023-05-01 00:00:00 2023-07-03 23:00:00 63 days 23:00:00\n",
      "y_train dates info:  2018-12-31 23:00:00 2023-04-30 23:00:00 1581 days 00:00:00\n",
      "X_train gaps in dates:  1\n",
      "X_test gaps in dates:  0\n",
      "y_train gaps in dates:  0\n",
      "X_train gaps in dates after filling missing dates:  0\n",
      "X_test gaps in dates after filling missing dates:  0\n",
      "Number of missing values in X_train:  239726\n",
      "Number of missing values in X_test:  38553\n",
      "Number of missing values in y_train:  5101\n",
      "Number of missing values in X_train after merging with y_train:  239772\n",
      "Final shape of X_train for location B:  (37945, 48)\n",
      "Final shape of X_test for location B:  (1536, 47)\n",
      "\n",
      "\n",
      "\n",
      "Processing location C...\n",
      "Shape of X_train_observed before dropping in-between hour rows:  (116825, 45)\n",
      "HEIHEI: X_train_observed gaps in dates:  0\n",
      "HEIHEI: X_train_observed first gap in dates:  DatetimeIndex([], dtype='datetime64[us]', name='ds', freq=None)\n",
      "HEIHEI: X_train_observed list of size (in days) of each gap:  []\n",
      "HEIHEI: X_train_observed gaps in dates after filling missing dates:  0\n",
      "Shape of X_train_observed after dropping in-between hour rows:  (29207, 45)\n",
      "Shape of X_train_estimated before dropping in-between hour rows:  (17576, 46)\n",
      "HEIHEI: X_train_estimated gaps in dates:  1\n",
      "HEIHEI: X_train_estimated first gap in dates:  DatetimeIndex(['2023-01-27'], dtype='datetime64[us]', name='ds', freq=None)\n",
      "HEIHEI: X_train_estimated list of size (in days) of each gap:  [1.01041667]\n",
      "HEIHEI: X_train_estimated gaps in dates after filling missing dates:  0\n",
      "Shape of X_train_estimated after dropping in-between hour rows:  (4418, 46)\n",
      "Shape of X_test before dropping in-between hour rows:  (2880, 46)\n",
      "HEIHEI: X_test gaps in dates:  17\n",
      "HEIHEI: X_test first gap in dates:  DatetimeIndex(['2023-05-06'], dtype='datetime64[us]', name='ds', freq=None)\n",
      "HEIHEI: X_test list of size (in days) of each gap:  [4.01041667 7.01041667 3.01041667 1.01041667 1.01041667 1.01041667\n",
      " 1.01041667 1.01041667 1.01041667 2.01041667 1.01041667 1.01041667\n",
      " 3.01041667 2.01041667 3.01041667 1.01041667 1.01041667]\n",
      "HEIHEI: X_test gaps in dates after filling missing dates:  0\n",
      "Shape of X_test after dropping in-between hour rows:  (1536, 46)\n",
      "X_train_observed shape: (29207, 46)\n",
      "X_train_estimated shape: (4418, 46)\n",
      "X_test shape: (1536, 46)\n",
      "y_train shape: (32155, 1)\n",
      "y_train columns:  Index(['y'], dtype='object')\n",
      "Shape of y_train before filling missing dates:  (32155, 1)\n",
      "Shape of y_train after filling missing dates:  (37945, 1)\n",
      "LOOK: Number of gaps in X_train_observed plus number of gaps in X_train_estimated before:  0\n",
      "LOOK: Number of gaps in X_train_observed plus number of gaps in X_train_estimated after:  1\n",
      "LOOK: list of size (in days) of each gap:  [180.]\n",
      "if the number is bigger after than before that means there is a gap in time between the observed and estimated training sets\n",
      "X_train dates info:  2019-01-01 00:00:00 2023-04-30 23:00:00 1580 days 23:00:00\n",
      "X_test dates info:  2023-05-01 00:00:00 2023-07-03 23:00:00 63 days 23:00:00\n",
      "y_train dates info:  2018-12-31 23:00:00 2023-04-30 23:00:00 1581 days 00:00:00\n",
      "X_train gaps in dates:  1\n",
      "X_test gaps in dates:  0\n",
      "y_train gaps in dates:  0\n",
      "X_train gaps in dates after filling missing dates:  0\n",
      "X_test gaps in dates after filling missing dates:  0\n",
      "Number of missing values in X_train:  240647\n",
      "Number of missing values in X_test:  38610\n",
      "Number of missing values in y_train:  11850\n",
      "Number of missing values in X_train after merging with y_train:  240693\n",
      "Final shape of X_train for location C:  (37945, 48)\n",
      "Final shape of X_test for location C:  (1536, 47)\n",
      "Final shape of X_train:  (110164, 48)\n",
      "Final shape of X_test:  (4608, 47)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from darts import TimeSeries\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def fix_datetime(X, name):\n",
    "    \"\"\"\n",
    "    Function to fix and standardize datetime in the given DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: DataFrame to be modified.\n",
    "    - name: String representing the name of the DataFrame, used for logging.\n",
    "    \n",
    "    Returns:\n",
    "    - Modified DataFrame with standardized datetime.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert 'date_forecast' to datetime format and replace original column with 'ds'\n",
    "    X['ds'] = pd.to_datetime(X['date_forecast'])\n",
    "    X.drop(columns=['date_forecast'], inplace=True, errors='ignore')\n",
    "\n",
    "    # Sort DataFrame by the new datetime column ('ds') and set it as the index\n",
    "    X.sort_values(by='ds', inplace=True)\n",
    "    X.set_index('ds', inplace=True)\n",
    "\n",
    "    # Log the shape of the DataFrame before dropping rows with in-between minutes\n",
    "    print(f\"Shape of {name} before dropping in-between hour rows: \", X.shape)\n",
    "\n",
    "    # Identify and log gaps in the date sequence\n",
    "    print(f\"HEIHEI: {name} gaps in dates: \", X.index.to_series().diff().dt.total_seconds().gt(60*15).sum())\n",
    "    print(f\"HEIHEI: {name} first gap in dates: \", X[X.index.to_series().diff().dt.total_seconds().gt(60*15)==True].index[:1])\n",
    "\n",
    "    # Calculate and log the size of each gap in the date sequence\n",
    "    temp = X.index.to_series().diff().dt.total_seconds()\n",
    "    if temp.shape[0] > 0:\n",
    "        print(f\"HEIHEI: {name} list of size (in days) of each gap: \", temp[temp.gt(60*15)].values / (60*60*24))\n",
    "    \n",
    "    # temporarily transform into darts time series to fill missing dates\n",
    "    # get date_calc if date_calc is column in X\n",
    "    temp_calc = None\n",
    "    if \"date_calc\" in X.columns:\n",
    "        temp_calc = X[\"date_calc\"]\n",
    "        X.drop(columns=['date_calc'], inplace=True)\n",
    "    X = TimeSeries.from_dataframe(df=X, freq=\"15T\", fill_missing_dates=True, fillna_value=None).pd_dataframe()\n",
    "    if temp_calc is not None:\n",
    "        X[\"date_calc\"] = temp_calc\n",
    "\n",
    "    print(f\"HEIHEI: {name} gaps in dates after filling missing dates: \", X.index.to_series().diff().dt.total_seconds().gt(60*15).sum())\n",
    "\n",
    "\n",
    "    # Drop rows where the minute part of the time is not 0\n",
    "    X = X[X.index.minute == 0]\n",
    "\n",
    "    # Log the shape of the DataFrame after dropping rows with in-between minutes\n",
    "    print(f\"Shape of {name} after dropping in-between hour rows: \", X.shape)\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "\n",
    "def convert_to_datetime(X_train_observed, X_train_estimated, X_test, y_train):\n",
    "    X_train_observed = fix_datetime(X_train_observed, \"X_train_observed\")\n",
    "    X_train_estimated = fix_datetime(X_train_estimated, \"X_train_estimated\")\n",
    "    X_test = fix_datetime(X_test, \"X_test\")\n",
    "\n",
    "\n",
    "    X_train_observed[\"estimated_diff_hours\"] = 0\n",
    "    X_train_estimated[\"estimated_diff_hours\"] = (X_train_estimated.index - pd.to_datetime(X_train_estimated[\"date_calc\"])).dt.total_seconds() / 3600.0\n",
    "    X_test[\"estimated_diff_hours\"] = (X_test.index - pd.to_datetime(X_test[\"date_calc\"])).dt.total_seconds() / 3600.0\n",
    "\n",
    "    X_train_estimated.drop(columns=['date_calc'], inplace=True)\n",
    "    X_test.drop(columns=['date_calc'], inplace=True)\n",
    "\n",
    "    y_train['ds'] = pd.to_datetime(y_train['time'])\n",
    "    y_train.drop(columns=['time'], inplace=True)\n",
    "    y_train.sort_values(by='ds', inplace=True)\n",
    "    y_train.set_index('ds', inplace=True)\n",
    "\n",
    "    return X_train_observed, X_train_estimated, X_test, y_train\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# location_map = {\n",
    "#     \"A\": 0,\n",
    "#     \"B\": 1,\n",
    "#     \"C\": 2\n",
    "# }\n",
    "\n",
    "\n",
    "def preprocess_data(X_train_observed, X_train_estimated, X_test, y_train, location):\n",
    "    # convert to datetime\n",
    "    X_train_observed, X_train_estimated, X_test, y_train = convert_to_datetime(X_train_observed, X_train_estimated, X_test, y_train)\n",
    "\n",
    "\n",
    "    # # cast all columns to float64\n",
    "    # X_train = X_train.astype('float64')\n",
    "    # X_test = X_test.astype('float64')\n",
    "\n",
    "\n",
    "    print(f\"X_train_observed shape: {X_train_observed.shape}\")\n",
    "    print(f\"X_train_estimated shape: {X_train_estimated.shape}\")\n",
    "    print(f\"X_test shape: {X_test.shape}\")\n",
    "    print(f\"y_train shape: {y_train.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "    y_train[\"y\"] = y_train[\"pv_measurement\"].astype('float64')\n",
    "    y_train.drop(columns=['pv_measurement'], inplace=True)\n",
    "    print(\"y_train columns: \", y_train.columns)\n",
    "\n",
    "    # temporarily transform into darts time series to fill missing dates\n",
    "    print(\"Shape of y_train before filling missing dates: \", y_train.shape)\n",
    "    y_train = TimeSeries.from_dataframe(df=y_train, freq=\"H\", fill_missing_dates=True, fillna_value=None).pd_dataframe()\n",
    "    print(\"Shape of y_train after filling missing dates: \", y_train.shape)\n",
    "\n",
    "\n",
    "    # number of gaps in X_train_observed + X_train_estimated before\n",
    "    print(f\"LOOK: Number of gaps in X_train_observed plus number of gaps in X_train_estimated before: \", X_train_observed.index.to_series().diff().dt.total_seconds().gt(3600).sum() + X_train_estimated.index.to_series().diff().dt.total_seconds().gt(3600).sum())\n",
    "    X_train = pd.concat([X_train_observed, X_train_estimated])\n",
    "    print(f\"LOOK: Number of gaps in X_train_observed plus number of gaps in X_train_estimated after: \", X_train.index.to_series().diff().dt.total_seconds().gt(3600).sum())\n",
    "    # print size of gaps in X_train\n",
    "    temp = X_train.index.to_series().diff().dt.total_seconds()\n",
    "    if temp.shape[0] > 0:\n",
    "        print(\"LOOK: list of size (in days) of each gap: \", temp[temp.gt(3600)].values / (60*60*24))\n",
    "    print(\"if the number is bigger after than before that means there is a gap in time between the observed and estimated training sets\")\n",
    "\n",
    "    # print info on dates in X_train, and if there are any missing dates\n",
    "    print(\"X_train dates info: \", X_train.index.min(), X_train.index.max(), X_train.index.max() - X_train.index.min())\n",
    "    print(\"X_test dates info: \", X_test.index.min(), X_test.index.max(), X_test.index.max() - X_test.index.min())\n",
    "    print(\"y_train dates info: \", y_train.index.min(), y_train.index.max(), y_train.index.max() - y_train.index.min())\n",
    "\n",
    "    # any gaps in dates?\n",
    "    print(\"X_train gaps in dates: \", X_train.index.to_series().diff().dt.total_seconds().gt(3600).sum())\n",
    "    print(\"X_test gaps in dates: \", X_test.index.to_series().diff().dt.total_seconds().gt(3600).sum())\n",
    "    print(\"y_train gaps in dates: \", y_train.index.to_series().diff().dt.total_seconds().gt(3600).sum())\n",
    "\n",
    "    # temporarily transform into darts time series to fill missing dates\n",
    "    X_train = TimeSeries.from_dataframe(df=X_train, freq=\"H\", fill_missing_dates=True, fillna_value=None).pd_dataframe()\n",
    "    X_test = TimeSeries.from_dataframe(df=X_test, freq=\"H\", fill_missing_dates=True, fillna_value=None).pd_dataframe()\n",
    "    print(\"X_train gaps in dates after filling missing dates: \", X_train.index.to_series().diff().dt.total_seconds().gt(3600).sum())\n",
    "    print(\"X_test gaps in dates after filling missing dates: \", X_test.index.to_series().diff().dt.total_seconds().gt(3600).sum())\n",
    "\n",
    "    \n",
    "\n",
    "    # clip all y values to 0 if negative\n",
    "    y_train[\"y\"] = y_train[\"y\"].clip(lower=0)\n",
    "    \n",
    "    # print Number of missing values in X train\n",
    "    print(\"Number of missing values in X_train: \", X_train.isnull().sum().sum())\n",
    "    print(\"Number of missing values in X_test: \", X_test.isnull().sum().sum())\n",
    "    # y_train missing values\n",
    "    print(\"Number of missing values in y_train: \", y_train.isnull().sum().sum())\n",
    "    X_train = pd.merge(X_train, y_train, how=\"outer\", left_index=True, right_index=True)\n",
    "    print(\"Number of missing values in X_train after merging with y_train: \", X_train.drop(columns=['y']).isnull().sum().sum())\n",
    "\n",
    "\n",
    "\n",
    "    X_train[\"location\"] = location\n",
    "    X_test[\"location\"] = location\n",
    "    \n",
    "    return X_train, X_test\n",
    "    \n",
    "\n",
    "\n",
    "# Define locations\n",
    "locations = ['A', 'B', 'C']\n",
    "\n",
    "X_trains = []\n",
    "X_tests = []\n",
    "y_trains = []\n",
    "# Loop through locations\n",
    "for loc in locations:\n",
    "    print(\"\\n\\n\")\n",
    "    print(f\"Processing location {loc}...\")\n",
    "    # Read target training data\n",
    "    y_train = pd.read_parquet(f'{loc}/train_targets.parquet')\n",
    "    \n",
    "    # Read estimated training data and add location feature\n",
    "    X_train_estimated = pd.read_parquet(f'{loc}/X_train_estimated.parquet')\n",
    "    \n",
    "    # Read observed training data and add location feature\n",
    "    X_train_observed= pd.read_parquet(f'{loc}/X_train_observed.parquet')\n",
    "\n",
    "    # Read estimated test data and add location feature\n",
    "    X_test_estimated = pd.read_parquet(f'{loc}/X_test_estimated.parquet')\n",
    "    \n",
    "    # Concatenate observed and estimated datasets for each location\n",
    "    #X_train = pd.concat([X_train_estimated, X_train_observed])\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # Preprocess data\n",
    "    X_train, X_test = preprocess_data(X_train_observed, X_train_estimated, X_test_estimated, y_train, loc)\n",
    "\n",
    "    print(f\"Final shape of X_train for location {loc}: \", X_train.shape)\n",
    "    print(f\"Final shape of X_test for location {loc}: \", X_test.shape)\n",
    "\n",
    "    # print(y_train.head(), y_train.shape)\n",
    "    # print(X_train.head(), X_train.shape)\n",
    "    # print(X_train.head(), X_train.shape)\n",
    "    # print(type(X_train['y']))\n",
    "\n",
    "    # Save data to csv\n",
    "    X_train.to_csv(f'{loc}/X_train.csv', index=True)\n",
    "    X_test.to_csv(f'{loc}/X_test.csv', index=True)\n",
    "\n",
    "\n",
    "    X_trains.append(X_train)\n",
    "    X_tests.append(X_test)\n",
    "\n",
    "# Concatenate all data and save to csv\n",
    "X_train = pd.concat(X_trains)\n",
    "X_test = pd.concat(X_tests)\n",
    "\n",
    "print(f\"Final shape of X_train: \", X_train.shape)\n",
    "print(f\"Final shape of X_test: \", X_test.shape)\n",
    "\n",
    "X_train.to_csv('X_train_raw.csv', index=True)\n",
    "X_test.to_csv('X_test_raw.csv', index=True)\n",
    "\n",
    "\n",
    "# save where nan y values are dropped\n",
    "X_train_non_nan = X_train.dropna(subset=['y'])\n",
    "X_train_non_nan.to_csv('X_train_non_nan.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = X_train_non_nan.copy()\n",
    "test_df = X_test.copy()\n",
    "\n",
    "\n",
    "# add sin and cos of sun_elevation:d and sun_azimuth:d\n",
    "df['sin_sun_elevation'] = np.sin(np.deg2rad(df['sun_elevation:d']))\n",
    "\n",
    "test_df['sin_sun_elevation'] = np.sin(np.deg2rad(test_df['sun_elevation:d']))\n",
    "\n",
    "# add global_rad_1h:J = diffuse_rad_1h:J + direct_rad_1h:J\n",
    "df['global_rad_1h:J'] = df['diffuse_rad_1h:J'] + df['direct_rad_1h:J']\n",
    "test_df['global_rad_1h:J'] = test_df['diffuse_rad_1h:J'] + test_df['direct_rad_1h:J']\n",
    "\n",
    "# dew_or_rime:idx, Change this to one variable for is_dew and one variable for is_rime (dew:1, rime:-1)\n",
    "df['is_dew'] = df['dew_or_rime:idx'].apply(lambda x: 1 if x == 1 else 0)\n",
    "df['is_rime'] = df['dew_or_rime:idx'].apply(lambda x: 1 if x == -1 else 0)\n",
    "\n",
    "test_df['is_dew'] = test_df['dew_or_rime:idx'].apply(lambda x: 1 if x == 1 else 0)\n",
    "test_df['is_rime'] = test_df['dew_or_rime:idx'].apply(lambda x: 1 if x == -1 else 0)\n",
    "\n",
    "\n",
    "EXOGENOUS = [\n",
    "    'estimated_diff_hours',\n",
    "    \"absolute_humidity_2m:gm3\",\n",
    "    \"air_density_2m:kgm3\",\n",
    "    \"dew_point_2m:K\",\n",
    "    \"diffuse_rad_1h:J\",\n",
    "    \"direct_rad_1h:J\",\n",
    "    \"effective_cloud_cover:p\",\n",
    "    \"fresh_snow_1h:cm\",\n",
    "    \"snow_depth:cm\",\n",
    "    \"sun_elevation:d\",\n",
    "    \"sun_azimuth:d\",\n",
    "    \"t_1000hPa:K\",\n",
    "    \"visibility:m\",\n",
    "    \"wind_speed_10m:ms\",\n",
    "    \"is_dew\",\n",
    "    \"is_rime\",\n",
    "    \"sin_sun_elevation\",\n",
    "    \"global_rad_1h:J\",\n",
    "    ]\n",
    "#additional_features_for_testing = \n",
    "\n",
    "df = df[EXOGENOUS + [\"y\", \"location\"]]\n",
    "test_df = test_df[EXOGENOUS+ [\"location\"]]\n",
    "\n",
    "# save to X_train_feature_engineered.csv\n",
    "df.to_csv('X_train_feature_engineered.csv', index=True)\n",
    "test_df.to_csv('X_test_feature_engineered.csv', index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the last submission number\n",
    "last_submission_number = int(max([int(filename.split('_')[1].split('.')[0]) for filename in os.listdir('submissions') if \"submission\" in filename]))\n",
    "print(\"Last submission number:\", last_submission_number)\n",
    "\n",
    "# Create the new filename\n",
    "new_filename = f'submission_{last_submission_number + 1}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded data from: X_train_feature_engineered.csv | Columns = 21 / 21 | Rows = 93024 -> 93024\n",
      "Loaded data from: X_test_feature_engineered.csv | Columns = 20 / 20 | Rows = 4608 -> 4608\n",
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20231005_111152\"\n",
      "Beginning AutoGluon training ... Time limit = 3600s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20231005_111152\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.10.12\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 22.1.0: Sun Oct  9 20:15:09 PDT 2022; root:xnu-8792.41.9~2/RELEASE_ARM64_T6000\n",
      "Disk Space Avail:   24.20 GB / 494.38 GB (4.9%)\n",
      "Train Data Rows:    34085\n",
      "Train Data Columns: 20\n",
      "Label Column: y\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (5733.42, 0.0, 630.59471, 1165.90242)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    6240.11 MB\n",
      "\tTrain Data (Original)  Memory Usage: 9.48 MB (0.2% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 1): ['location']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                      : 16 | ['estimated_diff_hours', 'absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'dew_point_2m:K', 'diffuse_rad_1h:J', ...]\n",
      "\t\t('int', [])                        :  2 | ['is_dew', 'is_rime']\n",
      "\t\t('object', ['datetime_as_object']) :  1 | ['ds']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                : 16 | ['estimated_diff_hours', 'absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'dew_point_2m:K', 'diffuse_rad_1h:J', ...]\n",
      "\t\t('int', ['bool'])            :  2 | ['is_dew', 'is_rime']\n",
      "\t\t('int', ['datetime_as_int']) :  5 | ['ds', 'ds.year', 'ds.month', 'ds.day', 'ds.dayofweek']\n",
      "\t0.1s = Fit runtime\n",
      "\t19 features in original data used to generate 23 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 5.79 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.15s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.07334604664808567, Train Rows: 31585, Val Rows: 2500\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 3599.85s of the 3599.85s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for location A...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-218.6492\t = Validation score   (-mean_absolute_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 3599.76s of the 3599.76s of remaining time.\n",
      "\t-176.0068\t = Validation score   (-mean_absolute_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3599.68s of the 3599.68s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 179.324\n",
      "[2000]\tvalid_set's l1: 171.801\n",
      "[3000]\tvalid_set's l1: 168.546\n",
      "[4000]\tvalid_set's l1: 166.446\n",
      "[5000]\tvalid_set's l1: 164.636\n",
      "[6000]\tvalid_set's l1: 163.417\n",
      "[7000]\tvalid_set's l1: 162.131\n",
      "[8000]\tvalid_set's l1: 161.328\n",
      "[9000]\tvalid_set's l1: 160.881\n",
      "[10000]\tvalid_set's l1: 160.377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-160.3755\t = Validation score   (-mean_absolute_error)\n",
      "\t128.34s\t = Training   runtime\n",
      "\t0.37s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 3470.62s of the 3470.62s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 174.814\n",
      "[2000]\tvalid_set's l1: 170.872\n",
      "[3000]\tvalid_set's l1: 169.441\n",
      "[4000]\tvalid_set's l1: 168.934\n",
      "[5000]\tvalid_set's l1: 168.648\n",
      "[6000]\tvalid_set's l1: 168.338\n",
      "[7000]\tvalid_set's l1: 168.111\n",
      "[8000]\tvalid_set's l1: 168.002\n",
      "[9000]\tvalid_set's l1: 167.867\n",
      "[10000]\tvalid_set's l1: 167.753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-167.7487\t = Validation score   (-mean_absolute_error)\n",
      "\t43.28s\t = Training   runtime\n",
      "\t0.41s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 3426.59s of the 3426.58s of remaining time.\n",
      "\t-189.013\t = Validation score   (-mean_absolute_error)\n",
      "\t13.96s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 3411.83s of the 3411.83s of remaining time.\n",
      "\t-178.1038\t = Validation score   (-mean_absolute_error)\n",
      "\t106.9s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 3304.92s of the 3304.92s of remaining time.\n",
      "\t-191.0823\t = Validation score   (-mean_absolute_error)\n",
      "\t2.73s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 3301.89s of the 3301.89s of remaining time.\n",
      "\t-199.1758\t = Validation score   (-mean_absolute_error)\n",
      "\t43.22s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 3258.62s of the 3258.62s of remaining time.\n",
      "\t-179.2972\t = Validation score   (-mean_absolute_error)\n",
      "\t32.1s\t = Training   runtime\n",
      "\t0.37s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 3226.01s of the 3226.01s of remaining time.\n"
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "train_data = TabularDataset('X_train_feature_engineered.csv')\n",
    "test_data = TabularDataset('X_test_feature_engineered.csv')\n",
    "label = 'y'\n",
    "metric = 'mean_absolute_error'\n",
    "time_limit = 60*60\n",
    "\n",
    "predictors = [None, None, None]\n",
    "\n",
    "loc = \"A\"\n",
    "print(f\"Training model for location {loc}...\")\n",
    "predictor = TabularPredictor(label=label, eval_metric=metric, path=f\"AutogluonModels/{new_filename}_{loc}\").fit(train_data[train_data[\"location\"] == loc], time_limit=time_limit)\n",
    "predictors[0] = predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20231005_103118\"\n",
      "Beginning AutoGluon training ... Time limit = 3600s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20231005_103118\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.10.12\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 22.1.0: Sun Oct  9 20:15:09 PDT 2022; root:xnu-8792.41.9~2/RELEASE_ARM64_T6000\n",
      "Disk Space Avail:   25.99 GB / 494.38 GB (5.3%)\n",
      "Train Data Rows:    32844\n",
      "Train Data Columns: 48\n",
      "Label Column: y\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (1152.3, -0.0, 96.82478, 193.94649)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    5954.33 MB\n",
      "\tTrain Data (Original)  Memory Usage: 16.49 MB (0.3% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 1): ['location']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                      : 46 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', ...]\n",
      "\t\t('object', ['datetime_as_object']) :  1 | ['ds']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                : 44 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', ...]\n",
      "\t\t('int', ['bool'])            :  2 | ['elevation:m', 'snow_density:kgm3']\n",
      "\t\t('int', ['datetime_as_int']) :  5 | ['ds', 'ds.year', 'ds.month', 'ds.day', 'ds.dayofweek']\n",
      "\t0.2s = Fit runtime\n",
      "\t47 features in original data used to generate 51 features in processed data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for location B...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTrain Data (Processed) Memory Usage: 12.94 MB (0.2% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.19s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.07611740348313238, Train Rows: 30344, Val Rows: 2500\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 3599.81s of the 3599.81s of remaining time.\n",
      "\t-27.5392\t = Validation score   (-mean_absolute_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 3599.69s of the 3599.69s of remaining time.\n",
      "\t-22.1104\t = Validation score   (-mean_absolute_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3599.57s of the 3599.57s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 28.2905\n",
      "[2000]\tvalid_set's l1: 26.315\n",
      "[3000]\tvalid_set's l1: 25.2222\n",
      "[4000]\tvalid_set's l1: 24.5407\n",
      "[5000]\tvalid_set's l1: 24.0041\n",
      "[6000]\tvalid_set's l1: 23.601\n",
      "[7000]\tvalid_set's l1: 23.3111\n",
      "[8000]\tvalid_set's l1: 23.0534\n",
      "[9000]\tvalid_set's l1: 22.8287\n",
      "[10000]\tvalid_set's l1: 22.6604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-22.6588\t = Validation score   (-mean_absolute_error)\n",
      "\t43.16s\t = Training   runtime\n",
      "\t0.34s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 3555.69s of the 3555.69s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 25.4515\n",
      "[2000]\tvalid_set's l1: 24.2397\n",
      "[3000]\tvalid_set's l1: 23.5963\n",
      "[4000]\tvalid_set's l1: 23.2499\n",
      "[5000]\tvalid_set's l1: 23.1186\n",
      "[6000]\tvalid_set's l1: 23.0154\n",
      "[7000]\tvalid_set's l1: 22.9528\n",
      "[8000]\tvalid_set's l1: 22.9163\n",
      "[9000]\tvalid_set's l1: 22.8923\n",
      "[10000]\tvalid_set's l1: 22.8727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-22.8726\t = Validation score   (-mean_absolute_error)\n",
      "\t66.22s\t = Training   runtime\n",
      "\t0.38s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 3488.74s of the 3488.74s of remaining time.\n",
      "\t-26.9273\t = Validation score   (-mean_absolute_error)\n",
      "\t24.64s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 3463.61s of the 3463.61s of remaining time.\n",
      "\t-24.5292\t = Validation score   (-mean_absolute_error)\n",
      "\t113.13s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 3350.46s of the 3350.46s of remaining time.\n",
      "\t-28.4244\t = Validation score   (-mean_absolute_error)\n",
      "\t4.37s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 3345.5s of the 3345.5s of remaining time.\n",
      "\t-29.732\t = Validation score   (-mean_absolute_error)\n",
      "\t42.23s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 3303.23s of the 3303.22s of remaining time.\n",
      "\t-25.022\t = Validation score   (-mean_absolute_error)\n",
      "\t48.83s\t = Training   runtime\n",
      "\t0.39s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 3253.84s of the 3253.83s of remaining time.\n",
      "\t-26.6617\t = Validation score   (-mean_absolute_error)\n",
      "\t379.85s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 2873.95s of the 2873.95s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 22.5161\n",
      "[2000]\tvalid_set's l1: 21.8756\n",
      "[3000]\tvalid_set's l1: 21.7026\n",
      "[4000]\tvalid_set's l1: 21.6445\n",
      "[5000]\tvalid_set's l1: 21.6268\n",
      "[6000]\tvalid_set's l1: 21.617\n",
      "[7000]\tvalid_set's l1: 21.6136\n",
      "[8000]\tvalid_set's l1: 21.6118\n",
      "[9000]\tvalid_set's l1: 21.6108\n",
      "[10000]\tvalid_set's l1: 21.6102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-21.6102\t = Validation score   (-mean_absolute_error)\n",
      "\t161.72s\t = Training   runtime\n",
      "\t1.17s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 2709.76s of remaining time.\n",
      "\t-20.1984\t = Validation score   (-mean_absolute_error)\n",
      "\t0.16s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 890.44s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20231005_103118\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loc = \"B\"\n",
    "print(f\"Training model for location {loc}...\")\n",
    "predictor = TabularPredictor(label=label, eval_metric=metric, path=f\"AutogluonModels/{new_filename}_{loc}\").fit(train_data[train_data[\"location\"] == loc], time_limit=time_limit)\n",
    "predictors[1] = predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20231005_104608\"\n",
      "Beginning AutoGluon training ... Time limit = 3600s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20231005_104608\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.10.12\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 22.1.0: Sun Oct  9 20:15:09 PDT 2022; root:xnu-8792.41.9~2/RELEASE_ARM64_T6000\n",
      "Disk Space Avail:   24.99 GB / 494.38 GB (5.1%)\n",
      "Train Data Rows:    26095\n",
      "Train Data Columns: 48\n",
      "Label Column: y\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and label-values can't be converted to int).\n",
      "\tLabel info (max, min, mean, stddev): (999.6, -0.0, 77.63106, 165.81688)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    6020.95 MB\n",
      "\tTrain Data (Original)  Memory Usage: 13.1 MB (0.2% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for location C...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tUseless Original Features (Count: 1): ['location']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['snow_drift:idx']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 1 | ['snow_drift:idx']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                      : 45 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', ...]\n",
      "\t\t('object', ['datetime_as_object']) :  1 | ['ds']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                : 43 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', ...]\n",
      "\t\t('int', ['bool'])            :  2 | ['elevation:m', 'snow_density:kgm3']\n",
      "\t\t('int', ['datetime_as_int']) :  5 | ['ds', 'ds.year', 'ds.month', 'ds.day', 'ds.dayofweek']\n",
      "\t0.3s = Fit runtime\n",
      "\t46 features in original data used to generate 50 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 10.07 MB (0.2% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.35s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.09580379383023568, Train Rows: 23595, Val Rows: 2500\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 3599.65s of the 3599.65s of remaining time.\n",
      "\t-25.4907\t = Validation score   (-mean_absolute_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 3599.55s of the 3599.55s of remaining time.\n",
      "\t-20.1032\t = Validation score   (-mean_absolute_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3599.45s of the 3599.45s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 16.5623\n",
      "[2000]\tvalid_set's l1: 15.9561\n",
      "[3000]\tvalid_set's l1: 15.6743\n",
      "[4000]\tvalid_set's l1: 15.5235\n",
      "[5000]\tvalid_set's l1: 15.4624\n",
      "[6000]\tvalid_set's l1: 15.4409\n",
      "[7000]\tvalid_set's l1: 15.4113\n",
      "[8000]\tvalid_set's l1: 15.3992\n",
      "[9000]\tvalid_set's l1: 15.3877\n",
      "[10000]\tvalid_set's l1: 15.3767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-15.375\t = Validation score   (-mean_absolute_error)\n",
      "\t38.96s\t = Training   runtime\n",
      "\t0.37s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 3559.75s of the 3559.75s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 16.8096\n",
      "[2000]\tvalid_set's l1: 16.3727\n",
      "[3000]\tvalid_set's l1: 16.2273\n",
      "[4000]\tvalid_set's l1: 16.1867\n",
      "[5000]\tvalid_set's l1: 16.1669\n",
      "[6000]\tvalid_set's l1: 16.1593\n",
      "[7000]\tvalid_set's l1: 16.1505\n",
      "[8000]\tvalid_set's l1: 16.1433\n",
      "[9000]\tvalid_set's l1: 16.1437\n",
      "[10000]\tvalid_set's l1: 16.1435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-16.1424\t = Validation score   (-mean_absolute_error)\n",
      "\t64.69s\t = Training   runtime\n",
      "\t0.35s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 3494.41s of the 3494.41s of remaining time.\n",
      "\t-18.8102\t = Validation score   (-mean_absolute_error)\n",
      "\t15.91s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 3478.13s of the 3478.13s of remaining time.\n",
      "\t-16.9078\t = Validation score   (-mean_absolute_error)\n",
      "\t107.43s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 3370.67s of the 3370.67s of remaining time.\n",
      "\t-19.0049\t = Validation score   (-mean_absolute_error)\n",
      "\t2.37s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 3367.92s of the 3367.92s of remaining time.\n",
      "\t-18.5567\t = Validation score   (-mean_absolute_error)\n",
      "\t28.28s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 3339.58s of the 3339.57s of remaining time.\n",
      "\t-16.9114\t = Validation score   (-mean_absolute_error)\n",
      "\t51.54s\t = Training   runtime\n",
      "\t0.38s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 3287.5s of the 3287.5s of remaining time.\n",
      "\t-17.2408\t = Validation score   (-mean_absolute_error)\n",
      "\t171.0s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 3116.47s of the 3116.46s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 16.1692\n",
      "[2000]\tvalid_set's l1: 16.0585\n",
      "[3000]\tvalid_set's l1: 16.0355\n",
      "[4000]\tvalid_set's l1: 16.0285\n",
      "[5000]\tvalid_set's l1: 16.0261\n",
      "[6000]\tvalid_set's l1: 16.0251\n",
      "[7000]\tvalid_set's l1: 16.0249\n",
      "[8000]\tvalid_set's l1: 16.0248\n",
      "[9000]\tvalid_set's l1: 16.0247\n",
      "[10000]\tvalid_set's l1: 16.0247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-16.0247\t = Validation score   (-mean_absolute_error)\n",
      "\t154.53s\t = Training   runtime\n",
      "\t1.05s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 2959.71s of remaining time.\n",
      "\t-14.7569\t = Validation score   (-mean_absolute_error)\n",
      "\t0.15s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 640.47s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20231005_104608\")\n"
     ]
    }
   ],
   "source": [
    "loc = \"C\"\n",
    "print(f\"Training model for location {loc}...\")\n",
    "predictor = TabularPredictor(label=label, eval_metric=metric, path=f\"AutogluonModels/{new_filename}_{loc}\").fit(train_data[train_data[\"location\"] == loc], time_limit=time_limit)\n",
    "predictors[2] = predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          prediction   id\n",
      "location                 \n",
      "A                720  720\n",
      "B                720  720\n",
      "C                720  720\n"
     ]
    }
   ],
   "source": [
    "# pull in test.csv and create submission.csv\n",
    "submission_ids_df = pd.read_csv('test.csv', index_col=0)\n",
    "# submission_ids_df has Id, Time, Location\n",
    "# X_test has location_A, location_B, location_C\n",
    "# we have to make sure that submission_ids_df has same dates as X_test, then predict y for each row in submission_ids_df, then save as submission.csv (using correct Id from submission_ids_df)\n",
    "\n",
    "# convert index to datetime\n",
    "submission_ids_df[\"id\"] = submission_ids_df.index\n",
    "submission_ids_df.set_index('time', inplace=True)\n",
    "submission_ids_df.index = pd.to_datetime(submission_ids_df.index).astype('datetime64[ns]')\n",
    "submission_ids_df\n",
    "\n",
    "# split submission_ids_df into each location\n",
    "print(submission_ids_df.groupby('location').count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded data from: X_test_raw.csv | Columns = 48 / 48 | Rows = 4608 -> 4608\n",
      "Loaded data from: test.csv | Columns = 4 / 4 | Rows = 2160 -> 2160\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>absolute_humidity_2m:gm3</th>\n",
       "      <th>air_density_2m:kgm3</th>\n",
       "      <th>ceiling_height_agl:m</th>\n",
       "      <th>clear_sky_energy_1h:J</th>\n",
       "      <th>clear_sky_rad:W</th>\n",
       "      <th>cloud_base_agl:m</th>\n",
       "      <th>dew_or_rime:idx</th>\n",
       "      <th>dew_point_2m:K</th>\n",
       "      <th>diffuse_rad:W</th>\n",
       "      <th>...</th>\n",
       "      <th>visibility:m</th>\n",
       "      <th>wind_speed_10m:ms</th>\n",
       "      <th>wind_speed_u_10m:ms</th>\n",
       "      <th>wind_speed_v_10m:ms</th>\n",
       "      <th>wind_speed_w_1000hPa:ms</th>\n",
       "      <th>estimated_diff_hours</th>\n",
       "      <th>location</th>\n",
       "      <th>id</th>\n",
       "      <th>time</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-05-01 00:00:00</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.286</td>\n",
       "      <td>912.700012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1041.199951</td>\n",
       "      <td>0.0</td>\n",
       "      <td>271.700012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>30210.699219</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>16.998889</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-05-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-05-01 01:00:00</td>\n",
       "      <td>4.3</td>\n",
       "      <td>1.287</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1095.400024</td>\n",
       "      <td>0.0</td>\n",
       "      <td>271.600006</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>29507.500000</td>\n",
       "      <td>3.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.3</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>17.998889</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-05-01 01:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-05-01 02:00:00</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.284</td>\n",
       "      <td>1482.099976</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1041.300049</td>\n",
       "      <td>0.0</td>\n",
       "      <td>271.200012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>29463.099609</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.8</td>\n",
       "      <td>3.2</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>18.998889</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-05-01 02:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-05-01 03:00:00</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.282</td>\n",
       "      <td>2306.699951</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1465.599976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>270.799988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>33727.101562</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.6</td>\n",
       "      <td>3.2</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>19.998889</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-05-01 03:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-05-01 04:00:00</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.282</td>\n",
       "      <td>2323.199951</td>\n",
       "      <td>59774.500000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>703.599976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>270.299988</td>\n",
       "      <td>31.400000</td>\n",
       "      <td>...</td>\n",
       "      <td>35927.601562</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.3</td>\n",
       "      <td>3.1</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>20.998889</td>\n",
       "      <td>A</td>\n",
       "      <td>4</td>\n",
       "      <td>2023-05-01 04:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2155</th>\n",
       "      <td>2023-07-03 19:00:00</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1.196</td>\n",
       "      <td>3532.399902</td>\n",
       "      <td>615338.812500</td>\n",
       "      <td>117.199997</td>\n",
       "      <td>2239.600098</td>\n",
       "      <td>0.0</td>\n",
       "      <td>281.600006</td>\n",
       "      <td>41.299999</td>\n",
       "      <td>...</td>\n",
       "      <td>41536.398438</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1.9</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.991389</td>\n",
       "      <td>C</td>\n",
       "      <td>2155</td>\n",
       "      <td>2023-07-03 19:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2156</th>\n",
       "      <td>2023-07-03 20:00:00</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1.199</td>\n",
       "      <td>3429.000000</td>\n",
       "      <td>269582.406250</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>1513.699951</td>\n",
       "      <td>0.0</td>\n",
       "      <td>281.899994</td>\n",
       "      <td>19.700001</td>\n",
       "      <td>...</td>\n",
       "      <td>40136.500000</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.991389</td>\n",
       "      <td>C</td>\n",
       "      <td>2156</td>\n",
       "      <td>2023-07-03 20:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2157</th>\n",
       "      <td>2023-07-03 21:00:00</td>\n",
       "      <td>8.8</td>\n",
       "      <td>1.202</td>\n",
       "      <td>2495.000000</td>\n",
       "      <td>71999.601562</td>\n",
       "      <td>4.900000</td>\n",
       "      <td>1342.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>282.200012</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>43266.101562</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.2</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.991389</td>\n",
       "      <td>C</td>\n",
       "      <td>2157</td>\n",
       "      <td>2023-07-03 21:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2158</th>\n",
       "      <td>2023-07-03 22:00:00</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.206</td>\n",
       "      <td>1997.400024</td>\n",
       "      <td>1378.300049</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1878.900024</td>\n",
       "      <td>0.0</td>\n",
       "      <td>282.600006</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>39017.898438</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.991389</td>\n",
       "      <td>C</td>\n",
       "      <td>2158</td>\n",
       "      <td>2023-07-03 22:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2159</th>\n",
       "      <td>2023-07-03 23:00:00</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.207</td>\n",
       "      <td>2005.599976</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1471.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>282.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>39026.000000</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.6</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.991389</td>\n",
       "      <td>C</td>\n",
       "      <td>2159</td>\n",
       "      <td>2023-07-03 23:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2160 rows  51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       ds  absolute_humidity_2m:gm3  air_density_2m:kgm3  \\\n",
       "0     2023-05-01 00:00:00                       4.4                1.286   \n",
       "1     2023-05-01 01:00:00                       4.3                1.287   \n",
       "2     2023-05-01 02:00:00                       4.2                1.284   \n",
       "3     2023-05-01 03:00:00                       4.1                1.282   \n",
       "4     2023-05-01 04:00:00                       3.9                1.282   \n",
       "...                   ...                       ...                  ...   \n",
       "2155  2023-07-03 19:00:00                       8.3                1.196   \n",
       "2156  2023-07-03 20:00:00                       8.5                1.199   \n",
       "2157  2023-07-03 21:00:00                       8.8                1.202   \n",
       "2158  2023-07-03 22:00:00                       9.0                1.206   \n",
       "2159  2023-07-03 23:00:00                       9.0                1.207   \n",
       "\n",
       "      ceiling_height_agl:m  clear_sky_energy_1h:J  clear_sky_rad:W  \\\n",
       "0               912.700012               0.000000         0.000000   \n",
       "1                      NaN               0.000000         0.000000   \n",
       "2              1482.099976               0.000000         0.000000   \n",
       "3              2306.699951               0.000000         0.000000   \n",
       "4              2323.199951           59774.500000        43.000000   \n",
       "...                    ...                    ...              ...   \n",
       "2155           3532.399902          615338.812500       117.199997   \n",
       "2156           3429.000000          269582.406250        40.000000   \n",
       "2157           2495.000000           71999.601562         4.900000   \n",
       "2158           1997.400024            1378.300049         0.000000   \n",
       "2159           2005.599976               0.000000         0.000000   \n",
       "\n",
       "      cloud_base_agl:m  dew_or_rime:idx  dew_point_2m:K  diffuse_rad:W  ...  \\\n",
       "0          1041.199951              0.0      271.700012       0.000000  ...   \n",
       "1          1095.400024              0.0      271.600006       0.000000  ...   \n",
       "2          1041.300049              0.0      271.200012       0.000000  ...   \n",
       "3          1465.599976              0.0      270.799988       0.000000  ...   \n",
       "4           703.599976              0.0      270.299988      31.400000  ...   \n",
       "...                ...              ...             ...            ...  ...   \n",
       "2155       2239.600098              0.0      281.600006      41.299999  ...   \n",
       "2156       1513.699951              0.0      281.899994      19.700001  ...   \n",
       "2157       1342.500000              0.0      282.200012       5.000000  ...   \n",
       "2158       1878.900024              0.0      282.600006       0.000000  ...   \n",
       "2159       1471.000000              0.0      282.500000       0.000000  ...   \n",
       "\n",
       "      visibility:m  wind_speed_10m:ms  wind_speed_u_10m:ms  \\\n",
       "0     30210.699219                4.0                  2.2   \n",
       "1     29507.500000                3.9                  2.0   \n",
       "2     29463.099609                3.7                  1.8   \n",
       "3     33727.101562                3.6                  1.6   \n",
       "4     35927.601562                3.4                  1.3   \n",
       "...            ...                ...                  ...   \n",
       "2155  41536.398438                2.2                  1.9   \n",
       "2156  40136.500000                2.1                  1.9   \n",
       "2157  43266.101562                2.4                  2.2   \n",
       "2158  39017.898438                2.0                  1.8   \n",
       "2159  39026.000000                1.7                  1.6   \n",
       "\n",
       "      wind_speed_v_10m:ms  wind_speed_w_1000hPa:ms  estimated_diff_hours  \\\n",
       "0                     3.4                     -0.0             16.998889   \n",
       "1                     3.3                     -0.0             17.998889   \n",
       "2                     3.2                     -0.0             18.998889   \n",
       "3                     3.2                     -0.0             19.998889   \n",
       "4                     3.1                     -0.0             20.998889   \n",
       "...                   ...                      ...                   ...   \n",
       "2155                 -1.2                      0.0             35.991389   \n",
       "2156                 -0.9                      0.0             36.991389   \n",
       "2157                 -1.0                      0.0             37.991389   \n",
       "2158                 -0.8                      0.0             38.991389   \n",
       "2159                 -0.7                      0.0             39.991389   \n",
       "\n",
       "      location    id                 time  prediction  \n",
       "0            A     0  2023-05-01 00:00:00           0  \n",
       "1            A     1  2023-05-01 01:00:00           0  \n",
       "2            A     2  2023-05-01 02:00:00           0  \n",
       "3            A     3  2023-05-01 03:00:00           0  \n",
       "4            A     4  2023-05-01 04:00:00           0  \n",
       "...        ...   ...                  ...         ...  \n",
       "2155         C  2155  2023-07-03 19:00:00           0  \n",
       "2156         C  2156  2023-07-03 20:00:00           0  \n",
       "2157         C  2157  2023-07-03 21:00:00           0  \n",
       "2158         C  2158  2023-07-03 22:00:00           0  \n",
       "2159         C  2159  2023-07-03 23:00:00           0  \n",
       "\n",
       "[2160 rows x 51 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming X_test and model are already defined\n",
    "# Assuming 'time' is also a datetime64[ns] column in X_test\n",
    "\n",
    "\n",
    "test_data = TabularDataset('X_test_raw.csv')\n",
    "test_ids = TabularDataset('test.csv')\n",
    "# merge test_data with test_ids\n",
    "test_data = pd.merge(test_data, test_ids, how=\"inner\", right_on=[\"time\", \"location\"], left_on=[\"ds\", \"location\"])\n",
    "\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict, grouped by location\n",
    "predictions = []\n",
    "location_map = {\n",
    "    \"A\": 0,\n",
    "    \"B\": 1,\n",
    "    \"C\": 2\n",
    "}\n",
    "for loc, group in test_data.groupby('location'):\n",
    "    predictions.append(predictors[location_map[loc]].predict(test_data[test_data[\"location\"] == loc]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         2.006019\n",
       "1         5.289860\n",
       "2         7.965479\n",
       "3        43.964508\n",
       "4       372.254974\n",
       "           ...    \n",
       "2155     77.931915\n",
       "2156     51.792599\n",
       "2157     31.644035\n",
       "2158     12.108985\n",
       "2159     11.182805\n",
       "Name: y, Length: 2160, dtype: float32"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenate predictions\n",
    "predictions = pd.concat(predictions)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.006019</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.289860</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.965479</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43.964508</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>372.254974</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2155</th>\n",
       "      <td>77.931915</td>\n",
       "      <td>2155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2156</th>\n",
       "      <td>51.792599</td>\n",
       "      <td>2156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2157</th>\n",
       "      <td>31.644035</td>\n",
       "      <td>2157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2158</th>\n",
       "      <td>12.108985</td>\n",
       "      <td>2158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2159</th>\n",
       "      <td>11.182805</td>\n",
       "      <td>2159</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2160 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      prediction    id\n",
       "0       2.006019     0\n",
       "1       5.289860     1\n",
       "2       7.965479     2\n",
       "3      43.964508     3\n",
       "4     372.254974     4\n",
       "...          ...   ...\n",
       "2155   77.931915  2155\n",
       "2156   51.792599  2156\n",
       "2157   31.644035  2157\n",
       "2158   12.108985  2158\n",
       "2159   11.182805  2159\n",
       "\n",
       "[2160 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save predictions series to csv as \"id\", \"prediction\"\n",
    "predictions_df = pd.DataFrame(predictions)\n",
    "predictions_df.rename(columns={\"y\": \"prediction\"}, inplace=True)\n",
    "predictions_df[\"id\"] = predictions_df.index\n",
    "predictions_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last submission number: 62\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Save the submission DataFrame to submissions folder, create new name based on last submission, format is submission_<last_submission_number + 1>.csv\n",
    "\n",
    "# Save the submission\n",
    "predictions_df.to_csv(os.path.join('submissions', f\"{new_filename}.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
