{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing location A...\n",
      "Processing location B...\n",
      "Processing location C...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def fix_datetime(X, name):\n",
    "    # Convert 'date_forecast' to datetime format and replace original column with 'ds'\n",
    "    X['ds'] = pd.to_datetime(X['date_forecast'])\n",
    "    X.drop(columns=['date_forecast'], inplace=True, errors='ignore')\n",
    "    X.sort_values(by='ds', inplace=True)\n",
    "    X.set_index('ds', inplace=True)\n",
    "\n",
    "    # Drop rows where the minute part of the time is not 0\n",
    "    X = X[X.index.minute == 0]\n",
    "    return X\n",
    "\n",
    "\n",
    "\n",
    "def convert_to_datetime(X_train_observed, X_train_estimated, X_test, y_train):\n",
    "    X_train_observed = fix_datetime(X_train_observed, \"X_train_observed\")\n",
    "    X_train_estimated = fix_datetime(X_train_estimated, \"X_train_estimated\")\n",
    "    X_test = fix_datetime(X_test, \"X_test\")\n",
    "\n",
    "\n",
    "    X_train_observed[\"estimated_diff_hours\"] = 0\n",
    "    X_train_estimated[\"estimated_diff_hours\"] = (X_train_estimated.index - pd.to_datetime(X_train_estimated[\"date_calc\"])).dt.total_seconds() / 3600\n",
    "    X_test[\"estimated_diff_hours\"] = (X_test.index - pd.to_datetime(X_test[\"date_calc\"])).dt.total_seconds() / 3600\n",
    "\n",
    "    X_train_estimated[\"estimated_diff_hours\"] = X_train_estimated[\"estimated_diff_hours\"].astype('int64')\n",
    "    # the filled once will get dropped later anyways, when we drop y nans\n",
    "    X_test[\"estimated_diff_hours\"] = X_test[\"estimated_diff_hours\"].fillna(-50).astype('int64')\n",
    "\n",
    "\n",
    "    X_train_estimated.drop(columns=['date_calc'], inplace=True)\n",
    "    X_test.drop(columns=['date_calc'], inplace=True)\n",
    "\n",
    "    y_train['ds'] = pd.to_datetime(y_train['time'])\n",
    "    y_train.drop(columns=['time'], inplace=True)\n",
    "    y_train.sort_values(by='ds', inplace=True)\n",
    "    y_train.set_index('ds', inplace=True)\n",
    "\n",
    "    return X_train_observed, X_train_estimated, X_test, y_train\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_data(X_train_observed, X_train_estimated, X_test, y_train, location):\n",
    "    # convert to datetime\n",
    "    X_train_observed, X_train_estimated, X_test, y_train = convert_to_datetime(X_train_observed, X_train_estimated, X_test, y_train)\n",
    "\n",
    "    y_train[\"y\"] = y_train[\"pv_measurement\"].astype('float64')\n",
    "    y_train.drop(columns=['pv_measurement'], inplace=True)\n",
    "    X_train = pd.concat([X_train_observed, X_train_estimated])\n",
    "    \n",
    "\n",
    "    # clip all y values to 0 if negative\n",
    "    y_train[\"y\"] = y_train[\"y\"].clip(lower=0)\n",
    "    \n",
    "    X_train = pd.merge(X_train, y_train, how=\"outer\", left_index=True, right_index=True)\n",
    "\n",
    "    X_train[\"location\"] = location\n",
    "    X_test[\"location\"] = location\n",
    "    \n",
    "    return X_train, X_test\n",
    "# Define locations\n",
    "locations = ['A', 'B', 'C']\n",
    "\n",
    "X_trains = []\n",
    "X_tests = []\n",
    "# Loop through locations\n",
    "for loc in locations:\n",
    "    print(f\"Processing location {loc}...\")\n",
    "    # Read target training data\n",
    "    y_train = pd.read_parquet(f'{loc}/train_targets.parquet')\n",
    "    \n",
    "    # Read estimated training data and add location feature\n",
    "    X_train_estimated = pd.read_parquet(f'{loc}/X_train_estimated.parquet')\n",
    "    \n",
    "    # Read observed training data and add location feature\n",
    "    X_train_observed= pd.read_parquet(f'{loc}/X_train_observed.parquet')\n",
    "\n",
    "    # Read estimated test data and add location feature\n",
    "    X_test_estimated = pd.read_parquet(f'{loc}/X_test_estimated.parquet')\n",
    "\n",
    "    # Preprocess data\n",
    "    X_train, X_test = preprocess_data(X_train_observed, X_train_estimated, X_test_estimated, y_train, loc)\n",
    "\n",
    "    X_trains.append(X_train)\n",
    "    X_tests.append(X_test)\n",
    "\n",
    "# Concatenate all data and save to csv\n",
    "X_train = pd.concat(X_trains)\n",
    "X_test = pd.concat(X_tests)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature enginering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporary\n",
    "X_train[\"hour\"] = X_train.index.hour\n",
    "X_train[\"weekday\"] = X_train.index.weekday\n",
    "# weekday or is_weekend\n",
    "X_train[\"is_weekend\"] = X_train[\"weekday\"].apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "# drop weekday\n",
    "#X_train.drop(columns=[\"weekday\"], inplace=True)\n",
    "X_train[\"month\"] = X_train.index.month\n",
    "X_train[\"year\"] = X_train.index.year\n",
    "\n",
    "X_test[\"hour\"] = X_test.index.hour\n",
    "X_test[\"weekday\"] = X_test.index.weekday\n",
    "\n",
    "# weekday or is_weekend\n",
    "X_test[\"is_weekend\"] = X_test[\"weekday\"].apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "# drop weekday\n",
    "#X_test.drop(columns=[\"weekday\"], inplace=True)\n",
    "X_test[\"month\"] = X_test.index.month\n",
    "X_test[\"year\"] = X_test.index.year\n",
    "\n",
    "\n",
    "\n",
    "to_drop = [\"snow_drift:idx\", \"snow_density:kgm3\"]\n",
    "\n",
    "X_train.drop(columns=to_drop, inplace=True)\n",
    "X_test.drop(columns=to_drop, inplace=True)\n",
    "\n",
    "X_train.dropna(subset=['y'], inplace=True)\n",
    "X_train.to_csv('X_train_raw.csv', index=True)\n",
    "X_test.to_csv('X_test_raw.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import autogluon.eda.auto as auto\n",
    "# auto.dataset_overview(train_data=X_train, test_data=X_test, label=\"y\", sample=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto.target_analysis(train_data=X_train, label=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last submission number: 78\n",
      "Now creating submission number: 79\n",
      "New filename: submission_79\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# Get the last submission number\n",
    "last_submission_number = int(max([int(filename.split('_')[1].split('.')[0]) for filename in os.listdir('submissions') if \"submission\" in filename]))\n",
    "print(\"Last submission number:\", last_submission_number)\n",
    "print(\"Now creating submission number:\", last_submission_number + 1)\n",
    "\n",
    "# Create the new filename\n",
    "new_filename = f'submission_{last_submission_number + 1}'\n",
    "\n",
    "hello = os.environ.get('HELLO')\n",
    "if hello is not None:\n",
    "    new_filename += f'_{hello}'\n",
    "\n",
    "print(\"New filename:\", new_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded data from: X_train_raw.csv | Columns = 52 / 52 | Rows = 93024 -> 93024\n"
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "train_data = TabularDataset('X_train_raw.csv')\n",
    "train_data.drop(columns=['ds'], inplace=True)\n",
    "\n",
    "label = 'y'\n",
    "metric = 'mean_absolute_error'\n",
    "time_limit = 60\n",
    "presets = 'best_quality'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = [None, None, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"AutogluonModels/submission_79_A\"\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=20\n",
      "Beginning AutoGluon training ... Time limit = 60s\n",
      "AutoGluon will save models to \"AutogluonModels/submission_79_A/\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.10.12\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Debian 5.10.191-1 (2023-08-16)\n",
      "Disk Space Avail:   102.14 GB / 105.09 GB (97.2%)\n",
      "Train Data Rows:    34085\n",
      "Train Data Columns: 50\n",
      "Label Column: y\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (5733.42, 0.0, 630.59471, 1165.90242)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    31398.19 MB\n",
      "\tTrain Data (Original)  Memory Usage: 15.34 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for location A...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 1): ['location']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 44 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', ...]\n",
      "\t\t('int', [])   :  5 | ['hour', 'weekday', 'is_weekend', 'month', 'year']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 43 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', ...]\n",
      "\t\t('int', [])       :  4 | ['hour', 'weekday', 'month', 'year']\n",
      "\t\t('int', ['bool']) :  2 | ['elevation:m', 'is_weekend']\n",
      "\t0.3s = Fit runtime\n",
      "\t49 features in original data used to generate 49 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 12.88 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.32s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 39.78s of the 59.68s of remaining time.\n",
      "\t-299.7062\t = Validation score   (-mean_absolute_error)\n",
      "\t0.05s\t = Training   runtime\n",
      "\t1.69s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 37.95s of the 57.85s of remaining time.\n",
      "\t-300.7424\t = Validation score   (-mean_absolute_error)\n",
      "\t0.05s\t = Training   runtime\n",
      "\t1.68s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 36.12s of the 56.02s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-165.3939\t = Validation score   (-mean_absolute_error)\n",
      "\t30.87s\t = Training   runtime\n",
      "\t21.58s\t = Validation runtime\n",
      "Completed 1/20 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 59.68s of the 17.97s of remaining time.\n",
      "\t-165.3939\t = Validation score   (-mean_absolute_error)\n",
      "\t0.3s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 9 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 17.65s of the 17.63s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-167.3696\t = Validation score   (-mean_absolute_error)\n",
      "\t11.57s\t = Training   runtime\n",
      "\t1.42s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 1.83s of the 1.82s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-176.7452\t = Validation score   (-mean_absolute_error)\n",
      "\t2.62s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Completed 1/20 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 59.68s of the -4.08s of remaining time.\n",
      "\t-167.2361\t = Validation score   (-mean_absolute_error)\n",
      "\t0.27s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 64.4s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/submission_79_A/\")\n"
     ]
    }
   ],
   "source": [
    "loc = \"A\"\n",
    "print(f\"Training model for location {loc}...\")\n",
    "predictor = TabularPredictor(label=label, eval_metric=metric, path=f\"AutogluonModels/{new_filename}_{loc}\").fit(train_data[train_data[\"location\"] == loc], time_limit=time_limit, presets=presets)\n",
    "predictors[0] = predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"AutogluonModels/submission_79_B\"\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=20\n",
      "Beginning AutoGluon training ... Time limit = 60s\n",
      "AutoGluon will save models to \"AutogluonModels/submission_79_B/\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.10.12\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Debian 5.10.191-1 (2023-08-16)\n",
      "Disk Space Avail:   102.13 GB / 105.09 GB (97.2%)\n",
      "Train Data Rows:    32844\n",
      "Train Data Columns: 50\n",
      "Label Column: y\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (1152.3, -0.0, 96.82478, 193.94649)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    31161.25 MB\n",
      "\tTrain Data (Original)  Memory Usage: 14.78 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for location B...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 1): ['location']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 44 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', ...]\n",
      "\t\t('int', [])   :  5 | ['hour', 'weekday', 'is_weekend', 'month', 'year']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 43 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', ...]\n",
      "\t\t('int', [])       :  4 | ['hour', 'weekday', 'month', 'year']\n",
      "\t\t('int', ['bool']) :  2 | ['elevation:m', 'is_weekend']\n",
      "\t0.2s = Fit runtime\n",
      "\t49 features in original data used to generate 49 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 12.42 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.26s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 39.82s of the 59.74s of remaining time.\n",
      "\t-56.8241\t = Validation score   (-mean_absolute_error)\n",
      "\t0.06s\t = Training   runtime\n",
      "\t1.64s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 37.87s of the 57.79s of remaining time.\n",
      "\t-56.7721\t = Validation score   (-mean_absolute_error)\n",
      "\t0.05s\t = Training   runtime\n",
      "\t1.56s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 36.15s of the 56.07s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-27.8389\t = Validation score   (-mean_absolute_error)\n",
      "\t31.82s\t = Training   runtime\n",
      "\t21.92s\t = Validation runtime\n",
      "Completed 1/20 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 59.74s of the 17.31s of remaining time.\n",
      "\t-27.8389\t = Validation score   (-mean_absolute_error)\n",
      "\t0.35s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 9 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 16.94s of the 16.92s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loc = \"B\"\n",
    "print(f\"Training model for location {loc}...\")\n",
    "predictor = TabularPredictor(label=label, eval_metric=metric, path=f\"AutogluonModels/{new_filename}_{loc}\").fit(train_data[train_data[\"location\"] == loc], time_limit=time_limit, presets=presets)\n",
    "predictors[1] = predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = \"C\"\n",
    "print(f\"Training model for location {loc}...\")\n",
    "predictor = TabularPredictor(label=label, eval_metric=metric, path=f\"AutogluonModels/{new_filename}_{loc}\").fit(train_data[train_data[\"location\"] == loc], time_limit=time_limit, presets=presets)\n",
    "predictors[2] = predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_data_with_dates = TabularDataset('X_train_raw.csv')\n",
    "train_data_with_dates[\"ds\"] = pd.to_datetime(train_data_with_dates[\"ds\"])\n",
    "\n",
    "test_data = TabularDataset('X_test_raw.csv')\n",
    "test_data[\"ds\"] = pd.to_datetime(test_data[\"ds\"])\n",
    "#test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = TabularDataset('test.csv')\n",
    "test_ids[\"time\"] = pd.to_datetime(test_ids[\"time\"])\n",
    "# merge test_data with test_ids\n",
    "test_data_merged = pd.merge(test_data, test_ids, how=\"inner\", right_on=[\"time\", \"location\"], left_on=[\"ds\", \"location\"])\n",
    "\n",
    "#test_data_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict, grouped by location\n",
    "predictions = []\n",
    "location_map = {\n",
    "    \"A\": 0,\n",
    "    \"B\": 1,\n",
    "    \"C\": 2\n",
    "}\n",
    "for loc, group in test_data.groupby('location'):\n",
    "    i = location_map[loc]\n",
    "    subset = test_data_merged[test_data_merged[\"location\"] == loc].reset_index(drop=True)\n",
    "    #print(subset)\n",
    "    pred = predictors[i].predict(subset)\n",
    "    subset[\"prediction\"] = pred\n",
    "    predictions.append(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot predictions for location A, in addition to train data for A\n",
    "for loc, idx in location_map.items():\n",
    "    fig, ax = plt.subplots(figsize=(20, 10))\n",
    "    # plot train data\n",
    "    train_data_with_dates[train_data_with_dates[\"location\"]==loc].plot(x='ds', y='y', ax=ax, label=\"train data\")\n",
    "\n",
    "    # plot predictions\n",
    "    predictions[idx].plot(x='ds', y='prediction', ax=ax, label=\"predictions\")\n",
    "\n",
    "    # title\n",
    "    ax.set_title(f\"Predictions for location {loc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate predictions\n",
    "submissions_df = pd.concat(predictions)\n",
    "submissions_df = submissions_df[[\"id\", \"prediction\"]]\n",
    "submissions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Save the submission DataFrame to submissions folder, create new name based on last submission, format is submission_<last_submission_number + 1>.csv\n",
    "\n",
    "# Save the submission\n",
    "print(f\"Saving submission to submissions/{new_filename}.csv\")\n",
    "submissions_df.to_csv(os.path.join('submissions', f\"{new_filename}.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save this notebook to submissions folder\n",
    "import subprocess\n",
    "import os\n",
    "subprocess.run([\"jupyter\", \"nbconvert\", \"--to\", \"pdf\", \"--output\", os.path.join('notebook_pdfs', f\"{new_filename}.pdf\"), \"autogluon_each_location.ipynb\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importance\n",
    "# location=\"A\"\n",
    "# split_time = pd.Timestamp(\"2022-10-28 22:00:00\")\n",
    "# estimated = train_data_with_dates[train_data_with_dates[\"ds\"] >= split_time]\n",
    "# estimated = estimated[estimated[\"location\"] == location]\n",
    "# predictors[0].feature_importance(feature_stage=\"original\", data=estimated, time_limit=60*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importance\n",
    "# observed = train_data_with_dates[train_data_with_dates[\"ds\"] < split_time]\n",
    "# observed = observed[observed[\"location\"] == location]\n",
    "# predictor.feature_importance(feature_stage=\"original\", data=observed, time_limit=60*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run([\"jupyter\", \"nbconvert\", \"--to\", \"pdf\", \"--output\", os.path.join('notebook_pdfs', f\"{new_filename}_with_feature_importance.pdf\"), \"autogluon_each_location.ipynb\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def execute_git_command(directory, command):\n",
    "    \"\"\"Execute a Git command in the specified directory.\"\"\"\n",
    "    try:\n",
    "        result = subprocess.check_output(['git', '-C', directory] + command, stderr=subprocess.STDOUT)\n",
    "        return result.decode('utf-8').strip(), True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Git command failed with message: {e.output.decode('utf-8').strip()}\")\n",
    "        return e.output.decode('utf-8').strip(), False\n",
    "\n",
    "\n",
    "git_repo_path = \".\"\n",
    "\n",
    "execute_git_command(git_repo_path, ['config', 'user.email', 'you@example.com'])\n",
    "execute_git_command(git_repo_path, ['config', 'user.name', 'Your Name'])\n",
    "\n",
    "branch_name = new_filename\n",
    "\n",
    "# add datetime to branch name\n",
    "branch_name += f\"_{pd.Timestamp.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "\n",
    "commit_msg = \"run result\"\n",
    "\n",
    "execute_git_command(git_repo_path, ['checkout', '-b',branch_name])\n",
    "\n",
    "# Navigate to your repo and commit changes\n",
    "execute_git_command(git_repo_path, ['add', '.'])\n",
    "execute_git_command(git_repo_path, ['commit', '-m',commit_msg])\n",
    "\n",
    "# Push to remote\n",
    "output, success = execute_git_command(git_repo_path, ['push', 'origin',branch_name])\n",
    "\n",
    "# If the push fails, try setting an upstream branch and push again\n",
    "if not success and 'upstream' in output:\n",
    "    print(\"Attempting to set upstream and push again...\")\n",
    "    execute_git_command(git_repo_path, ['push', '--set-upstream', 'origin',branch_name])\n",
    "    execute_git_command(git_repo_path, ['push', 'origin', branch_name])\n",
    "\n",
    "execute_git_command(git_repo_path, ['checkout', 'main'])"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m111"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
